
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quark.torch.quantization.config.config &#8212; AMD Quark 0.10 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/custom.css?v=a66ef196" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_header.css?v=9557e3d1" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_footer.css?v=7095035a" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../../_static/documentation_options.js?v=e0f31c2e"></script>
    <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script defer="defer" src="../../../../../_static/search.js?v=90a4452c"></script>
    <script src="../../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/quark/torch/quantization/config/config';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].innerText = "Show code ▼\n\n";

            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">AMD Quark 0.10 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../basic_usage.html">Getting started: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/basic_usage_onnx.html">Getting started: Quark for ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/basic_usage_pytorch.html">Getting started: Quark for PyTorch</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/pytorch_examples.html">PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_fx_image_classification.html">Image Classification Models FX Graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/sample_yolo_nas_quant.html">YOLO-NAS FX graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/sample_yolo_x_tiny_quant.html">YOLO-X Tiny FX Graph Quantization</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/onnx_examples.html">ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_layerwise_percentile.html">Layer-wise Percentile</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_auto_search.html">Auto-Search for General Yolov3 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_ryzenai_yolonas.html">Auto-Search for Ryzen AI Yolo-nas ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_ryzenai_autosearch_resnet50.html">Auto-Search for Ryzen AI Resnet50 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_ryzenai_yolov3_custom_evaluator.html">Auto-Search for Ryzen AI Yolov3 ONNX Quantization with Custom Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizing Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_crypto_mode.html">Quantizing a ResNet50 model in crypto mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/hugging_face_timm_quantization.html">Hugging Face TIMM Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_yolo_quantization.html">Yolo_nas and Yolox Quantization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported accelerators</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../supported_accelerators/ryzenai/index.html">AMD Ryzen AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../supported_accelerators/ryzenai/tutorial_quick_start_for_ryzenai.html">Quick Start for Ryzen AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../supported_accelerators/ryzenai/ryzen_ai_best_practice.html">Best Practice for Ryzen AI in AMD Quark ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_ryzenai.html">Auto-Search for Ryzen AI ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../supported_accelerators/ryzenai/tutorial_uint4_oga.html">Quantizing LLMs for ONNX Runtime GenAI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../supported_accelerators/ryzenai/tutorial_convert_fp32_or_fp16_to_bf16.html">FP32/FP16 to BF16 Model Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../supported_accelerators/ryzenai/tutorial_xint8_quantize.html">Power-of-Two Scales (XINT8) Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../supported_accelerators/ryzenai/tutorial_a8w8_and_a16w8_quantize.html">Float Scales (A8W8 and A16W8) Quantization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../supported_accelerators/mi_gpus/index.html">AMD Instinct</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Evaluation of Quantized Models</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/user_guide_config_for_llm.html">Configuring PyTorch Quantization for Large Language Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization from Scratch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_onnx.html">ONNX format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_hf.html">Hugging Face format (safetensors)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export_gguf.html">GGUF format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_quark.html">Quark format</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/feature_pruning_overall.html">LLM Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark and Trainer</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/torch/auto_smoothquant_document_and_example.html">Auto SmoothQuant</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/awq_document.html">Activation-aware Weight Quantization (AWQ)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../tutorials/torch/example_awq.html">AWQ end-to-end demo</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/custom_operators/ExtendedQuantizeLinear.html">ExtendedQuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/custom_operators/ExtendedDequantizeLinear.html">ExtendedDequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/custom_operators/ExtendedInstanceNormalization.html">ExtendedInstanceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/custom_operators/ExtendedLSTM.html">ExtendedLSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/custom_operators/BFPQuantizeDequantize.html">BFPQuantizeDequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/custom_operators/MXQuantizeDequantize.html">MXQuantizeDequantize</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_bfp16_quantization.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_microscaling_quantization.html">Microscaling (MX)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_microexponents_quantization.html">Microexponents (MX)</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">Quantizing a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/user_guide_auto_search.html">Automatic Search for Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/config/user_guide_onnx_model_inference_save_input_npy.html">Using ONNX Model Inference and Saving Input Data in NPY Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/torch/quickstart_tutorial/quickstart_tutorial.html">AMD Quark Tutorial: PyTorch Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/torch/diffusion_tutorial/diffusion_tutorial.html">Quantizing a Diffusion Model using Quark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/torch/depth_wise_pruning/llm_depth_pruning.html">LLM Model Depth-Wise Pruning (beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/torch/llm_tutorial/llm_tutorial.html">Quantizing a Large Language Model with Quark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Third-party contributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../intro_contrib.html">Introduction and guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../autoapi/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/config/template/index.html">Quantizer Template</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../autoapi/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../versions.html">AMD Quark release history</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../license.html">Quark license</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">quark.torch.quantization.config.config</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for quark.torch.quantization.config.config</h1><div class="highlight"><pre>
<span></span>#
# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#
&quot;&quot;&quot;Quark Quantization Config API for PyTorch&quot;&quot;&quot;

from __future__ import annotations

import json
from abc import ABC, abstractmethod
from dataclasses import asdict, dataclass, field, fields
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Type, TypeVar, Union, cast

import torch.nn as nn

if TYPE_CHECKING:
    from quark.torch.quantization.config.template import LLMTemplate

from quark.shares.utils.doc import add_start_docstring
from quark.shares.utils.log import ScreenLogger
from quark.torch.quantization.config.type import (
    ALL_DATA_TYPES,
    DeviceType,
    Dtype,
    QSchemeType,
    QuantizationMode,
    RoundType,
    ScaleType,
    TQTThresholdInitMeth,
    ZeroPointType,
)
from quark.torch.quantization.config.utils import dataclass_pretty_string
from quark.torch.quantization.constants import ONLY_DTYPE_CHANGE, QUARK_LAYER_TYPES, USING_NON_SCALED_QUANT
from quark.torch.quantization.observer import (
    OBSERVER_CLASSES,
    OBSERVER_MAP,
    PER_CHANNEL_OBSERVERS,
    PER_GROUP_OBSERVERS,
    PER_TENSOR_OBSERVERS,
    ObserverBase,
    PerBlockMXDiffsObserver,
    PerBlockMXObserver,
    PerChannelMinMaxObserver,
    PerGroupMinMaxObserver,
    PerTensorHistogramObserver,
    PerTensorHistogramObserverPro,
    PerTensorMinMaxObserver,
    PerTensorMSEObserver,
    PerTensorPercentileObserver,
    PlaceholderObserver,
)
from quark.version import __version__

logger = ScreenLogger(__name__)

DATA_TYPE_SPEC_DOCSTRING = r&quot;&quot;&quot;Helper class to define a :py:class:`.QuantizationSpec` using {0}.

Example:

.. code-block:: python

    quantization_spec = {1}({2}).to_quantization_spec()
&quot;&quot;&quot;

PER_TENSOR_OBSERVER_METHOD_MAP: dict[str, type[ObserverBase]] = {
    &quot;min_max&quot;: PerTensorMinMaxObserver,
    &quot;histogram&quot;: PerTensorHistogramObserver,
    &quot;histogrampro&quot;: PerTensorHistogramObserverPro,
    &quot;MSE&quot;: PerTensorMSEObserver,
    &quot;percentile&quot;: PerTensorPercentileObserver,
}

SCALE_TYPE_MAP = {&quot;float&quot;: ScaleType.float, &quot;power_of_2&quot;: ScaleType.pof2}

ROUND_METHOD_MAP = {&quot;round&quot;: RoundType.round, &quot;floor&quot;: RoundType.floor, &quot;half_even&quot;: RoundType.half_even}

ZERO_POINT_TYPE_MAP = {&quot;int32&quot;: ZeroPointType.int32, &quot;float32&quot;: ZeroPointType.float32}


def get_per_tensor_observer(observer_method: str | None = None) -&gt; type[ObserverBase] | None:
    if observer_method:
        assert observer_method in PER_TENSOR_OBSERVER_METHOD_MAP, (
            f&quot;Invalid observer method. Valid observer methods are {list(PER_TENSOR_OBSERVER_METHOD_MAP.keys())}&quot;
        )
        observer_cls = PER_TENSOR_OBSERVER_METHOD_MAP[observer_method]
    else:
        observer_cls = None
    return observer_cls


def get_scale_type(scale_type: str | None = None) -&gt; ScaleType | None:
    if scale_type:
        assert scale_type in SCALE_TYPE_MAP, f&quot;Invalid scale type. Valid scale types are {list(SCALE_TYPE_MAP.keys())}&quot;
        ret = SCALE_TYPE_MAP[scale_type]
    else:
        ret = None
    return ret


def get_round_method(round_method: str | None = None) -&gt; RoundType | None:
    if round_method:
        assert round_method in ROUND_METHOD_MAP, (
            f&quot;Invalid round method. Valid round methods are {list(ROUND_METHOD_MAP.keys())}&quot;
        )
        ret = ROUND_METHOD_MAP[round_method]
    else:
        ret = None
    return ret


def get_zero_point_type(zero_point_type: str | None = None) -&gt; ZeroPointType | None:
    if zero_point_type:
        assert zero_point_type in ZERO_POINT_TYPE_MAP, (
            f&quot;Invalid zero point type, Valid zero point type method are {list(ZERO_POINT_TYPE_MAP.keys())}&quot;
        )
        ret = ZERO_POINT_TYPE_MAP[zero_point_type]
    else:
        ret = None
    return ret


T = TypeVar(&quot;T&quot;, bound=&quot;ConfigBase&quot;)


<div class="viewcode-block" id="ConfigBase">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.ConfigBase">[docs]</a>
@dataclass(eq=True)
class ConfigBase(ABC):
    name = &quot;&quot;

    @classmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -&gt; T:
        return cls(**data)

    def update_from_dict(self, data: dict[str, Any]) -&gt; None:
        for field_name in data:
            setattr(self, field_name, data[field_name])

    def to_dict(self) -&gt; dict[str, Any]:
        return asdict(self)</div>



<div class="viewcode-block" id="Config">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Config">[docs]</a>
@dataclass(eq=True)
class Config(ConfigBase):
    &quot;&quot;&quot;
    A class that encapsulates comprehensive quantization configurations for a machine learning model, allowing for detailed and hierarchical control over quantization parameters across different model components.

    :param QuantizationConfig global_quant_config: Global quantization configuration applied to the entire model unless overridden at the layer level.
    :param Dict[torch.nn.Module, QuantizationConfig] layer_type_quant_config: A dictionary mapping from layer types (e.g., nn.Conv2d, nn.Linear) to their quantization configurations.
    :param Dict[str, QuantizationConfig] layer_quant_config: A dictionary mapping from layer names to their quantization configurations, allowing for per-layer customization. Default is ``{}``.
    :param Dict[str, QuantizationConfig] kv_cache_quant_config: A dictionary mapping from layer names to kv_cache quantization configurations. Default is ``{}``.
    :param Optional[QuantizationSpec] softmax_quant_spec: A quantization specifications of nn.functional.softmax output. Default is ``None``.
    :param List[str] exclude: A list of layer names to be excluded from quantization, enabling selective quantization of the model. Default is ``[]``.
    :param Optional[AlgoConfig] algo_config: Optional configuration for the quantization algorithm, such as GPTQ, AWQ and Qronos. After this process, the datatype/fake_datatype of weights will be changed with quantization scales. Default is ``None``.
    :param QuantizationMode quant_mode: The quantization mode to be used (``eager_mode`` or ``fx_graph_mode``). Default is ``QuantizationMode.eager_mode``.
    :param Optional[int] log_severity_level: 0:DEBUG, 1:INFO, 2:WARNING. 3:ERROR, 4:CRITICAL/FATAL. Default is ``1``.
    &quot;&quot;&quot;

    # Global quantization configuration applied to the entire model unless overridden at the layer level.
    global_quant_config: QuantizationConfig

    # A dictionary mapping from layer types (e.g., nn.Conv2d, nn.Linear) to their quantization configurations.
    layer_type_quant_config: dict[type[nn.Module], QuantizationConfig] = field(default_factory=dict)

    # A dictionary mapping from layer names to their quantization configurations, allowing for per-layer customization.
    layer_quant_config: dict[str, QuantizationConfig] = field(default_factory=dict)

    # A dictionary mapping from layer names to kv_cache quantization configurations.
    kv_cache_quant_config: dict[str, QuantizationConfig] = field(default_factory=dict)

    # A list of layer names to be grouped for kv_cache quantization, enabling per-group customization.
    kv_cache_group: list[str] = field(default_factory=list)

    # The minimum scale of kv_cache quantization.
    min_kv_scale: float = 0.0

    # A quantization specifications of nn.functional.softmax output.
    softmax_quant_spec: QuantizationSpec | None = None

    # A list of layer names to be excluded from quantization, enabling selective quantization of the model.
    exclude: list[str] = field(default_factory=list)

    # Optional configuration for the quantization algorithm, such as GPTQ, AWQ and Qronos
    # After this process, the datatype/fake_datatype of weights will be changed with quantization scales.
    algo_config: list[AlgoConfig] | None = None

    # The quantization mode to be used (eager_mode or fx_graph_mode)
    quant_mode: QuantizationMode = QuantizationMode.eager_mode

    # Log level for printing on screen
    log_severity_level: int | None = 1

    # Version of the quantization tool
    version: str | None = __version__

    def to_dict(self) -&gt; dict[str, Any]:
        config_dict: dict[str, Any] = {
            &quot;global_quant_config&quot;: self.global_quant_config.to_dict(),
            &quot;exclude&quot;: self.exclude,
            &quot;algo_config&quot;: [config.to_dict() for config in self.algo_config] if self.algo_config is not None else None,
            &quot;softmax_quant_spec&quot;: self.softmax_quant_spec.to_dict() if self.softmax_quant_spec is not None else None,
            &quot;quant_method&quot;: &quot;quark&quot;,
        }

        layer_type_quant_config_dict: dict[str, Any] = {}
        for layer_type, config in self.layer_type_quant_config.items():
            layer_type_quant_config_dict[layer_type.__name__] = config.to_dict()
        config_dict[&quot;layer_type_quant_config&quot;] = layer_type_quant_config_dict

        layer_quant_config_dict: dict[str, Any] = {}
        for name, config in self.layer_quant_config.items():
            layer_quant_config_dict[name] = config.to_dict()
        config_dict[&quot;layer_quant_config&quot;] = layer_quant_config_dict

        kv_cache_quant_config_dict: dict[str, Any] = {}
        for name, config in self.kv_cache_quant_config.items():
            kv_cache_quant_config_dict[name] = config.to_dict()
        config_dict[&quot;kv_cache_quant_config&quot;] = kv_cache_quant_config_dict

        config_dict[&quot;quant_mode&quot;] = self.quant_mode.name

        config_dict[&quot;version&quot;] = self.version

        return config_dict

    def __str__(self) -&gt; str:
        s = dataclass_pretty_string(self)
        return s

    @classmethod
    def from_dict(cls, config_dict: dict[str, Any]) -&gt; Config:
        global_quant_config = QuantizationConfig.from_dict(config_dict[&quot;global_quant_config&quot;])

        # TODO: Deprecate legacy configuration and remove the None check here.
        # Legacy (quark&lt;1.0) configuration used to allow layer_type_quant_config=None in the serialized config, inconstitant with
        # the type hints of the dataclass.
        layer_type_quant_config = {}
        if config_dict[&quot;layer_type_quant_config&quot;] is not None:
            for layer_type_name, layer_type_quantization_config in config_dict[&quot;layer_type_quant_config&quot;].items():
                if layer_type_name in QUARK_LAYER_TYPES:
                    layer_type_quant_config[QUARK_LAYER_TYPES[layer_type_name]] = QuantizationConfig.from_dict(
                        layer_type_quantization_config
                    )
                else:
                    raise NotImplementedError(
                        f&quot;Quark does not support reloading a quantization `Config` from a dictionary using custom `layer_type_quantization_config`. Found `&#39;{layer_type_name}&#39;` in `layer_type_quantization_config`, which is not among the supported {QUARK_LAYER_TYPES}.&quot;
                    )

        # TODO: Deprecate legacy configuration and remove the None check here.
        # Legacy (quark&lt;1.0) configuration used to allow layer_quant_config=None in the serialized config, inconstitant with
        # the type hints of the dataclass.
        if config_dict[&quot;layer_quant_config&quot;] is not None:
            layer_quant_config = {
                layer_name: QuantizationConfig.from_dict(quant_config_dict)
                for layer_name, quant_config_dict in config_dict[&quot;layer_quant_config&quot;].items()
            }
        else:
            layer_quant_config = {}

        if config_dict.get(&quot;kv_cache_quant_config&quot;) is not None:
            kv_cache_quant_config = {
                kv_cache_name: QuantizationConfig.from_dict(kv_cache_config_dict)
                for kv_cache_name, kv_cache_config_dict in config_dict[&quot;kv_cache_quant_config&quot;].items()
            }
        else:
            kv_cache_quant_config = {}

        # TODO: Deprecate legacy (quark&lt;1.0) configuration and remove the check here.
        # `exclude` used to be serialized as `None` when there was no exclude layer, instead of `[]`.
        if config_dict[&quot;exclude&quot;] is None:  # pragma: no cover
            exclude = []
        else:
            exclude = config_dict[&quot;exclude&quot;]

        if &quot;algo_config&quot; in config_dict and config_dict[&quot;algo_config&quot;] is not None:
            if isinstance(config_dict[&quot;algo_config&quot;], list):  # new config
                algo_config = [_load_quant_algo_config_from_dict(config) for config in config_dict[&quot;algo_config&quot;]]
            else:  # old config
                algo_config = [_load_quant_algo_config_from_dict(config_dict[&quot;algo_config&quot;])]
        else:
            algo_config = None

        # Get softmax_quant_spec configuration from config_dict
        softmax_quant_spec = (
            QuantizationSpec.from_dict(config_dict[&quot;softmax_quant_spec&quot;])
            if (&quot;softmax_quant_spec&quot; in config_dict and config_dict[&quot;softmax_quant_spec&quot;] is not None)
            else None
        )

        if &quot;quant_mode&quot; in config_dict:
            quant_mode = QuantizationMode[config_dict[&quot;quant_mode&quot;]]  # Access by name and not by value.
        else:
            # TODO: Deprecate legacy (quark&lt;1.0) configuration and remove the check here.
            # The key `&quot;quant_mode&quot;` used not to be serialized in the legacy quantization_config, inconstitant with
            # the type hints of the dataclass.
            quant_mode = QuantizationMode.eager_mode

        log_severity_level = 1  # `log_severity_level` is not saved.

        # get version from config_dict, if not found (e.g. models exported with amd-quark&lt;=0.8), set it to `None`.
        version = config_dict[&quot;version&quot;] if &quot;version&quot; in config_dict else None

        return cls(
            global_quant_config=global_quant_config,
            layer_type_quant_config=layer_type_quant_config,
            layer_quant_config=layer_quant_config,
            kv_cache_quant_config=kv_cache_quant_config,
            exclude=exclude,
            algo_config=algo_config,
            softmax_quant_spec=softmax_quant_spec,
            quant_mode=quant_mode,
            log_severity_level=log_severity_level,
            version=version,
        )

    @staticmethod
    def with_llm_template(
        template: LLMTemplate,
        scheme: str,
        algorithm: str | None = None,
        kv_cache_scheme: str | None = None,
        min_kv_scale: float = 0.0,
        attention_scheme: str | None = None,
        layer_config: dict[str, str] | None = None,
        layer_type_config: dict[type[nn.Module], str] | None = None,
        exclude_layers: list[str] | None = None,
    ) -&gt; Config:
        return template.get_config(
            scheme=scheme,
            algorithm=algorithm,
            kv_cache_scheme=kv_cache_scheme,
            min_kv_scale=min_kv_scale,
            attention_scheme=attention_scheme,
            layer_config=layer_config,
            layer_type_config=layer_type_config,
            exclude_layers=exclude_layers,
        )

    def __post_init__(self) -&gt; None:
        if self.algo_config is not None:
            for algo_config in self.algo_config:
                if algo_config.name == &quot;quarot&quot;:
                    if len(self.kv_cache_quant_config) &gt; 0 and not algo_config.r3:  # type: ignore
                        logger.warning(
                            f&quot;Quarot R3 rotation is disabled, but the KV cache is configured to quantized with: {self.kv_cache_quant_config}. KV cache quantization may benefit from R3 rotation if keys are quantized. Consider using `r3=True` in Quarot configuration.&quot;
                        )

                    if len(self.kv_cache_quant_config) == 0 and algo_config.r3:  # type: ignore
                        logger.warning(
                            &quot;No KV cache quantization configuration provided, but `QuaRotConfig.r3` is set to `True`. This setting is only useful in case KV cache quantization is used. Consider using `r3=False`.&quot;
                        )</div>



<div class="viewcode-block" id="QuantizationConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QuantizationConfig">[docs]</a>
@dataclass(eq=True)
class QuantizationConfig:
    &quot;&quot;&quot;
    A data class that specifies quantization configurations for different components of a module, allowing hierarchical control over how each tensor type is quantized.

    :param Optional[Union[QuantizationSpec, List[QuantizationSpec]]] input_tensors: Input tensors quantization specification. If None, following the hierarchical quantization setup. e.g. If the ``input_tensors`` in ``layer_type_quant_config`` is ``None``, the configuration from ``global_quant_config`` will be used instead. Defaults to ``None``. If None in ``global_quant_config``, ``input_tensors`` are not quantized.
    :param Optional[Union[QuantizationSpec, List[QuantizationSpec]]] output_tensors: Output tensors quantization specification. Defaults to ``None``. If ``None``, the same as above.
    :param Optional[Union[QuantizationSpec, List[QuantizationSpec]]] weight: The weights tensors quantization specification. Defaults to ``None``. If ``None``, the same as above.
    :param Optional[Union[QuantizationSpec, List[QuantizationSpec]]] bias: The bias tensors quantization specification. Defaults to ``None``. If ``None``, the same as above.
    :param Optional[DeviceType] target_device: Configuration specifying the target device (e.g., CPU, GPU, IPU) for the quantized model.
    &quot;&quot;&quot;

    input_tensors: Union[QuantizationSpec, list[QuantizationSpec]] | None = None

    output_tensors: Union[QuantizationSpec, list[QuantizationSpec]] | None = None

    weight: Union[QuantizationSpec, list[QuantizationSpec]] | None = None

    bias: Union[QuantizationSpec, list[QuantizationSpec]] | None = None

    target_device: DeviceType | None = None

    def to_dict(self) -&gt; dict[str, Any]:
        # TODO need to solve circle import problem
        # let the check_and_adjust_quant_config to be self.check_and_adjust_quant_config()
        from quark.torch.quantization.config.config_verification import check_and_adjust_quant_config

        self = check_and_adjust_quant_config(self)

        def convert_spec_to_dict(
            spec: Union[QuantizationSpec, list[QuantizationSpec]] | None,
        ) -&gt; Union[dict[str, Any], list[dict[str, Any]]] | None:
            if spec is None:
                return None
            elif isinstance(spec, list):
                return [s.to_dict() for s in spec]
            else:
                return spec.to_dict()

        return {
            &quot;input_tensors&quot;: convert_spec_to_dict(self.input_tensors),
            &quot;output_tensors&quot;: convert_spec_to_dict(self.output_tensors),
            &quot;weight&quot;: convert_spec_to_dict(self.weight),
            &quot;bias&quot;: convert_spec_to_dict(self.bias),
            &quot;target_device&quot;: self.target_device.value if self.target_device is not None else None,
        }

    @classmethod
    def from_dict(cls, quantization_config: dict[str, Any]) -&gt; QuantizationConfig:
        def convert_dict_to_spec(
            config: Union[dict[str, Any], list[dict[str, Any]]] | None,
        ) -&gt; Union[QuantizationSpec, list[QuantizationSpec]] | None:
            if config is None:
                return None
            elif isinstance(config, list):
                specs = [QuantizationSpec.from_dict(c) for c in config]
                assert all(spec is not None for spec in specs), &quot;all quantization specs must be valid (not None)&quot;
                # Cast to remove Optional after we&#39;ve verified no None values exist
                return cast(list[QuantizationSpec], specs)
            else:
                return QuantizationSpec.from_dict(config)

        input_tensors = convert_dict_to_spec(quantization_config[&quot;input_tensors&quot;])
        output_tensors = convert_dict_to_spec(quantization_config[&quot;output_tensors&quot;])
        weight = convert_dict_to_spec(quantization_config[&quot;weight&quot;])
        bias = convert_dict_to_spec(quantization_config[&quot;bias&quot;])

        # TODO: Deprecate legacy configuration.
        # Legacy (quark&lt;1.0) saved quantization_config does not have the key `&quot;target_device&quot;`.
        target_device = quantization_config[&quot;target_device&quot;] if &quot;target_device&quot; in quantization_config else None
        target_device = DeviceType(target_device) if target_device is not None else None

        return cls(
            input_tensors=input_tensors,
            output_tensors=output_tensors,
            weight=weight,
            bias=bias,
            target_device=target_device,
        )</div>



<div class="viewcode-block" id="TwoStageSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.TwoStageSpec">[docs]</a>
@dataclass
class TwoStageSpec(ConfigBase):
    &quot;&quot;&quot;
    A data class that specifies two-stage quantization configurations for different components of a module,
    allowing hierarchical control over how each tensor type is quantized.
    &quot;&quot;&quot;

    first_stage: Union[DataTypeSpec, QuantizationSpec]
    second_stage: Union[DataTypeSpec, QuantizationSpec]

    @abstractmethod
    def to_quantization_spec(self) -&gt; list[QuantizationSpec]:
        pass</div>



<div class="viewcode-block" id="ProgressiveSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.ProgressiveSpec">[docs]</a>
@dataclass
class ProgressiveSpec(TwoStageSpec):
    &quot;&quot;&quot;
    A data class that specifies a progressive quantization specification for a tensor.
    The first stage quantizes the input tensor, while the second stage quantizes the output from the first stage.

    For example, to progressively quantize a float16 tensor:

    1. First quantize it to fp8_e4m3 using fp8_e4m3 per-tensor quantization, get a fp8_e4m3 tensor.
    2. Then quantize the fp8_e4m3 tensor to int4 using int4 per-channel quantization, get a int4 tensor.

    The configuration for this example would be:

    .. code-block:: python

        quant_spec = ProgressiveSpec(
            first_stage=FP8E4M3PerTensorSpec(observer_method=&quot;min_max&quot;,
                                             is_dynamic=False),
            second_stage=Int4PerChannelSpec(symmetric=False,
                                            scale_type=&quot;float&quot;,
                                            round_method=&quot;half_even&quot;,
                                            ch_axis=0,
                                            is_dynamic=False)
        ).to_quantization_spec()
    &quot;&quot;&quot;

    def to_quantization_spec(self) -&gt; list[QuantizationSpec]:
        return [
            self.first_stage.to_quantization_spec() if isinstance(self.first_stage, DataTypeSpec) else self.first_stage,
            self.second_stage.to_quantization_spec()
            if isinstance(self.second_stage, DataTypeSpec)
            else self.second_stage,
        ]</div>



<div class="viewcode-block" id="ScaleQuantSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.ScaleQuantSpec">[docs]</a>
@dataclass
class ScaleQuantSpec(TwoStageSpec):
    &quot;&quot;&quot;
    A data class that specifies a two-stage quantization process for scale quantization.

    The quantization happens in two stages:

    1. First stage quantizes the input tensor itself.
    2. Second stage quantizes the scale values from the first stage quantization.

    For example, given a float16 tensor:

    1. First quantize the tensor to fp4_e2m1 using fp4_e2m1 per-group quantization, producing a fp4_e2m1 tensor with float16 scale values.
    2. Then quantize those float16 scale values to fp8_e4m3 using fp8_e4m3 per-tensor quantization.

    The configuration for this example would be:

    .. code-block:: python

        quant_spec = ScaleQuantSpec(
            first_stage=FP4PerGroupSpec(group_size=16, is_dynamic=False),
            second_stage=FP8E4M3PerTensorSpec(observer_method=&quot;min_max&quot;, is_dynamic=False)
        ).to_quantization_spec()
    &quot;&quot;&quot;

    def to_quantization_spec(self) -&gt; list[QuantizationSpec]:
        second_stage_spec = (
            self.second_stage.to_quantization_spec()
            if isinstance(self.second_stage, DataTypeSpec)
            else self.second_stage
        )
        second_stage_spec.is_scale_quant = True
        return [
            self.first_stage.to_quantization_spec() if isinstance(self.first_stage, DataTypeSpec) else self.first_stage,
            second_stage_spec,
        ]</div>



<div class="viewcode-block" id="DataTypeSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.DataTypeSpec">[docs]</a>
class DataTypeSpec(ConfigBase):
    @abstractmethod
    def to_quantization_spec(self) -&gt; QuantizationSpec:
        pass</div>



<div class="viewcode-block" id="Uint4PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint4PerTensorSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;uint4 per tensor quantization&quot;, &quot;Uint4PerTensorSpec&quot;, &quot;is_dynamic=True, symmetric=False&quot;
    )
)
class Uint4PerTensorSpec(DataTypeSpec):
    observer_method: str | None = None
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.uint4,
            observer_cls=get_per_tensor_observer(self.observer_method),
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_tensor,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Uint4PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint4PerChannelSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;uint4 per channel quantization&quot;,
        &quot;Uint4PerChannelSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=0,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Uint4PerChannelSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    zero_point_type: str | None = &quot;int32&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.uint4,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_channel,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            zero_point_type=get_zero_point_type(self.zero_point_type),
        )</div>



<div class="viewcode-block" id="Uint4PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint4PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;uint4 per group quantization&quot;,
        &quot;Uint4PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=False,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=1,
        is_dynamic=False,
        group_size=128
    &quot;&quot;&quot;,
    )
)
class Uint4PerGroupSpec(DataTypeSpec):
    symmetric: bool = False
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    scale_type: str | None = None
    round_method: str | None = &quot;half_even&quot;
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.uint4,
            observer_cls=PerGroupMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="Int3PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int3PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int3 per group quantization&quot;,
        &quot;Int3PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        is_dynamic=False,
        group_size=32,
    &quot;&quot;&quot;,
    )
)
class Int3PerGroupSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int3,
            observer_cls=PerGroupMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="Int3PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int3PerChannelSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int3 per channel quantization&quot;,
        &quot;Int3PerChannelSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=False,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=0,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Int3PerChannelSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int3,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_channel,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Int2PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int2PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int2 per group quantization&quot;,
        &quot;Int2PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        is_dynamic=False,
        group_size=32,
    &quot;&quot;&quot;,
    )
)
class Int2PerGroupSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int2,
            observer_cls=PerGroupMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="Int4PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int4PerTensorSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int4 per tensor quantization&quot;,
        &quot;Int4PerTensorSpec&quot;,
        r&quot;&quot;&quot;
        observer_method=&quot;min_max&quot;,
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Int4PerTensorSpec(DataTypeSpec):
    observer_method: str | None = None
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int4,
            observer_cls=get_per_tensor_observer(self.observer_method),
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_tensor,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Int4PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int4PerChannelSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int4 per channel quantization&quot;,
        &quot;Int4PerChannelSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=False,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=0,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Int4PerChannelSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int4,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_channel,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Int4PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int4PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int4 per group quantization&quot;,
        &quot;Int4PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=1,
        is_dynamic=False,
        group_size=128
    &quot;&quot;&quot;,
    )
)
class Int4PerGroupSpec(DataTypeSpec):
    symmetric: bool = True
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    scale_type: str | None = None
    round_method: str | None = &quot;half_even&quot;
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int4,
            observer_cls=PerGroupMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="Uint8PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint8PerTensorSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;uint8 per tensor quantization&quot;,
        &quot;Uint8PerTensorSpec&quot;,
        r&quot;&quot;&quot;
        observer_method=&quot;percentile&quot;,
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Uint8PerTensorSpec(DataTypeSpec):
    observer_method: str | None = None
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.uint8,
            observer_cls=get_per_tensor_observer(self.observer_method),
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_tensor,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Uint8PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint8PerChannelSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;uint8 per channel quantization&quot;,
        &quot;Uint8PerChannelSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=0,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Uint8PerChannelSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.uint8,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_channel,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Uint8PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint8PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;uint8 per group quantization&quot;,
        &quot;Uint8PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=False,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=1,
        is_dynamic=False,
        group_size=128
    &quot;&quot;&quot;,
    )
)
class Uint8PerGroupSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.uint8,
            observer_cls=PerGroupMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="Int8PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int8PerTensorSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int8 per tensor quantization&quot;,
        &quot;Int8PerTensorSpec&quot;,
        r&quot;&quot;&quot;
        observer_method=&quot;min_max&quot;,
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Int8PerTensorSpec(DataTypeSpec):
    observer_method: str | None = None
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int8,
            observer_cls=get_per_tensor_observer(self.observer_method),
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_tensor,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Int8PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int8PerChannelSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int8 per channel quantization&quot;,
        &quot;Int8PerChannelSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=False,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=0,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class Int8PerChannelSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int8,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_channel,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="Int8PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int8PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;int8 per group quantization&quot;,
        &quot;Int8PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        symmetric=True,
        scale_type=&quot;float&quot;,
        round_method=&quot;half_even&quot;,
        ch_axis=1,
        is_dynamic=False,
        group_size=128
    &quot;&quot;&quot;,
    )
)
class Int8PerGroupSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.int8,
            observer_cls=PerGroupMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="FP8E4M3PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E4M3PerTensorSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;FP8E4M3 per tensor quantization&quot;,
        &quot;FP8E4M3PerTensorSpec&quot;,
        r&quot;&quot;&quot;
        observer_method=&quot;min_max&quot;,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class FP8E4M3PerTensorSpec(DataTypeSpec):
    observer_method: str | None = None
    scale_type: str | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e4m3,
            observer_cls=get_per_tensor_observer(self.observer_method),
            symmetric=True,
            scale_type=get_scale_type(self.scale_type),
            round_method=RoundType.half_even,
            qscheme=QSchemeType.per_tensor,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="FP8E4M3PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E4M3PerChannelSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;FP8E4M3 per channel quantization&quot;, &quot;FP8E4M3PerChannelSpec&quot;, &quot;is_dynamic=False, ch_axis=0&quot;
    )
)
class FP8E4M3PerChannelSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e4m3,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_channel,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="FP8E4M3PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E4M3PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;FP8E4M3 per group quantization&quot;,
        &quot;FP8E4M3PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        group_size=group_size,
        is_dynamic=True
    &quot;&quot;&quot;,
    )
)
class FP8E4M3PerGroupSpec(DataTypeSpec):
    scale_format: str | None = &quot;float32&quot;
    scale_calculation_mode: str | None = None
    ch_axis: int | None = -1
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e4m3,
            observer_cls=PerBlockMXObserver,
            symmetric=None,
            scale_type=ScaleType.float,
            scale_format=self.scale_format,
            scale_calculation_mode=self.scale_calculation_mode,
            round_method=RoundType.half_even,
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="FP8E5M2PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E5M2PerTensorSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;FP8E5M2 per tensor quantization&quot;,
        &quot;FP8E5M2PerTensorSpec&quot;,
        r&quot;&quot;&quot;
        observer_method=&quot;min_max&quot;,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class FP8E5M2PerTensorSpec(DataTypeSpec):
    observer_method: str | None = None
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e5m2,
            observer_cls=get_per_tensor_observer(self.observer_method),
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_tensor,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="FP8E5M2PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E5M2PerChannelSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;FP8E5M2 per channel quantization&quot;, &quot;FP8E5M2PerChannelSpec&quot;, &quot;is_dynamic=False, ch_axis=0&quot;
    )
)
class FP8E5M2PerChannelSpec(DataTypeSpec):
    symmetric: bool | None = None
    scale_type: str | None = None
    round_method: str | None = None
    ch_axis: int | None = None
    is_dynamic: bool | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e5m2,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=self.symmetric,
            scale_type=get_scale_type(self.scale_type),
            round_method=get_round_method(self.round_method),
            qscheme=QSchemeType.per_channel,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
        )</div>



<div class="viewcode-block" id="FP8E5M2PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E5M2PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;FP8E5M2 per group quantization&quot;,
        &quot;FP8E5M2PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        group_size=group_size,
        is_dynamic=True
    &quot;&quot;&quot;,
    )
)
class FP8E5M2PerGroupSpec(DataTypeSpec):
    scale_format: str | None = &quot;float32&quot;
    scale_calculation_mode: str | None = None
    ch_axis: int | None = -1
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e5m2,
            observer_cls=PerBlockMXObserver,
            symmetric=None,
            scale_type=ScaleType.float,
            scale_format=self.scale_format,
            scale_calculation_mode=self.scale_calculation_mode,
            round_method=RoundType.half_even,
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="FP4PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP4PerGroupSpec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;FP4 per group quantization&quot;,
        &quot;FP4PerGroupSpec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        group_size=group_size,
        is_dynamic=True
    &quot;&quot;&quot;,
    )
)
class FP4PerGroupSpec(DataTypeSpec):
    scale_format: str | None = &quot;float32&quot;
    scale_calculation_mode: str | None = None
    ch_axis: int | None = -1
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp4,
            observer_cls=PerBlockMXObserver,
            symmetric=None,
            scale_type=ScaleType.float,
            scale_format=self.scale_format,
            scale_calculation_mode=self.scale_calculation_mode,
            round_method=RoundType.half_even,
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="FP6E2M3PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP6E2M3PerGroupSpec">[docs]</a>
@dataclass
class FP6E2M3PerGroupSpec(DataTypeSpec):
    scale_format: str | None = &quot;float32&quot;
    scale_calculation_mode: str | None = None
    ch_axis: int | None = -1
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp6_e2m3,
            observer_cls=PerBlockMXObserver,
            symmetric=None,
            scale_type=ScaleType.float,
            scale_format=self.scale_format,
            scale_calculation_mode=self.scale_calculation_mode,
            round_method=RoundType.half_even,
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="FP6E3M2PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP6E3M2PerGroupSpec">[docs]</a>
@dataclass
class FP6E3M2PerGroupSpec(DataTypeSpec):
    scale_format: str | None = &quot;float32&quot;
    scale_calculation_mode: str | None = None
    ch_axis: int | None = -1
    is_dynamic: bool | None = None
    group_size: int | None = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp6_e3m2,
            observer_cls=PerBlockMXObserver,
            symmetric=None,
            scale_type=ScaleType.float,
            scale_format=self.scale_format,
            scale_calculation_mode=self.scale_calculation_mode,
            round_method=RoundType.half_even,
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=self.group_size,
        )</div>



<div class="viewcode-block" id="Float16Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Float16Spec">[docs]</a>
@dataclass(eq=True)
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;float16 data type. The resulting QuantizationSpec does not quantize the tensor.&quot;, &quot;Float16Spec&quot;, &quot;&quot;
    )
)
class Float16Spec(DataTypeSpec):
    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.float16)</div>



<div class="viewcode-block" id="Bfloat16Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Bfloat16Spec">[docs]</a>
@dataclass(eq=True)
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;bfloat16 data type. The resulting QuantizationSpec does not quantize the tensor.&quot;, &quot;Bfloat16Spec&quot;, &quot;&quot;
    )
)
class Bfloat16Spec(DataTypeSpec):
    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.bfloat16)</div>



<div class="viewcode-block" id="OCP_MXSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXSpec">[docs]</a>
class OCP_MXSpec(DataTypeSpec):
    OCP_MX_SPEC_KWARGS = {
        &quot;observer_cls&quot;: PerBlockMXObserver,
        &quot;symmetric&quot;: None,
        &quot;scale_type&quot;: ScaleType.float,
        &quot;round_method&quot;: RoundType.half_even,
        &quot;scale_format&quot;: &quot;e8m0&quot;,
        &quot;qscheme&quot;: QSchemeType.per_group,
        &quot;group_size&quot;: 32,
    }</div>



<div class="viewcode-block" id="OCP_MXFP8E4M3Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXFP8E4M3Spec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX OCP data type using FP8E4M3&quot;,
        &quot;OCP_MXFP8E4M3Spec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class OCP_MXFP8E4M3Spec(OCP_MXSpec):
    is_dynamic: bool = True
    ch_axis: int = -1
    scale_calculation_mode: str = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e4m3,
            scale_calculation_mode=self.scale_calculation_mode,
            is_dynamic=self.is_dynamic,
            ch_axis=self.ch_axis,
            **self.OCP_MX_SPEC_KWARGS,
        )  # type: ignore[arg-type]</div>



<div class="viewcode-block" id="OCP_MXFP8E5M2Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXFP8E5M2Spec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX OCP data type using FP8E5M2&quot;,
        &quot;OCP_MXFP8E5M2Spec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class OCP_MXFP8E5M2Spec(OCP_MXSpec):
    is_dynamic: bool = True
    ch_axis: int = -1
    scale_calculation_mode: str = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp8_e5m2,
            scale_calculation_mode=self.scale_calculation_mode,
            is_dynamic=self.is_dynamic,
            ch_axis=self.ch_axis,
            **self.OCP_MX_SPEC_KWARGS,
        )  # type: ignore[arg-type]</div>



<div class="viewcode-block" id="OCP_MXFP6E3M2Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXFP6E3M2Spec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX OCP data type using FP6E3M2&quot;,
        &quot;OCP_MXFP6E3M2Spec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class OCP_MXFP6E3M2Spec(OCP_MXSpec):
    is_dynamic: bool = True
    ch_axis: int = -1
    scale_calculation_mode: str = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp6_e3m2,
            scale_calculation_mode=self.scale_calculation_mode,
            is_dynamic=self.is_dynamic,
            ch_axis=self.ch_axis,
            **self.OCP_MX_SPEC_KWARGS,
        )  # type: ignore[arg-type]</div>



<div class="viewcode-block" id="OCP_MXFP6E2M3Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXFP6E2M3Spec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX OCP data type using FP6E2M3&quot;,
        &quot;OCP_MXFP6E2M3Spec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class OCP_MXFP6E2M3Spec(OCP_MXSpec):
    is_dynamic: bool = True
    ch_axis: int = -1
    scale_calculation_mode: str = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp6_e2m3,
            scale_calculation_mode=self.scale_calculation_mode,
            is_dynamic=self.is_dynamic,
            ch_axis=self.ch_axis,
            **self.OCP_MX_SPEC_KWARGS,
        )  # type: ignore[arg-type]</div>



<div class="viewcode-block" id="OCP_MXFP4Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXFP4Spec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX OCP data type using FP4&quot;,
        &quot;OCP_MXFP4Spec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class OCP_MXFP4Spec(OCP_MXSpec):
    is_dynamic: bool = True
    ch_axis: int = -1
    scale_calculation_mode: str = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp4,
            scale_calculation_mode=self.scale_calculation_mode,
            is_dynamic=self.is_dynamic,
            ch_axis=self.ch_axis,
            **self.OCP_MX_SPEC_KWARGS,
        )  # type: ignore[arg-type]</div>



<div class="viewcode-block" id="OCP_MXINT8Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXINT8Spec">[docs]</a>
@dataclass
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX OCP data type using INT8&quot;,
        &quot;OCP_MXINT8Spec&quot;,
        r&quot;&quot;&quot;
        ch_axis=-1,
        is_dynamic=False
    &quot;&quot;&quot;,
    )
)
class OCP_MXINT8Spec(OCP_MXSpec):
    is_dynamic: bool = True
    ch_axis: int = -1
    scale_calculation_mode: str = &quot;even&quot;

    # TODO: support Dtype.int8 in PerBlockMXObserver.
    # Dtype.int8 still uses NonScaledFakeQuantize (see tensor_quantize.py),
    # which it needs not to.
    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.mx,
            mx_element_dtype=Dtype.int8,
            scale_calculation_mode=self.scale_calculation_mode,
            ch_axis=self.ch_axis,
            group_size=32,
        )  # type: ignore[arg-type]</div>



<div class="viewcode-block" id="OCP_MXFP4DiffsSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.OCP_MXFP4DiffsSpec">[docs]</a>
@dataclass
class OCP_MXFP4DiffsSpec(DataTypeSpec):
    ch_axis: int | None = None
    is_dynamic: bool | None = None
    scale_calculation_mode: str | None = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.fp4,
            observer_cls=PerBlockMXDiffsObserver,
            symmetric=None,
            scale_type=ScaleType.float,
            round_method=RoundType.half_even,
            scale_format=&quot;e8m0&quot;,
            scale_calculation_mode=self.scale_calculation_mode,
            qscheme=QSchemeType.per_group,
            ch_axis=self.ch_axis,
            is_dynamic=self.is_dynamic,
            group_size=32,
        )</div>



<div class="viewcode-block" id="MX6Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.MX6Spec">[docs]</a>
@dataclass(eq=True)
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX6 data type as defined in https://arxiv.org/pdf/2302.08007. More details are available in the :doc:`Two Level Quantization Formats &lt;/pytorch/adv_two_level&gt;` documentation&quot;,
        &quot;MX6Spec&quot;,
        &quot;is_dynamic=False&quot;,
    )
)
class MX6Spec(DataTypeSpec):
    ch_axis: int = -1
    block_size: int = 32
    scale_calculation_mode: str | None = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.mx6,
            ch_axis=self.ch_axis,
            group_size=self.block_size,
            scale_calculation_mode=self.scale_calculation_mode,
        )</div>



<div class="viewcode-block" id="MX9Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.MX9Spec">[docs]</a>
@dataclass(eq=True)
@add_start_docstring(
    DATA_TYPE_SPEC_DOCSTRING.format(
        &quot;MX9 data type as defined in https://arxiv.org/pdf/2302.08007. More details are available in the :doc:`Two Level Quantization Formats &lt;/pytorch/adv_two_level&gt;` documentation&quot;,
        &quot;MX9Spec&quot;,
        &quot;is_dynamic=False&quot;,
    )
)
class MX9Spec(DataTypeSpec):
    ch_axis: int = -1
    block_size: int = 32
    scale_calculation_mode: str | None = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.mx9,
            ch_axis=self.ch_axis,
            group_size=self.block_size,
            scale_calculation_mode=self.scale_calculation_mode,
        )</div>



<div class="viewcode-block" id="BFP16Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.BFP16Spec">[docs]</a>
@dataclass
@add_start_docstring(DATA_TYPE_SPEC_DOCSTRING.format(&quot;bfp16 data type&quot;, &quot;BFP16Spec&quot;, &quot;is_dynamic=False&quot;))
class BFP16Spec(DataTypeSpec):
    ch_axis: int = -1
    scale_calculation_mode: str | None = &quot;even&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(
            dtype=Dtype.bfp16, ch_axis=self.ch_axis, group_size=8, scale_calculation_mode=self.scale_calculation_mode
        )</div>



<div class="viewcode-block" id="QuantizationSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QuantizationSpec">[docs]</a>
@dataclass(eq=True)
class QuantizationSpec:
    &quot;&quot;&quot;
    A data class that defines the specifications for quantizing tensors within a model.

    :param Dtype dtype: The data type for quantization (e.g., int8, int4).
    :param Optional[bool] is_dynamic: Specifies whether dynamic or static quantization should be used. Default is ``None``, which indicates no specification.
    :param Optional[Type[ObserverBase]] observer_cls: The class of observer to be used for determining quantization parameters like min/max values. Default is ``None``.
    :param Optional[QSchemeType] qscheme: The quantization scheme to use, such as per_tensor, per_channel or per_group. Default is ``None``.
    :param Optional[int] ch_axis: The channel axis for per-channel quantization. Default is ``None``.
    :param Optional[int] group_size: The size of the group for per-group quantization, also the block size for MX datatypes. Default is ``None``.
    :param Optional[bool] symmetric: Indicates if the quantization should be symmetric around zero. If True, quantization is symmetric. If ``None``, it defers to a higher-level or global setting. Default is ``None``.
    :param Optional[RoundType] round_method: The rounding method during quantization, such as half_even. If None, it defers to a higher-level or default method. Default is ``None``.
    :param Optional[ScaleType] scale_type: Defines the scale type to be used for quantization, like power of two or float. If ``None``, it defers to a higher-level setting or uses a default method. Default is ``None``.
    :param Optional[Dtype] mx_element_dtype: Defines the data type to be used for the element type when using mx datatypes, the shared scale effectively uses FP8 E8M0.
    :param Optional[bool] is_scale_quant: Indicates whether this spec is for quantizing scales rather than tensors. Default is ``False``.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.type import Dtype, ScaleType, RoundType, QSchemeType
        from quark.torch.quantization.config.config import QuantizationSpec
        from quark.torch.quantization.observer.observer import PerChannelMinMaxObserver

        quantization_spec = QuantizationSpec(
            dtype=Dtype.int8,
            qscheme=QSchemeType.per_channel,
            observer_cls=PerChannelMinMaxObserver,
            symmetric=True,
            scale_type=ScaleType.float,
            round_method=RoundType.half_even,
            is_dynamic=False,
            ch_axis=1,
        )
    &quot;&quot;&quot;

    ###################################################################################################
    # Quantization Specification for Dtype in [Bfloat16, FP8, Int, MX]

    dtype: Dtype

    observer_cls: type[ObserverBase] | None = None

    ###################################################################################################
    # Quantization Specification for Dtype in [FP8, Int, MX]

    is_dynamic: bool | None = None

    qscheme: QSchemeType | None = None

    ch_axis: int | None = None

    group_size: int | None = None

    ###################################################################################################
    # Quantization Specification for Dtype in [Int]

    symmetric: bool | None = None

    round_method: RoundType | None = None

    scale_type: ScaleType | None = None

    scale_format: str | None = None

    scale_calculation_mode: str | None = None

    qat_spec: QATSpec | None = None
    ###################################################################################################
    # Quantization Specification for Dtype in [MX]
    mx_element_dtype: Dtype | None = None
    ###################################################################################################
    # Quantization zero point Specification for Dtype
    zero_point_type: ZeroPointType | None = ZeroPointType.int32

    ###################################################################################################
    # Indicates whether this spec is for quantizing scales rather than tensors
    is_scale_quant: bool = False

    def __post_init__(self) -&gt; None:
        &quot;&quot;&quot;
        When the user init a QuantizationSpec, we need to check whether the config is valid.
        for example:
            1. observer_cls -&gt; PerTensorPowOf2MinMSEObserver
            2. qscheme      -&gt; QSchemeType.per_channel

        For the above config, the `per_channel` is in conflict with PerTensorPowOf2MinMSEObserver
        Target:
            Once user config a Config like above that contains any conflict, we need to \
                throw an exception and tell the user what the conflict is.
        &quot;&quot;&quot;
        # NOTE: for developers, every time a new dtype is added, please add the corresponding check for the new dtype.
        if self.dtype not in ALL_DATA_TYPES:
            raise ValueError(f&quot;The value dtype={self.dtype} is not among the supported dtypes {ALL_DATA_TYPES}.&quot;)

        # NOTE: for developers, every time a new observer is added, please add the corresponding check for the new observer.
        if self.observer_cls is not None and self.observer_cls not in OBSERVER_CLASSES:
            raise ValueError(
                f&quot;The value observer_cls={self.observer_cls} is not among the supported observer_cls: {OBSERVER_CLASSES}.&quot;
            )

        if self.dtype in [
            Dtype.int8,
            Dtype.uint8,
            Dtype.int16,
            Dtype.uint16,
            Dtype.int4,
            Dtype.uint4,
            Dtype.int3,
            Dtype.int2,
            Dtype.int32,
            Dtype.fp8_e4m3,
            Dtype.fp8_e5m2,
        ]:
            if self.is_dynamic is None:
                raise ValueError(
                    f&quot;The field `is_dynamic` cannot be None when Dtype is {self.dtype.name} in QuantizationSpec.&quot;
                )
            if self.observer_cls is None:
                raise ValueError(
                    f&quot;The field `observer_cls` cannot be None when Dtype is {self.dtype.name} in QuantizationSpec.&quot;
                )
            if self.qscheme is None:
                raise ValueError(
                    f&quot;The field `qscheme` cannot be None when Dtype is {self.dtype.name} in QuantizationSpec. Please reconfigure the quantization settings accordingly.&quot;
                )
        if self.dtype in [
            Dtype.int8,
            Dtype.uint8,
            Dtype.int16,
            Dtype.uint16,
            Dtype.int4,
            Dtype.uint4,
            Dtype.int3,
            Dtype.int32,
        ]:
            if self.symmetric is None:
                raise ValueError(
                    f&quot;The field `symmetric` cannot be None when Dtype is {self.dtype.name} in QuantizationSpec. Please reconfigure the quantization settings accordingly.&quot;
                )
            if self.round_method is None:
                raise ValueError(
                    f&quot;The field `round_method` cannot be None when Dtype is {self.dtype.name} in QuantizationSpec. Please reconfigure the quantization settings accordingly.&quot;
                )
            if self.scale_type is None:
                raise ValueError(
                    f&quot;The field `scale_type` cannot be None when Dtype is {self.dtype.name} in QuantizationSpec. Please reconfigure the quantization settings accordingly.&quot;
                )

        # CASE 1:  will only init NonScaledFakeQuantize and PlaceholderObserver,
        #   NOTE: quark/torch/quantization/tensor_quantize.py FakeQuantizeBase:get_fake_quantize
        #   in this case will using NonScaledFakeQuantize, and only init PlaceholderObserver()
        #   As a results, quant forward func: quark.torch.kernel.non_scaled_fake_quantize
        #   /torch/kernel/hw_emulation/hw_emulation_interface.py: def non_scaled_fake_quantize
        if self.dtype in USING_NON_SCALED_QUANT:
            # 1.In quark/torch/kernel/__init__.py: class NonScaledFakeQuantizeFunction
            #   During quantization: the following needed
            #    1.input_tensor 2.quant_dtype 2.mx_element_dtype 3.axis 4.block_size 5.scale_calculation_mode needed
            # 2.In init PlaceholderObserver, only qspec.dtype needed
            # Summary for QuantizationSpec: 1.dtype(r) 2.mx_element_dtype(o) 3.axis(r) 4.group_size(r) 5.scale_calculation_mode(o)

            required_fields = [&quot;dtype&quot;, &quot;ch_axis&quot;, &quot;group_size&quot;]
            oprional_fields = [&quot;mx_element_dtype&quot;, &quot;scale_calculation_mode&quot;]

            if self.ch_axis is None:
                raise ValueError(
                    f&quot;When using dtype={self.dtype}, quantization_spec.ch_axis must be specified. Got `ch_axis=None`.&quot;
                )

            if self.group_size is None:
                raise ValueError(
                    f&quot;When using dtype={self.dtype}, quantization_spec.group_size must be specified. Got `group_size=None`.&quot;
                )

            if self.dtype == Dtype.mx and self.mx_element_dtype is None:
                raise ValueError(
                    f&quot;When using dtype={self.dtype}, quantization_spec.mx_element_dtype must be specified. Got `mx_element_dtype=None`.&quot;
                )

            for each_field in fields(self):
                if each_field.name not in required_fields + oprional_fields:
                    value = getattr(self, each_field.name)
                    default_value = each_field.default
                    if value != default_value:
                        logger.warning(
                            f&quot;When using dtype={self.dtype}, QuantizationSpec.{each_field.name} will not take effect. Got {each_field.name}={value} but the default is {each_field.name}={default_value}.&quot;
                        )
            return

        # CASE 2: # NOTE only quantization_spec.dtype is needed
        # in quantization actually call: fake_quantize_with_dtype_convert
        if self.dtype in ONLY_DTYPE_CHANGE:
            required_fields = [&quot;dtype&quot;]
            for each_field in fields(self):
                if each_field.name not in required_fields:
                    value = getattr(self, each_field.name)
                    default_value = each_field.default
                    if value != default_value:
                        logger.warning(
                            f&quot;In {self.dtype} quant, QuantizationSpec.{each_field.name} will not take effect. User supplied: {value} User should skip setting this field&quot;
                        )

            return

        # CASE 3
        # NOTE: quark/torch/quantization/tensor_quantize.py FakeQuantizeBase:get_fake_quantize
        # we will init ScaledFakeQuantize and the corresponding observer
        # def scaled_fake_quantize, in Quark/quark/torch/kernel/hw_emulation/hw_emulation_interface.py
        #    1. fake_quantize_int:      qscheme, axis (if channel/group), group_size, round_mode
        #    2. fake_quantize_fp8_e4m3: qscheme, axis (if channel/group), group_size
        #    3. fake_quantize_fp8_e5m2: qscheme, axis (if channel/group), group_size
        #    4. fake_quantize_fp4_fp6:  qscheme, axis (if channel/group), group_size, quant_dtype(channel/group)
        assert self.observer_cls is not None, &quot;Supplied QuantizationSpec&#39;s observer_cls is None&quot;
        assert self.qscheme is not None, &quot;Supplied QuantizationSpec&#39;s qscheme is None&quot;

        if self.qscheme == QSchemeType.per_tensor:
            assert self.observer_cls in PER_TENSOR_OBSERVERS, (
                f&quot;You select Tensor wise quant, the observer_cls you select is {self.observer_cls} not support tesnor wise quant.&quot;
            )

        elif self.qscheme == QSchemeType.per_channel:
            assert self.observer_cls in PER_CHANNEL_OBSERVERS, (
                f&quot;You select channel wise quant, the observer_cls you select is {self.observer_cls} not support channel wise quant.&quot;
            )
            assert isinstance(self.ch_axis, int), (
                &quot;You select channel wise quant, user must assigned int num to ch_axis.&quot;
            )

        elif self.qscheme == QSchemeType.per_group:
            assert self.observer_cls in PER_GROUP_OBSERVERS, (
                f&quot;You select block wise quant, the observer_cls you select is {self.observer_cls} not support block wise quant.&quot;
            )
            assert isinstance(self.ch_axis, int), (
                &quot;You select block/channel wise quant, user must assigned int num to ch_axis.&quot;
            )
            assert isinstance(self.group_size, int), (
                &quot;You select block/channel wise quant, user must assigned int num to group_size.&quot;
            )
        else:  # NOTE for developer
            raise ModuleNotFoundError(
                f&quot;Please decide {self.observer_cls.__name__} belongs to which kind of quant (tensor/channel/group).&quot;
            )

    def set_group_size(self, group_size: int) -&gt; None:
        assert isinstance(group_size, int) and (group_size &gt; 0 or group_size == -1), (
            &quot;Group size must be a positive integer or -1 (which means group size equals to dimension size).&quot;
        )
        self.group_size = group_size

    def to_dict(self) -&gt; dict[str, Any]:
        # TODO: qat_spec, mx_element_dtype missing.
        return {
            &quot;dtype&quot;: self.dtype.name,
            &quot;is_dynamic&quot;: self.is_dynamic,
            &quot;qscheme&quot;: self.qscheme.name if self.qscheme is not None else None,
            &quot;ch_axis&quot;: self.ch_axis,
            &quot;group_size&quot;: self.group_size,
            &quot;symmetric&quot;: self.symmetric,
            &quot;round_method&quot;: self.round_method.name if self.round_method is not None else None,
            &quot;scale_type&quot;: self.scale_type.name if self.scale_type is not None else None,
            &quot;scale_format&quot;: self.scale_format,
            &quot;scale_calculation_mode&quot;: self.scale_calculation_mode,
            &quot;mx_element_dtype&quot;: self.mx_element_dtype.name if self.mx_element_dtype is not None else None,
            &quot;observer_cls&quot;: self.observer_cls.__name__ if self.observer_cls is not None else None,
            &quot;is_scale_quant&quot;: self.is_scale_quant,
        }

    @classmethod
    def from_dict(cls, config_dict: dict[str, Any] | None) -&gt; QuantizationSpec | None:
        if config_dict is None:
            return None

        dtype = Dtype[config_dict[&quot;dtype&quot;]]

        if config_dict.get(&quot;mx_element_dtype&quot;, None) is not None:
            mx_element_dtype = Dtype[config_dict[&quot;mx_element_dtype&quot;]]
        else:
            mx_element_dtype = None

        if config_dict.get(&quot;qscheme&quot;, None) is not None:
            qscheme = QSchemeType[config_dict[&quot;qscheme&quot;]]
        else:
            qscheme = None

        if config_dict.get(&quot;round_method&quot;, None) is not None:
            round_method = RoundType[config_dict[&quot;round_method&quot;]]
        else:
            round_method = None

        if config_dict.get(&quot;scale_type&quot;, None) is not None:
            scale_type = ScaleType[config_dict[&quot;scale_type&quot;]]
        else:
            scale_type = None

        if config_dict.get(&quot;scale_format&quot;, None) is not None:
            scale_format = config_dict[&quot;scale_format&quot;]
        else:
            scale_format = None

        if config_dict.get(&quot;scale_calculation_mode&quot;, None) is not None:
            scale_calculation_mode = config_dict[&quot;scale_calculation_mode&quot;]
        else:
            scale_calculation_mode = None

        # TODO: Deprecate legacy configuration.
        # Accomodate the legacy (quark&lt;1.0) export which used custom keys.
        is_dynamic = config_dict[&quot;is_dynamic&quot;] if &quot;is_dynamic&quot; in config_dict else config_dict[&quot;dynamic&quot;]
        ch_axis = config_dict[&quot;ch_axis&quot;] if &quot;ch_axis&quot; in config_dict else config_dict[&quot;axis&quot;]

        group_size = config_dict[&quot;group_size&quot;]
        symmetric = config_dict[&quot;symmetric&quot;]

        if &quot;observer_cls&quot; in config_dict:
            if config_dict[&quot;observer_cls&quot;] in OBSERVER_MAP:
                observer_cls = OBSERVER_MAP[config_dict[&quot;observer_cls&quot;]]
            else:  # pragma: no cover
                logger.warning(
                    f&quot;Unknown observer_cls={config_dict[&#39;observer_cls&#39;]}. Loading the QuantizationSpec with observer_cls=PlaceholderObserver.&quot;
                )
                observer_cls = PlaceholderObserver
        else:  # pragma: no cover
            # quark&lt;1.0 used not to save the `observer_cls` in `QuantizationSpec.to_dict()`.
            observer_cls = PlaceholderObserver

        is_scale_quant = config_dict.get(&quot;is_scale_quant&quot;, False)

        return cls(
            dtype=dtype,
            is_dynamic=is_dynamic,
            qscheme=qscheme,
            ch_axis=ch_axis,
            group_size=group_size,
            symmetric=symmetric,
            round_method=round_method,
            scale_type=scale_type,
            scale_format=scale_format,
            scale_calculation_mode=scale_calculation_mode,
            mx_element_dtype=mx_element_dtype,
            observer_cls=observer_cls,  # type: ignore[arg-type]
            is_scale_quant=is_scale_quant,
        )</div>



<div class="viewcode-block" id="QATSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QATSpec">[docs]</a>
@dataclass
class QATSpec(ConfigBase):
    pass</div>



<div class="viewcode-block" id="TQTSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.TQTSpec">[docs]</a>
@dataclass
class TQTSpec(QATSpec):
    &quot;&quot;&quot;
    Configuration for the Trained Quantization Thresholds (TQT) post-training quantization method, implementing https://arxiv.org/abs/1903.08066.
    &quot;&quot;&quot;

    threshold_init_meth: TQTThresholdInitMeth | None = None</div>



<div class="viewcode-block" id="load_pre_optimization_config_from_file">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.load_pre_optimization_config_from_file">[docs]</a>
def load_pre_optimization_config_from_file(file_path: str) -&gt; PreQuantOptConfig:
    &quot;&quot;&quot;
    Load pre-optimization configuration from a JSON file.

    :param file_path: The path to the JSON file containing the pre-optimization configuration.
    :type file_path: str
    :return: The pre-optimization configuration.
    :rtype: PreQuantOptConfig
    &quot;&quot;&quot;
    with open(file_path) as file:
        algo_config_info = json.load(file)
    return _load_pre_optimization_config_from_dict(algo_config_info)</div>



<div class="viewcode-block" id="load_quant_algo_config_from_file">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.load_quant_algo_config_from_file">[docs]</a>
def load_quant_algo_config_from_file(file_path: str) -&gt; AlgoConfig:
    &quot;&quot;&quot;
    Load quantization algorithm configuration from a JSON file.

    :param file_path: The path to the JSON file containing the quantization algorithm configuration.
    :type file_path: str
    :return: The quantization algorithm configuration.
    :rtype: AlgoConfig
    &quot;&quot;&quot;
    with open(file_path) as file:
        algo_config_info = json.load(file)
    return _load_quant_algo_config_from_dict(algo_config_info)</div>



def _load_pre_optimization_config_from_dict(pre_optimization_config_dict: dict[str, Any]) -&gt; PreQuantOptConfig:
    &quot;&quot;&quot;
    Load pre-optimization configuration from a dictionary.

    :param pre_optimization_config_dict: A dictionary containing the pre-optimization configuration.
    :type pre_optimization_config_dict: Dict[str, Any]
    :return: The pre-optimization configuration.
    :rtype: PreQuantOptConfig
    :raises ValueError: If the configuration name is not recognized.
    &quot;&quot;&quot;
    # Deprecate old settings for GQA
    pre_optimization_config_dict.pop(&quot;num_attention_heads&quot;, None)
    pre_optimization_config_dict.pop(&quot;num_key_value_heads&quot;, None)

    if pre_optimization_config_dict[&quot;name&quot;] == &quot;rotation&quot;:
        return cast(PreQuantOptConfig, RotationConfig.from_dict(pre_optimization_config_dict))
    elif pre_optimization_config_dict[&quot;name&quot;] == &quot;quarot&quot;:
        return cast(PreQuantOptConfig, QuaRotConfig.from_dict(pre_optimization_config_dict))
    elif pre_optimization_config_dict[&quot;name&quot;] == &quot;smooth&quot;:
        return cast(PreQuantOptConfig, SmoothQuantConfig.from_dict(pre_optimization_config_dict))
    else:
        raise ValueError(f&quot;Unknown algorithm name {pre_optimization_config_dict[&#39;name&#39;]}&quot;)


def _load_quant_algo_config_from_dict(algo_config_dict: dict[str, Any]) -&gt; AlgoConfig:
    &quot;&quot;&quot;
    Load quantization algorithm configuration from a dictionary.

    :param algo_config_dict: A dictionary containing the quantization algorithm configuration.
    :type algo_config_dict: Dict[str, Any]
    :return: The quantization algorithm configuration.
    :rtype: AlgoConfig
    :raises ValueError: If the configuration name is not recognized.
    &quot;&quot;&quot;
    # Deprecate old settings for GQA
    algo_config_dict.pop(&quot;num_attention_heads&quot;, None)
    algo_config_dict.pop(&quot;num_key_value_heads&quot;, None)

    if algo_config_dict[&quot;name&quot;] == &quot;rotation&quot;:
        return cast(AlgoConfig, RotationConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;quarot&quot;:
        return cast(AlgoConfig, QuaRotConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;smooth&quot;:
        return cast(AlgoConfig, SmoothQuantConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;awq&quot;:
        return cast(AlgoConfig, AWQConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;gptq&quot;:  # pragma: no cover
        return cast(AlgoConfig, GPTQConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;autosmoothquant&quot;:  # pragma: no cover:
        return cast(AlgoConfig, AutoSmoothQuantConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;qronos&quot;:  # pragma: no cover
        return cast(AlgoConfig, QronosConfig.from_dict(algo_config_dict))
    else:
        raise ValueError(f&quot;Unknown algorithm name {algo_config_dict[&#39;name&#39;]}&quot;)


<div class="viewcode-block" id="AlgoConfigBase">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AlgoConfigBase">[docs]</a>
@dataclass
class AlgoConfigBase(ConfigBase):
    pass</div>



<div class="viewcode-block" id="PreQuantOptConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.PreQuantOptConfig">[docs]</a>
@dataclass
class PreQuantOptConfig(AlgoConfigBase):
    pass</div>



<div class="viewcode-block" id="AlgoConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AlgoConfig">[docs]</a>
@dataclass
class AlgoConfig(AlgoConfigBase):
    pass</div>



<div class="viewcode-block" id="SmoothQuantConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.SmoothQuantConfig">[docs]</a>
@dataclass
class SmoothQuantConfig(AlgoConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for Smooth Quantization.

    :param str name: The name of the configuration, typically used to identify different quantization settings. Default is ``&quot;smooth&quot;``.
    :param int alpha: The factor of adjustment in the quantization formula, influencing how aggressively weights are quantized. Default is ``1``.
    :param float scale_clamp_min: The minimum scaling factor to be used during quantization, preventing the scale from becoming too small. Default is ``1e-3``.
    :param List[Dict[str, Any]] scaling_layers: Specific settings for scaling layers, allowing customization of quantization parameters for different layers within the model. Default is ``None``.
    :param str model_decoder_layers: Specifies any particular decoder layers in the model that might have unique quantization requirements. Default is ``None``.

    The parameter ``scaling_layers`` can be left to an empty list (default), in which case they will be automatically detected.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.config import SmoothQuantConfig

        scaling_layers=[
            {
                &quot;prev_op&quot;: &quot;input_layernorm&quot;,
                &quot;layers&quot;: [&quot;self_attn.q_proj&quot;, &quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;],
                &quot;inp&quot;: &quot;self_attn.q_proj&quot;,
                &quot;module2inspect&quot;: &quot;self_attn&quot;
            },
            {
                &quot;prev_op&quot;: &quot;post_attention_layernorm&quot;,
                &quot;layers&quot;: [&quot;mlp.gate_proj&quot;, &quot;mlp.up_proj&quot;],
                &quot;inp&quot;: &quot;mlp.gate_proj&quot;,
                &quot;module2inspect&quot;: &quot;mlp&quot;
            }
        ]

        smoothquant_config = SmoothQuantConfig(
            scaling_layers=scaling_layers,
            model_decoder_layers=&quot;model.layers&quot;
        )
    &quot;&quot;&quot;

    name: str = &quot;smooth&quot;
    alpha: float = 1
    scale_clamp_min: float = 1e-3
    scaling_layers: list[dict[str, Any]] = field(default_factory=list)
    model_decoder_layers: str = &quot;&quot;</div>



<div class="viewcode-block" id="RotationConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.RotationConfig">[docs]</a>
@dataclass
class RotationConfig(AlgoConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for rotation settings in processing algorithms.

    :param str name: The name of the configuration, typically used to identify different rotation settings. Default is ``&quot;rotation&quot;``.
    :param bool random: A boolean flag indicating whether the rotation should be applied randomly. This can be useful for data augmentation purposes where random rotations may be required. Default is ``False``.
    :param List[Dict[str, Any]] scaling_layers: Specific settings for scaling layers, allowing customization of quantization parameters for different layers within the model. Default is ``[]``.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.config import RotationConfig

        scaling_layers=[
            {
                &quot;prev_op&quot;: &quot;input_layernorm&quot;,
                &quot;layers&quot;: [&quot;self_attn.q_proj&quot;, &quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;],
                &quot;inp&quot;: &quot;self_attn.q_proj&quot;,
                &quot;module2inspect&quot;: &quot;self_attn&quot;
            },
            {
                &quot;prev_op&quot;: &quot;post_attention_layernorm&quot;,
                &quot;layers&quot;: [&quot;mlp.gate_proj&quot;, &quot;mlp.up_proj&quot;],
                &quot;inp&quot;: &quot;mlp.gate_proj&quot;,
                &quot;module2inspect&quot;: &quot;mlp&quot;
            }
        ]

        rotation_config = RotationConfig(
            scaling_layers=scaling_layers,
            model_decoder_layers=&quot;model.layers&quot;
        )
    &quot;&quot;&quot;

    model_decoder_layers: str
    scaling_layers: dict[str, list[dict[str, Any]]]
    name: str = &quot;rotation&quot;
    random: bool = False</div>



<div class="viewcode-block" id="QuaRotConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QuaRotConfig">[docs]</a>
@dataclass
class QuaRotConfig(AlgoConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for the QuaRot algorithm.

    :param str name: The name of the configuration, typically used to identify different rotation settings. Default is ``&quot;quarot&quot;``.
    :param bool r1: Whether to apply ``R1`` rotation. See `SpinQuant paper &lt;https://arxiv.org/abs/2405.16406&gt;`__ for details. Defaults to ``True``.
    :param bool r2: Whether to apply ``R2`` rotation. See `SpinQuant paper &lt;https://arxiv.org/abs/2405.16406&gt;`__ for details. Defaults to ``True``.
    :param bool r3: Whether to apply ``R3`` rotation. It is only useful when using KV cache quantization. See `SpinQuant paper &lt;https://arxiv.org/abs/2405.16406&gt;`__ for details. Defaults to ``True``.
    :param bool r4: Whether to apply ``R4`` rotation. See `SpinQuant paper &lt;https://arxiv.org/abs/2405.16406&gt;`__ for details. Defaults to ``True``.
    :param Optional[int] rotation_size: The size of rotations to apply on activations/weights. By default, the activation last dimension (e.g. ``hidden_size``), or weight input/output channel dimension is used as rotation size. In case the parameter ``rotation_size`` is specified, smaller rotations of size ``(rotation_size, rotation_size)`` are applied per-block. Defaults to ``None``.
    :param bool random_r1: A boolean flag indicating whether ``R1`` should be a random Hadamard matrix. See `SpinQuant paper &lt;https://arxiv.org/abs/2405.16406&gt;`__ for details. This can be useful for data augmentation purposes where random rotations may be required. Default is ``False``.
    :param bool random_r2: A boolean flag indicating whether ``R2`` should be a random Hadamard matrix. See `SpinQuant paper &lt;https://arxiv.org/abs/2405.16406&gt;`__ for details. This can be useful for data augmentation purposes where random rotations may be required. Default is ``False``. ``random_r1`` and ``random_r2`` are only relevant if we are using Hadamard rotations for ``R1`` and ``R2``. If the argument ``optimized_rotation_path`` is specified, then we will load ``R1`` and ``R2`` matrices from a file instad of using Hadamard matrices.
    :param List[Dict[str, str]] scaling_layers: Specific settings for scaling layers, allowing customization of quantization parameters for different layers within the model. Default is ``None``.
    :param Optional[str] optimized_rotation_path: The path to the file &#39;R.bin&#39; that has saved optimized ``R1`` (per model) and ``R2`` (per decoder) matrices. If this is specified, ``R1`` and ``R2`` rotations will be loaded from this file. Otherwise they will be Hadamard matrices.
    :param str backbone: A string indicating the path to the model backbone.
    :param str model_decoder_layers: A string indicating the path to the list of decoder layers.
    :param str v_proj: A string indicating the path to the v projection layer, starting from the decoder layer it is in.
    :param str o_proj: A string indicating the path to the o projection layer, starting from the decoder layer it is in.
    :param str self_attn: A string indicating the path to the self attention block, starting from the decoder layer it is in.
    :param str mlp: A string indicating the path to the multilayer perceptron layer, starting from the decoder layer it is in.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.config import QuaRotConfig

        quarot_config = QuaRotConfig(
            model_decoder_layers=&quot;model.layers&quot;,
            v_proj=&quot;self_attn.v_proj&quot;,
            o_proj=&quot;self_attn.o_proj&quot;,
            self_attn=&quot;self_attn&quot;,
            mlp=&quot;mlp&quot;
        )
    &quot;&quot;&quot;

    scaling_layers: dict[str, list[dict[str, Any]]]
    name: str = &quot;quarot&quot;
    r1: bool = True
    r2: bool = True
    r3: bool = True
    r4: bool = True
    rotation_size: bool | None = None
    random_r1: bool = False
    random_r2: bool = False
    optimized_rotation_path: str | None = None
    backbone: str = &quot;model&quot;
    model_decoder_layers: str = &quot;model.layers&quot;
    v_proj: str = &quot;self_attn.v_proj&quot;
    o_proj: str = &quot;self_attn.o_proj&quot;
    self_attn: str = &quot;self_attn&quot;
    mlp: str = &quot;mlp&quot;

    def __post_init__(self) -&gt; None:
        if (self.random_r1 or self.random_r2) and self.rotation_size is not None:
            raise NotImplementedError(
                f&quot;random_r1=True or random_r2=True along with a custom rotation_size={self.rotation_size} is not supported at the moment in QuaRotConfig. Please open an issue.&quot;
            )

        if self.optimized_rotation_path is not None and self.rotation_size is not None:
            raise NotImplementedError(
                f&quot;Using a preset optimized_rotation_path={self.optimized_rotation_path} along with a custom rotation_size={self.rotation_size} is not supported. Please open an issue.&quot;
            )</div>



<div class="viewcode-block" id="AutoSmoothQuantConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AutoSmoothQuantConfig">[docs]</a>
@dataclass
class AutoSmoothQuantConfig(AlgoConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for AutoSmoothQuant.

    :param str name: The name of the quantization configuration. Default is ``&quot;autosmoothquant&quot;``.
    :param List[Dict[str, str]] scaling_layers: Configuration details for scaling layers within the model, specifying custom scaling parameters per layer. Default is ``None``.
    :param str compute_scale_loss: Calculate the best scale loss, &quot;MSE&quot; or &quot;MAE&quot;. Default is ``&quot;MSE&quot;``.
    :param str model_decoder_layers: Specifies the layers involved in model decoding that may require different quantization parameters. Default is ``None``.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.config import AutoSmoothQuantConfig

        scaling_layers = [
            {
                &quot;prev_op&quot;: &quot;input_layernorm&quot;,
                &quot;layers&quot;: [&quot;self_attn.q_proj&quot;, &quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;],
                &quot;inp&quot;: &quot;self_attn.q_proj&quot;,
                &quot;module2inspect&quot;: &quot;self_attn&quot;
            },
            {
                &quot;prev_op&quot;: &quot;self_attn.v_proj&quot;,
                &quot;layers&quot;: [&quot;self_attn.o_proj&quot;],
                &quot;inp&quot;: &quot;self_attn.o_proj&quot;
            },
            {
                &quot;prev_op&quot;: &quot;post_attention_layernorm&quot;,
                &quot;layers&quot;: [&quot;mlp.gate_proj&quot;, &quot;mlp.up_proj&quot;],
                &quot;inp&quot;: &quot;mlp.gate_proj&quot;,
                &quot;module2inspect&quot;: &quot;mlp&quot;
            },
            {
                &quot;prev_op&quot;: &quot;mlp.up_proj&quot;,
                &quot;layers&quot;: [&quot;mlp.down_proj&quot;],
                &quot;inp&quot;: &quot;mlp.down_proj&quot;
            }
        ]

        autosmoothquant_config = AutoSmoothQuantConfig(
            model_decoder_layers=&quot;model.layers&quot;,
            scaling_layers=scaling_layers
        )
    &quot;&quot;&quot;

    name: str = &quot;autosmoothquant&quot;
    scaling_layers: list[dict[str, Any]] | None = None
    model_decoder_layers: str | None = None
    compute_scale_loss: str | None = &quot;MSE&quot;</div>



<div class="viewcode-block" id="AWQConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AWQConfig">[docs]</a>
@dataclass
class AWQConfig(AlgoConfig):
    &quot;&quot;&quot;
    Configuration for Activation-aware Weight Quantization (AWQ).

    :param str name: The name of the quantization configuration. Default is ``&quot;awq&quot;``.
    :param List[Dict[str, Any]] scaling_layers: Configuration details for scaling layers within the model, specifying custom scaling parameters per layer. Default is ``None``.
    :param str model_decoder_layers: Specifies the layers involved in model decoding that may require different quantization parameters. Default is ``None``.

    The parameter ``scaling_layers`` can be left to an empty list (default), in which case they will be automatically detected.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.config import AWQConfig

        scaling_layers = [
            {
                &quot;prev_op&quot;: &quot;input_layernorm&quot;,
                &quot;layers&quot;: [&quot;self_attn.q_proj&quot;, &quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;],
                &quot;inp&quot;: &quot;self_attn.q_proj&quot;,
                &quot;module2inspect&quot;: &quot;self_attn&quot;
            },
            {
                &quot;prev_op&quot;: &quot;post_attention_layernorm&quot;,
                &quot;layers&quot;: [&quot;mlp.gate_proj&quot;, &quot;mlp.up_proj&quot;],
                &quot;inp&quot;: &quot;mlp.gate_proj&quot;,
                &quot;module2inspect&quot;: &quot;mlp&quot;
            },
        ]

        awq_config = AWQConfig(
            model_decoder_layers=&quot;model.layers&quot;,
            scaling_layers=scaling_layers
        )
    &quot;&quot;&quot;

    name: str = &quot;awq&quot;
    scaling_layers: list[dict[str, Any]] = field(default_factory=list)
    model_decoder_layers: str = field(default_factory=str)</div>



<div class="viewcode-block" id="GPTQConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.GPTQConfig">[docs]</a>
@dataclass
class GPTQConfig(AlgoConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for Accurate Post-Training Quantization for Generative Pre-trained Transformers (GPTQ).

    :param str name: The configuration name. Default is ``&quot;gptq&quot;``.
    :param int block_size: GPTQ divides the columns into blocks of size block_size and quantizes each block separately. Default is ``128``.
    :param float damp_percent: The percentage used to dampen the quantization effect, aiding in the maintenance of accuracy post-quantization. Default is ``0.01``.
    :param bool desc_act: Indicates whether descending activation is used, typically to enhance model performance with quantization. Default is ``True``.
    :param bool static_groups: Specifies whether the order of groups for quantization are static or can be dynamically adjusted. Default is ``True``. Quark export only support static_groups as True.
    :param List[str] inside_layer_modules: Lists the names of internal layer modules within the model that require specific quantization handling. Default is ``None``.
    :param str model_decoder_layers: Specifies custom settings for quantization on specific decoder layers of the model. Default is ``None``.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.config import GPTQConfig

        gptq_config = GPTQConfig(
            inside_layer_modules=[
                &quot;self_attn.k_proj&quot;,
                &quot;self_attn.v_proj&quot;,
                &quot;self_attn.q_proj&quot;,
                &quot;self_attn.o_proj&quot;,
                &quot;mlp.up_proj&quot;,
                &quot;mlp.gate_proj&quot;,
                &quot;mlp.down_proj&quot;
            ],
            model_decoder_layers=&quot;model.layers&quot;
        )

    &quot;&quot;&quot;

    name: str = &quot;gptq&quot;
    block_size: int = 128
    damp_percent: float = 0.01
    desc_act: bool = True
    static_groups: bool = True
    inside_layer_modules: list[str] = field(default_factory=list)
    model_decoder_layers: str = field(default_factory=str)

    def __post_init__(self) -&gt; None:
        if self.desc_act and not self.static_groups:
            raise ValueError(
                &quot;AMD Quark does not support using GPTQ with `desc_act=True` and `static_groups=False`. Please use `static_groups=True`, or disable `desc_act`.&quot;
            )</div>



<div class="viewcode-block" id="QronosConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QronosConfig">[docs]</a>
@dataclass
class QronosConfig(AlgoConfig):
    &quot;&quot;&quot;
    Configuration for Qronos, an advanced post-training quantization algorithm. Implemented as proposed in https://arxiv.org/pdf/2505.11695

    :param List[str] inside_layer_modules: Lists the names of internal layer modules within the model that require specific quantization handling.
    :param str model_decoder_layers: Specifies custom settings for quantization on specific decoder layers of the model.
    :param str name: The configuration name. Default is ``&quot;qronos&quot;``.
    :param int block_size: Qronos divides the columns into blocks of size block_size and quantizes each block separately. Default is ``128``.
    :param bool desc_act: Indicates whether descending activation is used, typically to enhance model performance with quantization. Default is ``True``.
    :param bool static_groups: Specifies whether the order of groups for quantization are static or can be dynamically adjusted. Default is ``True``. Quark export only supports ``static_groups=True``.
    :param float alpha: Dampening factor for numerical stability during matrix inversions. Default is ``1e-6``.
    :param float beta: Stabilisation factor for Cholesky decomposition. Default is ``1e4``.

    Example:

    .. code-block:: python

        from quark.torch.quantization.config.config import QronosConfig

        qronos_config = QronosConfig(
            inside_layer_modules=[
                &quot;self_attn.k_proj&quot;,
                &quot;self_attn.v_proj&quot;,
                &quot;self_attn.q_proj&quot;,
                &quot;self_attn.o_proj&quot;,
                &quot;mlp.up_proj&quot;,
                &quot;mlp.gate_proj&quot;,
                &quot;mlp.down_proj&quot;
            ],
            model_decoder_layers=&quot;model.layers&quot;
        )
    &quot;&quot;&quot;

    inside_layer_modules: list[str]
    model_decoder_layers: str
    name: str = &quot;qronos&quot;
    block_size: int = 128
    desc_act: bool = True
    static_groups: bool = True
    alpha: float = 1e-3
    beta: float = 1e4

    def __post_init__(self) -&gt; None:
        if self.desc_act and not self.static_groups:
            raise ValueError(
                &quot;AMD Quark does not support using Qronos with `desc_act=True` and `static_groups=False`. Please use `static_groups=True`.&quot;
            )

        if self.block_size &lt;= 0:
            raise ValueError(f&quot;Number of blocks must be positive, got {self.block_size}.&quot;)</div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Sep 26, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2025 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>