
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Full List of Quantization Configuration Features &#8212; AMD Quark 0.10 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a66ef196" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_header.css?v=9557e3d1" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_footer.css?v=7095035a" />
    <link rel="stylesheet" type="text/css" href="../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e0f31c2e"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script defer="defer" src="../_static/search.js?v=90a4452c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'onnx/appendix_full_quant_config_features';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Calibration Methods" href="config/calibration_methods.html" />
    <link rel="prev" title="Configuring ONNX Quantization" href="user_guide_config_description.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].innerText = "Show code ▼\n\n";

            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">AMD Quark 0.10 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage.html">Getting started: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_usage_onnx.html">Getting started: Quark for ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/basic_usage_pytorch.html">Getting started: Quark for PyTorch</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/pytorch_examples.html">PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_fx_image_classification.html">Image Classification Models FX Graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/sample_yolo_nas_quant.html">YOLO-NAS FX graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/sample_yolo_x_tiny_quant.html">YOLO-X Tiny FX Graph Quantization</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx_examples.html">ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_layerwise_percentile.html">Layer-wise Percentile</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_auto_search.html">Auto-Search for General Yolov3 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_ryzenai_yolonas.html">Auto-Search for Ryzen AI Yolo-nas ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_ryzenai_autosearch_resnet50.html">Auto-Search for Ryzen AI Resnet50 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_ryzenai_yolov3_custom_evaluator.html">Auto-Search for Ryzen AI Yolov3 ONNX Quantization with Custom Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizing Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_crypto_mode.html">Quantizing a ResNet50 model in crypto mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="hugging_face_timm_quantization.html">Hugging Face TIMM Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_yolo_quantization.html">Yolo_nas and Yolox Quantization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported accelerators</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../supported_accelerators/ryzenai/index.html">AMD Ryzen AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_quick_start_for_ryzenai.html">Quick Start for Ryzen AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/ryzen_ai_best_practice.html">Best Practice for Ryzen AI in AMD Quark ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_ryzenai.html">Auto-Search for Ryzen AI ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_uint4_oga.html">Quantizing LLMs for ONNX Runtime GenAI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_convert_fp32_or_fp16_to_bf16.html">FP32/FP16 to BF16 Model Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_xint8_quantize.html">Power-of-Two Scales (XINT8) Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_a8w8_and_a16w8_quantize.html">Float Scales (A8W8 and A16W8) Quantization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../supported_accelerators/mi_gpus/index.html">AMD Instinct</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_perplexity.html">Evaluation of Quantized Models</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../pytorch/user_guide_config_for_llm.html">Configuring PyTorch Quantization for Large Language Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization from Scratch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/export/quark_export_onnx.html">ONNX format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/export/quark_export_hf.html">Hugging Face format (safetensors)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/export/quark_export_gguf.html">GGUF format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/export/quark_export_quark.html">Quark format</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/feature_pruning_overall.html">LLM Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark and Trainer</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/auto_smoothquant_document_and_example.html">Auto SmoothQuant</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/awq_document.html">Activation-aware Weight Quantization (AWQ)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/torch/example_awq.html">AWQ end-to-end demo</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="user_guide_config_description.html">Configuring ONNX Quantization</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="user_guide_supported_optype_datatype.html">Data and OP Types</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="custom_operators/ExtendedQuantizeLinear.html">ExtendedQuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_operators/ExtendedDequantizeLinear.html">ExtendedDequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_operators/ExtendedInstanceNormalization.html">ExtendedInstanceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_operators/ExtendedLSTM.html">ExtendedLSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_operators/BFPQuantizeDequantize.html">BFPQuantizeDequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="custom_operators/MXQuantizeDequantize.html">MXQuantizeDequantize</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_bfp16_quantization.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_microscaling_quantization.html">Microscaling (MX)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_microexponents_quantization.html">Microexponents (MX)</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="accuracy_algorithms/quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_gptq.html">Quantizing a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide_auto_search.html">Automatic Search for Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="config/user_guide_onnx_model_inference_save_input_npy.html">Using ONNX Model Inference and Saving Input Data in NPY Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/quickstart_tutorial/quickstart_tutorial.html">AMD Quark Tutorial: PyTorch Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/diffusion_tutorial/diffusion_tutorial.html">Quantizing a Diffusion Model using Quark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/depth_wise_pruning/llm_depth_pruning.html">LLM Model Depth-Wise Pruning (beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/llm_tutorial/llm_tutorial.html">Quantizing a Large Language Model with Quark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Third-party contributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro_contrib.html">Introduction and guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../autoapi/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/config/template/index.html">Quantizer Template</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../autoapi/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_faq.html">ONNX FAQ</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../versions.html">AMD Quark release history</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">Quark license</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="user_guide_config_description.html" class="nav-link">Configuring ONNX Quantization</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Full List of Quantization Configuration Features</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Full List of Quantization Configuration Features</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-configuration">Quantization Configuration</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="full-list-of-quantization-configuration-features">
<h1>Full List of Quantization Configuration Features<a class="headerlink" href="#full-list-of-quantization-configuration-features" title="Link to this heading">#</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>It’s very simple to quantize a model using the ONNX quantizer of Quark, only a few straightforward Python statements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">QConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.spec</span><span class="w"> </span><span class="kn">import</span> <span class="n">QLayerConfig</span><span class="p">,</span> <span class="n">Int8Spec</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span><span class="n">global_config</span><span class="o">=</span><span class="n">QLayerConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">Int8Spec</span><span class="p">(),</span> <span class="n">weight</span><span class="o">=</span><span class="n">Int8Spec</span><span class="p">()))</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">model_input</span><span class="p">,</span> <span class="n">model_output</span><span class="p">,</span> <span class="n">calibration_data_reader</span><span class="p">)</span>
</pre></div>
</div>
<p>As shown in the code, just create a quantization configuration and use it to initialize a quantizer, and then call the quantizer’s <em>quantize_model()</em> API, which has 3 main parameters:</p>
<ul class="simple">
<li><p><strong>model_input</strong>: (String or ModelProto) This parameter specifies the file path of the model that is to be quantized. When a file path cannot be specified, the loaded ModelProto can also be passed in directly.</p></li>
<li><p><strong>model_output</strong>: (Optional String) This parameter specifies the file path where the quantized model will be saved. You can leave it unspecified (it will default to None), and the ModelProto format quantized model will be returned by the API.</p></li>
<li><p><strong>calibration_data_reader</strong>: (Optional Object) This parameter is a calibration data reader that enumerates the calibration data and generates inputs for the original model. You can leave it unspecified (it will default to None), and simply enable <em>UseRandomData</em> in extra options of quantization configuration to use random data for calibration.</p></li>
</ul>
<p>The next section will provide a detailed list of all parameters in the quantization configuration.</p>
</section>
<section id="quantization-configuration">
<h2>Quantization Configuration<a class="headerlink" href="#quantization-configuration" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.spec</span><span class="w"> </span><span class="kn">import</span> <span class="n">QLayerConfig</span><span class="p">,</span> <span class="n">Int8Spec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">QConfig</span>

<span class="n">quant_config</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
   <span class="n">global_config</span> <span class="o">=</span> <span class="n">QLayerConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">Int8Spec</span><span class="p">(),</span> <span class="n">weight</span><span class="o">=</span><span class="n">Int8Spec</span><span class="p">()),</span>
   <span class="n">specific_layer_config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">DataType</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
   <span class="n">layer_type_config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="n">DataType</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
   <span class="n">exclude</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
   <span class="n">algo_config</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">AlgoConfig</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
   <span class="n">use_external_data_format</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
   <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>global_config</strong>: (QLayerConfig) The global quantization configuration applied to all layers unless overridden. Defaults to QLayerConfig(activaiton=Int8Spec(), weight=Int8Spec()) .</p>
<ul>
<li><p>activation/weight (QTensorConfig): The Tensor-level quantization configuration of activation or weight. The options are Int8Spec, UInt8Spec, XInt8Spec, Int16Spec, UInt16Spec, Int32Spec, UInt32Spec, BFloat16Spec, BFP16Spec, Int4Spec, UInt4Spec. It includes attributes whether symmetric quantization is used, the type of scaling strategy, the calibration method applied, and the level of quantization granularity.</p>
<ul>
<li><p>symmetric (bool): Whether use symmetric quantization for QTensorConfigs like Int8Spec. For signed data types such as Int8Spec, the default value is True, while for unsigned data types such as UInt8Spec, the default value is False.</p></li>
<li><p>scale_type (ScaleType): The scale type of QTensorConfigs like Int8Spec. The options are ScaleType.Float32, ScaleType.PowerOf2 and ScaleType.Int16.</p></li>
<li><p>calibration_method (CalibMethod): The calibration method of QTensorConfigs like Int8Spec. The options are CalibMethod.MinMax, CalibMethod.MinMSE, CalibMethod.Percentile, CalibMethod.Entropy, CalibMethod.LayerwisePercentile and CalibMethod.Distribution.</p></li>
<li><p>quant_granularity (QuantGranularity): The quantization granularity of QTensorConfigs like Int8Spec. The options are QuantGranularity.Tensor, QuantGranularity.Channel and QuantGranularity.Group.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>specific_layer_config</strong>: (Dictionary or None) Dictionary mapping specific layer names to their quantizaiton data type like {Int16: [“/conv1/Conv”, “/conv2/Conv”], BFloat16: [“/matmul1/MatMul”, “/matmul2/MatMul”]}. Defaults to None.</p></li>
<li><p><strong>layer_type_config</strong>: (Dictionary or None) Dictionary mapping all nodes to the given operation type to their quantizaiton data type like {Int16: [“Conv”, “ConvTranspose”], None: [“MatMul”, “Gemm”]}. Data type is None means excluding all nodes of these operation types. Defaults to None.</p></li>
<li><p><strong>exclude</strong>: (List or None) Excludes the nodes specified, nodes matched by regular expressions, and the specified subgraphs from quantization like [“/conv1/Conv”, “^/layer0/.*”, ([“Conv1”], [“Conv2”]), ([“Relu9”, “MatMul10”])]. “/conv1/Conv” is the name of a node; “^/layer0/.*” is a regular expression pattern; ([“Conv1”], [“Conv2”]), ([“Relu9”, “MatMul10”]) is a subgraph that starts with “Conv1” and “Conv2” and ends with “Relu9” and “MatMul10”. Defaults to None.</p></li>
<li><p><strong>algo_config</strong>: (List) Each element in this list is an instance of an algorithm class like [CLEConfig(cle_steps=2), AdaRoundConfig(learning_rate=0.1, num_iterations=100)]. Defaults to None.</p></li>
<li><p><strong>use_external_data_format</strong>: (Boolean) This option is used for large size (&gt;2GB) model. The model proto and data will be stored in separate files. The default is False.</p></li>
<li><p><strong>kwargs</strong>: (Any or None) The kwargs for various options in different cases. Current used:</p>
<ul>
<li><p><strong>OpTypesToQuantize</strong>: (List of Strings or None) If specified, only operators of the given types will be quantized (e.g., [‘Conv’] to only quantize Convolutional layers). By default, all supported operators will be quantized.</p></li>
<li><p><strong>ExtraOpTypesToQuantize</strong>: (List of Strings or None) If specified, the given operator types will be included as additional targets for quantization, expanding the set of operators to be quantized without replacing the existing configuration (e.g., [‘Gemm’] to include Gemm layers in addition to the currently specified types). By default, no extra operator types will be added for quantization.</p></li>
<li><p><strong>ExecutionProviders</strong>: (List of Strings) This parameter defines the execution providers that will be used by ONNX Runtime to do calibration for the specified model. The default value ‘CPUExecutionProvider’ implies that the model will be computed using the CPU as the execution provider. You can also set this to other execution providers supported by ONNX Runtime such as ‘ROCMExecutionProvider’ and ‘CUDAExecutionProvider’ for GPU-based computation, if they are available in your environment. The default is [‘CPUExecutionProvider’].</p></li>
<li><p><strong>OptimizeModel</strong>:(Boolean) If True, optimizes the model before quantization. Model optimization performs certain operator fusion that makes quantization tool’s job easier. For instance, a Conv/ConvTranspose/Gemm operator followed by BatchNormalization can be fused into one during the optimization, which can be quantized very efficiently. The default value is True.</p></li>
<li><p><strong>ConvertFP16ToFP32</strong>: (Boolean) This parameter controls whether to convert the input model from float16 to float32 before quantization. For float16 models, it is recommended to set this parameter to True. The default value is False. When using convert_fp16_to_fp32 in AMD Quark for ONNX, it requires onnxsim to simplify the ONNX model. Please make sure that onnxsim is installed by using ‘python -m pip install onnxsim’.</p></li>
<li><p><strong>ConvertNCHWToNHWC</strong>: (Boolean) This parameter controls whether to convert the input NCHW model to input NHWC model before quantization. For input NCHW models, it is recommended to set this parameter to True. The default value is False.</p></li>
<li><p><strong>DebugMode</strong>: (Boolean) Flag to enable debug mode. In this mode, all debugging message will be printed. Default is False.</p></li>
<li><p><strong>CryptoMode</strong>: (Boolean) Flag to enable crypto mode. In this mode, all message will be blocked, and all intermediate data related to the model will not be saved to disk. In addition, the input model to the <em>quantize_model</em> API should be a ModelProto object. Please that it only supports &lt;2GB ModelProto object. Default is False.</p></li>
<li><p><strong>PrintSummary</strong>: (Boolean) Flag to print summary of quantization. Default is True.</p></li>
<li><p><strong>IgnoreWarnings</strong>: (Boolean) Flag to suppress the warnings globally. Default is True.</p></li>
<li><p><strong>LogSeverityLevel</strong>: (Int) This parameter is used to select the severity level of screen printing logs. Its value ranges from 0 to 4: 0 for DEBUG, 1 for INFO, 2 for WARNING, 3 for ERROR and 4 for CRITICAL or FATAL. Default value is 1, which means printing all messages including INFO, WARNING, ERROR and etc by default.</p></li>
<li><p><strong>ActivationScaled</strong>: (Boolean) If True, all activations will be scaled to the exact numeric range. The default is True for integer data type quantization and False for BFloat16 and Float16, which means by default the BFloat16/Float16 quantization will cast float32 tensors to BFloat16/Float16 directly.</p></li>
<li><p><strong>WeightScaled</strong>: (Boolean) If True, all weights will be scaled to the exact numeric range. The default is True for integer data type quantization and False for BFloat16 and Float16, which means by default the BFloat16/Float16 quantization will cast float32 tensors to BFloat16/Float16 directly.</p></li>
<li><p><strong>QuantizeFP16</strong>: (Boolean) If True, the data type of the input model should be float16. It only takes effect when onnxruntime version is 1.18 or above. The default is False.</p></li>
<li><p><strong>UseFP32Scale</strong>: (Boolean) If True, the scale of the quantized model is converted from float16 to float32 when the quantization is done. It only takes effect only if QuantizeFP16 is True. It must be False when UseMatMulNBits is True. The default is True.</p></li>
<li><p><strong>UseUnsignedReLU</strong>: (Boolean) If True, the output tensor of ReLU and Clip, whose min is 0, will be forced to be asymmetric. The default is False.</p></li>
<li><p><strong>QuantizeBias</strong>: (Boolean) If True, quantize the Bias as a normal weights. The default is True. For DPU/NPU devices, this must be set to True.</p></li>
<li><p><strong>Int32Bias</strong>: (Boolean) If True, bias will be quantized in int32 data type; if false, it will have the same data type as weight. The default is False when enable_npu_cnn is True. Otherwise the default is True.</p></li>
<li><p><strong>Int16Bias</strong>: (Boolean) If True, bias will be quantized in int16 data type; The default is False. <strong>Note</strong>: 1. ONNXRuntime only supports Int16 Bias inference when the opset version is 21 or higher, so please ensure that the input model’s opset version is 21 or higher. 2. It is recommended to use this together with ADAROUND or ADAQUANT; otherwise, the quantized model with Int16 bias may suffer from poor accuracy.</p></li>
<li><p><strong>RemoveInputInit</strong>: (Boolean) If True, initializer in graph inputs will be removed because it will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. The default is True.</p></li>
<li><p><strong>SimplifyModel</strong>: (Boolean) If True, The input model will be simplified using the onnxsim tool. The default is True.</p></li>
<li><p><strong>EnableSubgraph</strong>: (Boolean) If True, the subgraph will be quantized. The default is False. More support for this feature is planned in the future.</p></li>
<li><p><strong>ForceQuantizeNoInputCheck</strong>: (Boolean) If True, latent operators such as maxpool and transpose will always quantize their inputs, generating quantized outputs even if their inputs have not been quantized. The default behavior can be overridden for specific nodes using nodes_to_exclude.</p></li>
<li><p><strong>MatMulConstBOnly</strong>: (Boolean) If True, only MatMul operations with a constant ‘B’ will be quantized. The default is False for static mode and True for dynmaic mode.</p></li>
<li><p><strong>AddQDQPairToWeight</strong>: (Boolean) If True, both QuantizeLinear and DeQuantizeLinear nodes are inserted for weight, maintaining its floating-point format. The default is False, which quantizes floating-point weight and feeds it solely to an inserted DeQuantizeLinear node. In the PowerOfTwoMethod calibration method, this setting will also be effective for the bias.</p></li>
<li><p><strong>OpTypesToExcludeOutputQuantization</strong>: (List of Strings or None) If specified, the output of operators with these types will not be quantized. The default is an empty list.</p></li>
<li><p><strong>DedicatedQDQPair</strong>: (Boolean) If True, an identical and dedicated QDQ pair is created for each node. The default is False, allowing multiple nodes to share a single QDQ pair as their inputs.</p></li>
<li><p><strong>QDQOpTypePerChannelSupportToAxis</strong>: (Dictionary) Sets the channel axis for specific operator types (e.g., {‘MatMul’: 1}). This is only effective when per-channel quantization is supported and per_channel is True. If a specific operator type supports per-channel quantization but no channel axis is explicitly specified, the default channel axis will be used. For DPU/NPU devices, this must be set to {} as per-channel quantization is currently unsupported. The default is an empty dict ({}).</p></li>
<li><p><strong>CalibTensorRangeSymmetric</strong>: (Boolean) If True, the final range of the tensor during calibration will be symmetrically set around the central point “0”. The default is False. In PowerOfTwoMethod calibration method, the default is True.</p></li>
<li><p><strong>CalibMovingAverage</strong>: (Boolean) If True, the moving average of the minimum and maximum values will be computed when the calibration method selected is MinMax. The default is False. In PowerOfTwoMethod calibration method, this should be set to False.</p></li>
<li><p><strong>CalibMovingAverageConstant</strong>: (Float) Specifies the constant smoothing factor to use when computing the moving average of the minimum and maximum values. The default is 0.01. This is only effective when the calibration method selected is MinMax and CalibMovingAverage is set to True. In PowerOfTwoMethod calibration method, this option is unsupported.</p></li>
<li><p><strong>Percentile</strong>: (Float) If the calibration method is set to ‘quark.onnx.CalibrationMethod.Percentile,’ then this parameter can be set to the percentage for percentile. The default is 99.999.</p></li>
<li><p><strong>LWPMetric</strong>: (String) If the calibration method is set to ‘quark.onnx.LayerWiseMethod.LayerWisePercentile,’ then this parameter can be set to select the metric to judge the percentile value. The default is mae.</p></li>
<li><p><strong>ActivationBitWidth</strong>: (Int) If the calibration method is set to ‘quark.onnx.LayerWiseMethod.LayerWisePercentile’, then this parameter can be set to calculate the quantize/dequantize error. The default is 8.</p></li>
<li><p><strong>PercentileCandidates</strong>: (List) If the calibration method is set to ‘quark.onnx.LayerWiseMethod.LayerWisePercentile’ then this parameter can be set to the percentage for percentiles. The default is [99.99, 99.999, 99.9999].</p></li>
<li><p><strong>UseRandomData</strong>: (Boolean) Required to be true when the RandomDataReader is needed. The default value is false.</p></li>
<li><p><strong>RandomDataReaderInputShape</strong>: (Dict) It is required to use dict {name : shape} to specify a certain input. For example, RandomDataReaderInputShape={“image” : [1, 3, 224, 224]} for the input named “image”. The default value is an empty dict {}.</p></li>
<li><p><strong>RandomDataReaderInputDataRange</strong>: (Dict or None) Specifies the data range for each inputs if used random data reader (calibration_data_reader is None). Currently, if set to None then the random value will be 0 or 1 for all inputs, otherwise range [-128,127] for unsigned int, range [0,255] for signed int and range [0,1] for other float inputs. The default is None.</p></li>
<li><p><strong>Int16Scale</strong>: (Boolean) If True, the float scale will be replaced by the closest value corresponding to M and 2<strong>N, where the range of M and 2</strong>N is within the representation range of int16 and uint16. The default is False.</p></li>
<li><p><strong>MinMSEModePof2Scale</strong>: (String) When using quark.onnx.PowerOfTwoMethod.MinMSE, you can specify the method for calculating minmse. By default, minmse is calculated using all calibration data. Alternatively, you can set the mode to “MostCommon”, where minmse is calculated for each batch separately and take the most common value. The default setting is ‘All’.</p></li>
<li><p><strong>ConvertOpsetVersion</strong>: (Int or None) Specifies the target opset version for the ONNX model. If set, the model’s opset version will be updated accordingly. The default is None.</p></li>
<li><p><strong>ConvertBNToConv</strong>: (Boolean) If True, the BatchNormalization operation will be converted to Conv operation. The default is True when enable_npu_cnn is True.</p></li>
<li><p><strong>ConvertReduceMeanToGlobalAvgPool</strong>: (Boolean) If True, the Reduce Mean operation will be converted to Global Average Pooling operation. The default is True when enable_npu_cnn is True.</p></li>
<li><p><strong>SplitLargeKernelPool</strong>: (Boolean) If True, the large kernel Global Average Pooling operation will be split into multiple Average Pooling operation. The default is True when enable_npu_cnn is True.</p></li>
<li><p><strong>ConvertSplitToSlice</strong>: (Boolean) If True, the Split operation will be converted to Slice operation. The default is True when enable_npu_cnn is True.</p></li>
<li><p><strong>FuseInstanceNorm</strong>: (Boolean) If True, the split instance norm operation will be fused to InstanceNorm operation. The default is True.</p></li>
<li><p><strong>FuseL2Norm</strong>: (Boolean) If True, a set of L2norm operations will be fused to L2Norm operation. The default is True.</p></li>
<li><p><strong>FuseGelu</strong>: (Boolean) If True, a set of Gelu operations will be fused to Gelu operation. The default is True.</p></li>
<li><p><strong>FuseLayerNorm</strong>: (Boolean) If True, a set of LayerNorm operations will be fused to LayerNorm operation. The default is True.</p></li>
<li><p><strong>ConvertClipToRelu</strong>: (Boolean) If True, the Clip operations that has a min value of 0 will be converted to ReLU operations. The default is True when enable_npu_cnn is True.</p></li>
<li><p><strong>SimulateDPU</strong>: (Boolean) If True, a simulation transformation that replaces some operations with an approximate implementation will be applied for DPU when enable_npu_cnn is True. The default is True.</p></li>
<li><p><strong>ConvertLeakyReluToDPUVersion</strong>: (Boolean) If True, the Leaky Relu operation will be converted to DPU version when SimulateDPU is True. The default is True.</p></li>
<li><p><strong>ConvertSigmoidToHardSigmoid</strong>: (Boolean) If True, the Sigmoid operation will be converted to Hard Sigmoid operation when SimulateDPU is True. The default is True.</p></li>
<li><p><strong>ConvertHardSigmoidToDPUVersion</strong>: (Boolean) If True, the Hard Sigmoid operation will be converted to DPU version when SimulateDPU is True. The default is True.</p></li>
<li><p><strong>ConvertAvgPoolToDPUVersion</strong>: (Boolean) If True, the global or kernel-based Average Pooling operation will be converted to DPU version when SimulateDPU is True. The default is True.</p></li>
<li><p><strong>ConvertClipToDPUVersion</strong>: (Boolean) If True, the Clip operation will be converted to DPU version when SimulateDPU is True. The default is False.</p></li>
<li><p><strong>ConvertReduceMeanToDPUVersion</strong>: (Boolean) If True, the ReduceMean operation will be converted to DPU version when SimulateDPU is True. The default is True.</p></li>
<li><p><strong>ConvertSoftmaxToDPUVersion</strong>: (Boolean) If True, the Softmax operation will be converted to DPU version when SimulateDPU is True. The default is False.</p></li>
<li><p><strong>NPULimitationCheck</strong>: (Boolean) If True, the quantization position will be adjust due to the limitation of DPU/NPU. The default is True.</p></li>
<li><p><strong>MaxLoopNum</strong>: (Int) The quantizer adjusts or aligns the quantization position through loops, this option is used to set the maximum number of loops. The default value is 5.</p></li>
<li><p><strong>AdjustShiftCut</strong>: (Boolean) If True, adjust the shift cut of nodes when NPULimitationCheck is True. The default is True.</p></li>
<li><p><strong>AdjustShiftBias</strong>: (Boolean) If True, adjust the shift bias of nodes when NPULimitationCheck is True. The default is True.</p></li>
<li><p><strong>AdjustShiftRead</strong>: (Boolean) If True, adjust the shift read of nodes when NPULimitationCheck is True. The default is True.</p></li>
<li><p><strong>AdjustShiftWrite</strong>: (Boolean) If True, adjust the shift write of nodes when NPULimitationCheck is True. The default is True.</p></li>
<li><p><strong>AdjustHardSigmoid</strong>: (Boolean) If True, adjust the position of hard sigmoid nodes when NPULimitationCheck is True. The default is True.</p></li>
<li><p><strong>AdjustShiftSwish</strong>: (Boolean) If True, adjust the shift swish when NPULimitationCheck is True. The default is True.</p></li>
<li><p><strong>AlignConcat</strong>: (Boolean) If True, adjust the quantization position of concat when NPULimitationCheck is True. The default is True, when the power-of-two scale is used, otherwise it’s False.</p></li>
<li><p><strong>AlignPool</strong>: (Boolean) If True, adjust the quantization position of pooling when NPULimitationCheck is True. The default is True, when the power-of-two scale is used, otherwise it’s False.</p></li>
<li><p><strong>AlignPad</strong>: (Boolean) If True, adjust the quantization position of pad when NPULimitationCheck is True. The default is True, when the power-of-two scale is used, otherwise it’s False.</p></li>
<li><p><strong>AlignSlice</strong>: (Boolean) If True, adjust the quantization position of slice when NPULimitationCheck is True. The default is True, when the power-of-two scale is used, otherwise it’s False.</p></li>
<li><p><strong>AlignTranspose</strong>: (Boolean) If True, adjust the quantization position of transpose when NPULimitationCheck is True. The default is False.</p></li>
<li><p><strong>AlignReshape</strong>: (Boolean) If True, adjust the quantization position of reshape when NPULimitationCheck is True. The default is False.</p></li>
<li><p><strong>AdjustBiasScale</strong>: (Boolean) If True, adjust the bias scale equal to activation scale multiply by weights scale. The default is True.</p></li>
<li><p><strong>TensorsRangeFile</strong>: (None or String) This parameter is used to manage tensor range information, and it should has a “.json” suffix because this file will be save in that format. When set to None, the tensor ranges will be calculated from scratch and will not be saved. If set to a string representing a file path, and the file does not exist, the tensor range information will be computed and saved to that file. If the file already exists, the tensor range information will be loaded from it and will not be recalculated. This file can help to save the calibration time when to rerun FastFinetune algorithm or to reproduce some calibrated models. The default value is None.</p></li>
<li><p><strong>ReplaceClip6Relu</strong>: (Boolean) If True, Replace Clip(0,6) with Relu in the model. The default is False.</p></li>
<li><p><strong>CopySharedInit</strong>: (List or None) Specifies the node op_types to run duplicating initializer in the model for separate quantization use across different nodes, e.g. [‘Conv’, ‘Gemm’, ‘Mul’] input, only shared initializer in these nodes will be duplicated. None means that skip this conversion while empty list means that run this for all op_types included in the given model, default is None.</p></li>
<li><p><strong>CopyBiasInit</strong>: (List or None) Specifies the node operation types to run duplicating bias initializer in the model for separate quantization use across different nodes, e.g. [‘Conv’, ‘Gemm’, ‘Mul’] input, only shared bias initializer in these nodes will be duplicated. None means that skip this conversion while empty list means that run this for all operation types included in the given model. The default is an empty list when using quantization with float scale like A8W8 and A16W8. The default is None otherwise.</p></li>
<li><p><strong>RemoveQDQConvClip</strong>: (Boolean) If True, the QDQ between Conv/Add/Gemm and Clip will be removed for DPU. The default is True.</p></li>
<li><p><strong>RemoveQDQConvRelu</strong>: (Boolean) If True, the QDQ between Conv/Add/Gemm and Relu will be removed for DPU. The default is True.</p></li>
<li><p><strong>RemoveQDQConvLeakyRelu</strong>: (Boolean) If True, the QDQ between Conv/Add/Gemm and LeakyRelu will be removed for DPU. The default is True.</p></li>
<li><p><strong>RemoveQDQConvPRelu</strong>: (Boolean) If True, the QDQ between Conv/Add/Gemm and PRelu will be removed for DPU. The default is True.</p></li>
<li><p><strong>RemoveQDQConvGelu</strong>: (Boolean) If True, the QDQ between Conv/Add/Gemm and Gelu will be removed. The default is False.</p></li>
<li><p><strong>RemoveQDQMulAdd</strong>: (Boolean) If True, the QDQ between Mul and Add will be removed for NPU. The default is False.</p></li>
<li><p><strong>RemoveQDQBetweenOps</strong>: (List of tuples (Strings, Strings) or None) This parameter accepts a list of tuples representing operation type pairs (e.g., Conv and Relu). If set, the QDQ between the specified pairs of operations will be removed for NPU. The default is None.</p></li>
<li><p><strong>RemoveQDQInstanceNorm</strong>: (Boolean) If True, the QDQ between InstanceNorm and Relu/LeakyRelu/PRelu will be removed for DPU. The default is False.</p></li>
<li><p><strong>FoldBatchNorm</strong>: (Boolean) If True, the BatchNormalization operation will be fused with Conv, ConvTranspose or Gemm operation. The BatchNormalization operation after Concat operation will also be fused, if the all input operations of the Concat operation are Conv, ConvTranspose or Gemm operatons.The default is True.</p></li>
<li><p><strong>BF16WithClip</strong>: (Boolean) If True, during BFloat16 quantization, insert “Clip” node before customized “QuantizeLinear” node to add boundary protection for activation. The default is False.</p></li>
<li><p><strong>BF16QDQToCast</strong>: (Boolean) If True, during BFloat16 quantization, replace QuantizeLinear/DeQuantizeLinear ops with Cast ops to accelerate BFloat16 quantized inference. The default is False.</p></li>
<li><p><strong>FixShapes</strong>: (String) Set the input and output shapes of the quantized model to a fixed shape by default if not explicitly specified. The example: ‘FixShapes’:’input_1:[1,224,224,3];input_2:[1,96,96,3];output_1:[1,100];output_2:[1,1000]’</p></li>
<li><p><strong>FoldRelu</strong>: (Boolean) If True, the Relu will be fold to Conv when use ExtendedQuantFormat. The default is False.</p></li>
<li><p><strong>CalibDataSize</strong>: (Int) This parameter controls how many data are used for calibration. The default to using all the data in the calibration dataloader.</p></li>
<li><p><strong>CalibOptimizeMem</strong>: (Boolean) If True, caches intermediate data of activations on disk to reduce the memory consumption during calibration. This option is only effective for the PowerOfTwoMethod.MinMSE method. The default is True.</p></li>
<li><p><strong>CalibWorkerNum</strong>: (Int) This parameter controls how many workers (processes) to collect data. The more workers there are, the less time it takes, but the more memory it consumes (because each worker requires independent memory space). It supports all methods except for CalibrationMethod.MinMax and PowerOfTwoMethod.NonOverflow. The default is 1.</p></li>
<li><p><strong>SaveTensorHistFig</strong>: (Boolean) If True, save the tensor histogram to the file ‘tensor_hist’ in the working directory. The default is False.</p></li>
<li><p><strong>QuantizeAllOpTypes</strong>: (Boolean) If True, all operation types will be quantized. In the BF16 config, the default is True, while for others, the default is False.</p></li>
<li><p><strong>WeightsOnly</strong>: (Boolean) If True, only quantize weights of the model. The default is False.</p></li>
<li><p><strong>AlignEltwiseQuantType</strong>: (Boolean) If True, quantize weights of the node with the activation quant type if node type in [Mul, Add, Sub, Div, Min, Max] when quant_format is ExtendedQuantFormat.QDQ and enable_npu_cnn is False and enable_npu_transformer is False. The default is False.</p></li>
<li><p><strong>EnableVaimlBF16</strong>: (Boolean) If True, the bfloat16 quantized model with vitis qdq will be converted to a bfloat16 quantized model with bfloat16 weights stored as float32. Vaiml is the name of a compiler, the bfloat16 quantized model can be directly deployed on the compiler if the parameter is True. The default is False.</p></li>
<li><p><strong>UseMatMulNBits</strong>: (Boolean) If True, only quantize weights with nbits for MatMul of the model. The default is False.</p></li>
<li><p><strong>MatMulNBitsParams</strong>: (Dictionary) A parameter used to specify the settings for MatMulNBits Quantizer:</p>
<ul>
<li><p><strong>Algorithm</strong>: (str) The algorithm in MatMulNBits Quantization determines which algorithm (“DEFAULT”, “GPTQ”, “HQQ”) to be used to quantize weights. The default is “DEFAULT”.</p></li>
<li><p><strong>GroupSize</strong>: (int) The block size in MatMulNBits Quantization determines how many weights share a scale. The default is 128.</p></li>
<li><p><strong>Symmetric</strong>: (Boolean) If True, symmetrize quantization for weights. The default is True.</p></li>
<li><p><strong>Bits</strong>: (int) The target bits to quantize. Only 4b quantization is supported for inference, additional bits support is planned.</p></li>
<li><p><strong>AccuracyLevel</strong>: (int) The quantization level of input, can be: 0(unset), 1(fp32), 2(fp16), 3(bf16), or 4(int8). The default is 0.</p></li>
</ul>
</li>
<li><p><strong>EvalMetrics</strong>: (Boolean) If True, enables evaluation of the quantized model by measuring cosine similarity and L2 loss. The default is False.</p></li>
<li><p><strong>EvalDataReader</strong>: (DataReader) This parameter is used only when EvalMetrics is set to True. It allows the user to provide a custom data reader for evaluating the quantized model’s cosine similarity and L2 loss metrics against the float model.</p></li>
<li><p><strong>TmpDir</strong>: (String) Specifies the directory used to cache intermediate files. The default value is None, in which case the system temporary directory will be used for the caching. This argument can be set for either absolute or relative path.</p></li>
<li><p><strong>EncryptionAlgorithm</strong>: (String) A parameter used to specify the encryption algorithm for crypto mode, only “AES-256” algorithm is supported currently. The default value is None, which means it will not save any intermediate models/files to disk in crypto mode.</p></li>
</ul>
</li>
</ul>
<p>Table 7. Quantization Data Types can be selected</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>data_type</p></th>
<th class="head"><p>comments</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Int4
UInt4
Int8
UInt8
Int16
UInt16
Int32
UInt32
BFloat16
BFP16</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="user_guide_config_description.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Configuring ONNX Quantization</p>
      </div>
    </a>
    <a class="right-next"
       href="config/calibration_methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Calibration Methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-configuration">Quantization Configuration</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Sep 26, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2025 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>