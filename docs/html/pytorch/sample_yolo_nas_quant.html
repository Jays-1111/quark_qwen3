
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>YOLO-NAS FX graph Quantization &#8212; AMD Quark 0.10 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a66ef196" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_header.css?v=9557e3d1" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_footer.css?v=7095035a" />
    <link rel="stylesheet" type="text/css" href="../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e0f31c2e"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script defer="defer" src="../_static/search.js?v=90a4452c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'pytorch/sample_yolo_nas_quant';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="YOLO-X Tiny FX Graph Quantization" href="sample_yolo_x_tiny_quant.html" />
    <link rel="prev" title="Image Classification Models FX Graph Quantization" href="example_quark_fx_image_classification.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].innerText = "Show code ▼\n\n";

            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">AMD Quark 0.10 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic_usage.html">Getting started: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/basic_usage_onnx.html">Getting started: Quark for ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_usage_pytorch.html">Getting started: Quark for PyTorch</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="pytorch_examples.html">PyTorch Examples</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="example_quark_torch_llm_ptq.html">Language Model PTQ</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="example_quark_fx_image_classification.html">Image Classification Models FX Graph Quantization</a></li>
<li class="toctree-l3 current active"><a class="current reference internal" href="#">YOLO-NAS FX graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="sample_yolo_x_tiny_quant.html">YOLO-X Tiny FX Graph Quantization</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/onnx_examples.html">ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_layerwise_percentile.html">Layer-wise Percentile</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_auto_search.html">Auto-Search for General Yolov3 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_ryzenai_yolonas.html">Auto-Search for Ryzen AI Yolo-nas ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_ryzenai_autosearch_resnet50.html">Auto-Search for Ryzen AI Resnet50 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_ryzenai_yolov3_custom_evaluator.html">Auto-Search for Ryzen AI Yolov3 ONNX Quantization with Custom Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizing Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_crypto_mode.html">Quantizing a ResNet50 model in crypto mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/hugging_face_timm_quantization.html">Hugging Face TIMM Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_yolo_quantization.html">Yolo_nas and Yolox Quantization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported accelerators</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../supported_accelerators/ryzenai/index.html">AMD Ryzen AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_quick_start_for_ryzenai.html">Quick Start for Ryzen AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/ryzen_ai_best_practice.html">Best Practice for Ryzen AI in AMD Quark ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_ryzenai.html">Auto-Search for Ryzen AI ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_uint4_oga.html">Quantizing LLMs for ONNX Runtime GenAI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_convert_fp32_or_fp16_to_bf16.html">FP32/FP16 to BF16 Model Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_xint8_quantize.html">Power-of-Two Scales (XINT8) Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../supported_accelerators/ryzenai/tutorial_a8w8_and_a16w8_quantize.html">Float Scales (A8W8 and A16W8) Quantization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../supported_accelerators/mi_gpus/index.html">AMD Instinct</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_eval_perplexity.html">Evaluation of Quantized Models</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="user_guide_config_for_llm.html">Configuring PyTorch Quantization for Large Language Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="user_guide_config_description.html">Configuring PyTorch Quantization from Scratch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="export/quark_export_onnx.html">ONNX format</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/quark_export_hf.html">Hugging Face format (safetensors)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="export/quark_export_gguf.html">GGUF format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="export/quark_export_quark.html">Quark format</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="feature_pruning_overall.html">LLM Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_qat.html">Language Model QAT Using Quark and Trainer</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/auto_smoothquant_document_and_example.html">Auto SmoothQuant</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="awq_document.html">Activation-aware Weight Quantization (AWQ)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/torch/example_awq.html">AWQ end-to-end demo</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../onnx/custom_operators/ExtendedQuantizeLinear.html">ExtendedQuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/custom_operators/ExtendedDequantizeLinear.html">ExtendedDequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/custom_operators/ExtendedInstanceNormalization.html">ExtendedInstanceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/custom_operators/ExtendedLSTM.html">ExtendedLSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/custom_operators/BFPQuantizeDequantize.html">BFPQuantizeDequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/custom_operators/MXQuantizeDequantize.html">MXQuantizeDequantize</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tutorial_bfp16_quantization.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tutorial_microscaling_quantization.html">Microscaling (MX)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tutorial_microexponents_quantization.html">Microexponents (MX)</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/accuracy_algorithms/quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_gptq.html">Quantizing a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/user_guide_auto_search.html">Automatic Search for Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/config/user_guide_onnx_model_inference_save_input_npy.html">Using ONNX Model Inference and Saving Input Data in NPY Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/quickstart_tutorial/quickstart_tutorial.html">AMD Quark Tutorial: PyTorch Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/diffusion_tutorial/diffusion_tutorial.html">Quantizing a Diffusion Model using Quark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/depth_wise_pruning/llm_depth_pruning.html">LLM Model Depth-Wise Pruning (beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torch/llm_tutorial/llm_tutorial.html">Quantizing a Large Language Model with Quark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Third-party contributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro_contrib.html">Introduction and guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../autoapi/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/config/template/index.html">Quantizer Template</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../autoapi/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../versions.html">AMD Quark release history</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">Quark license</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="pytorch_examples.html" class="nav-link">Accessing PyTorch Examples</a></li>
    
    
    <li class="breadcrumb-item"><a href="example_quark_torch_vision.html" class="nav-link">Vision Model Quantization Using Quark FX Graph Mode</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">YOLO-NAS FX graph Quantization</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>YOLO-NAS FX graph Quantization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#highlight-overview">Highlight Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-information-knowledge">Prerequisites information/knowledge</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-network-structure">Understanding the Network structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-network-output">Understanding the network output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-fx-graph-model">PyTorch FX-Graph model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-summary">Overall summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation">Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-ptq-qat-for-yolo-nas">Perform PTQ &amp; QAT for YOLO-NAS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-data-and-model">1.Prepare data and model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-quantization-config-and-quantizer">2.Set quantization Config and Quantizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration-ptq-training-qat-optional">3.Calibration (PTQ) / Training (QAT) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exported-to-onnx-prepare-for-npu-compile">4.Exported to Onnx (prepare for NPU compile)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation-before-export">5.Preparation before export</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#export-to-onnx-model">6.Export to ONNX model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-onnx-model-optional">7.Visualization ONNX model (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compile-and-deploy-on-amd-npu-device">8.Compile and deploy on AMD NPU device</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-start">Quick start</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-result">Quantization Result</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="yolo-nas-fx-graph-quantization">
<h1>YOLO-NAS FX graph Quantization<a class="headerlink" href="#yolo-nas-fx-graph-quantization" title="Link to this heading">#</a></h1>
<p>In this example, we present an Object Detection Model Quantization workflow. We used YOLO-NAS as a demonstration to illustrate the effectiveness of FX-graph-based QAT and PTQ.</p>
<ol class="arabic simple">
<li><p>We conduct <strong>QAT</strong> (Quantization-Aware Training) experiments and show competitive results compared with <strong>PTQ</strong> (Post-Training Quantization).</p></li>
<li><p>The finally exported ONNX model can used for NPU hardware compile and deployment.</p></li>
</ol>
<section id="highlight-overview">
<h2>Highlight Overview<a class="headerlink" href="#highlight-overview" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Quantization schema</strong>: INT8, symmetric, powof2 format quantization scale for weight, bias, activation.</p></li>
<li><p><strong>Hardware friendly</strong>: We give step-by-step instructions and let the user smoothly deploy in AMD NPU for hardware acceleration.</p></li>
</ul>
</section>
<section id="prerequisites-information-knowledge">
<h2>Prerequisites information/knowledge<a class="headerlink" href="#prerequisites-information-knowledge" title="Link to this heading">#</a></h2>
<p>YOLO-NAS is an object detection model in computer vision tasks. Developed by Deci AI. The original Github repo can be found here <a class="reference external" href="https://github.com/Deci-AI/super-gradients">super-gradients</a>.</p>
<p><strong>NOTE</strong>: In this <a class="reference external" href="https://github.com/Deci-AI/super-gradients/issues/2064">Issue</a>, the pre-trained weight download link move to <a class="reference external" href="https://sg-hub-nv.s3.amazonaws.com/models/yolo_nas_s_coco.pth">yolo_nas_s_coco.pth</a>. Please download the pre-trained weight in advance and put it in the proper place.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>super-gradients<span class="o">==</span><span class="m">3</span>.7.1
$<span class="w"> </span>wget<span class="w"> </span>https://sg-hub-nv.s3.amazonaws.com/models/yolo_nas_s_coco.pth
$<span class="w"> </span>mv<span class="w"> </span>yolo_nas_s_coco.pth<span class="w"> </span>/home/<span class="o">{</span>USER_NAME<span class="o">}</span>/.cache/torch/hub/checkpoints/
</pre></div>
</div>
<p>Before you perform the quantization, If applicable, users can put some time into understanding of YOLO-NAS model architecture. The better understanding of this model the better debugging/solving in real hardware deployment.</p>
<section id="understanding-the-network-structure">
<h3>Understanding the Network structure<a class="headerlink" href="#understanding-the-network-structure" title="Link to this heading">#</a></h3>
<p><strong>NOTE</strong>: This model adopts NAS (Neural Architecture Search) technology, in the validation process some parts of the model are folded, so the training and validation models are different. Relative code can be found: <a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/modules/qarepvgg_block.py#L184">qarepvgg_block.py</a>.</p>
<p>We now using the tool <cite>netron</cite> to visualize the difference between the training and validation models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">yolo_nas</span> <span class="o">=</span> <span class="n">super_gradients</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;yolo_nas_s&quot;</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;coco&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># This model is training format.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">yolo_nas</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span>  <span class="s2">&quot;./fp32_yolo_nas_m.onnx&quot;</span><span class="p">)</span>
<span class="c1"># Perform the model folding (merge several conv + bn to one conv layer)</span>
<span class="n">yolo_nas</span><span class="o">.</span><span class="n">prep_model_for_conversion</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">yolo_nas</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span>  <span class="s2">&quot;./fold_fp32_yolo_nas_m.onnx&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In the <strong>training process</strong>, all modules are not folded. The model structures as follows:</p>
<img alt="../_images/1_original_fp32_train_stage.png" class="align-center" src="../_images/1_original_fp32_train_stage.png" />
<p>In the <strong>validation process</strong>, after calling func: <code class="docutils literal notranslate"><span class="pre">prep_model_for_conversion</span></code>, the above modules are folded, and the folded model is visualized as follows:</p>
<img alt="../_images/2_folded_fp32_validation_stage.png" class="align-center" src="../_images/2_folded_fp32_validation_stage.png" />
</section>
<section id="understanding-the-network-output">
<h3>Understanding the network output<a class="headerlink" href="#understanding-the-network-output" title="Link to this heading">#</a></h3>
<p>The model output is processed by class <a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/detection_models/yolo_nas/dfl_heads.py#L200">NDFLHeads</a>, the output contains two parts <code class="docutils literal notranslate"><span class="pre">decoded_predictions</span></code> and <code class="docutils literal notranslate"><span class="pre">raw_predictions</span></code>. The following parameters are shown in <a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/datasets/detection_datasets/coco_detection.py#L12">COCO Dataset Param</a> (80 classes).</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">decoded_predictions</span></code> contains 2 parts: (<strong>NOTE:</strong> This part tensors/values are used for inference)</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">pred_bboxes</span></code>: with shape: [batch_size, 8400, 4]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pred_scores</span></code>: with shape: [batch_size, 8400, 80]</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">raw_predictions</span></code> contains 6 parts: (<strong>NOTE:</strong> This part tensors/values are used for training, inference/deployment not needed.)</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">cls_score_list</span></code>: with shape [batch size, 8400, 80]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reg_distri_list</span></code>: with shape [batch size, 8400, 68]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">anchors</span></code>: with shape [8400, 4]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">anchor_points</span></code>: with shape [8400, 2]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_anchors_list</span></code>: [6400, 1600, 400]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride_tensor</span></code>: [8400, 1]</p></li>
</ul>
</li>
</ul>
</section>
<section id="pytorch-fx-graph-model">
<h3>PyTorch FX-Graph model<a class="headerlink" href="#pytorch-fx-graph-model" title="Link to this heading">#</a></h3>
<p>In the Quark Fx-graph-based quantization tool, we adopt the PyTorch Fx as inter-represent. More information can be found <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#module-torch.fx">torch.fx</a>. PyTorch.fx is a fully described graph like the <cite>ONNX</cite> graph. Every operation is represented as <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#module-torch.fx">Node</a>. As every node saves information about the operation type, we can easily recognize the computation pattern and insert the quantizer to perform quantization. We use the official PyTorch <a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html#performing-quantization">API</a> to get the Fx graph. Or to say transform <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> to <code class="docutils literal notranslate"><span class="pre">torch.fx.GraphModule</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch version &lt; 2.5</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch._export</span><span class="w"> </span><span class="kn">import</span> <span class="n">capture_pre_autograd_graph</span>
<span class="n">example_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="p">)</span>
<span class="n">graph_model</span> <span class="o">=</span> <span class="n">capture_pre_autograd_graph</span><span class="p">(</span><span class="n">SimpleConv</span><span class="p">(),</span> <span class="n">example_inputs</span><span class="p">)</span>

<span class="c1"># PyTorch version &gt;= 2.5</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">export_for_training</span>
<span class="n">example_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),)</span>
<span class="n">graph_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><span class="n">SimpleConv</span><span class="p">(),</span> <span class="n">example_args</span><span class="p">)</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>NOTE</strong>: Using <cite>to_folder</cite> function, this function allows you to dump out the generated FX code to a folder. Which can help you understand the fx-graph concept well.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">graph_model</span><span class="o">.</span><span class="n">to_folder</span><span class="p">(</span><span class="s2">&quot;./</span><span class="si">{folder_to_save}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The dumped code as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="o">...</span>
<span class="n">conv2d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">conv2d</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight_1</span><span class="p">,</span> <span class="n">bias_1</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">batch_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">conv2d</span><span class="p">,</span> <span class="n">bn_weight_1</span><span class="p">,</span> <span class="n">bn_bias_1</span><span class="p">,</span> <span class="n">bm_mean_1</span><span class="p">,</span> <span class="n">bn_var_1</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">relu_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">relu_</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">batch_norm</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
</section>
<section id="overall-summary">
<h3>Overall summary<a class="headerlink" href="#overall-summary" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>As YOLO-NAS adopts the NAS technology, the model used for training is different from the inference model. But for PTQ/QAT, we must fold the network first, then insert the quantizer to perform quantization.</p></li>
<li><p>As we perform QAT, we need to keep the <code class="docutils literal notranslate"><span class="pre">raw_predictions</span></code> in quantization fine-tuning. After finishing the training and before exporting to <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> model, we need to mask/delete this part.</p></li>
<li><p>Have a glance at PyTorch <code class="docutils literal notranslate"><span class="pre">torch.fx.GraphModule</span></code> (<a class="reference external" href="https://pytorch.org/docs/stable/fx.html#torch.fx.GraphModule">torch.fx.GraphModule</a>) and transform graph API <code class="docutils literal notranslate"><span class="pre">torch.export.export_for_training</span></code> (<a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html#performing-quantization">export_for_training</a>). Which can help you debug and more easily quantize the model in your desired manner.</p></li>
</ol>
</section>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p>Install the required third-party Python packages:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>super_gradients
</pre></div>
</div>
</li>
<li><p>Prepare the <a class="reference external" href="https://cocodataset.org/#download">COCO Dataset</a> 2017 Dataset</p>
<ol class="arabic">
<li><p>Download coco dataset: <a class="reference external" href="http://images.cocodataset.org/annotations/annotations_trainval2017.zip">annotations</a>, <a class="reference external" href="http://images.cocodataset.org/zips/train2017.zip">train2017</a>, <a class="reference external" href="http://images.cocodataset.org/zips/val2017.zip">val2017</a></p></li>
<li><p>After Unzip, the data directory structure would be the following:</p></li>
<li><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>coco_data_dir
├── annotations
│      ├─ instances_train2017.json
│      ├─ instances_val2017.json
│      └─ ...
└── images
    ├── train2017
    │   ├─ 000000000001.jpg
    │   └─ ...
    └── val2017
        └─ ...
</pre></div>
</div>
</li>
</ol>
<p>More and direct instruction you can see: <a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/datasets/detection_datasets/coco_detection.py#L12">class COCODetectionDataset</a></p>
</li>
<li><p>Modify the network code to indicate the quantization scope:</p>
<ol class="arabic">
<li><p><strong>Reason</strong>:</p>
<p>In the YOLO-NAS source code, there is a large amount of code used for generating the bounding box and bounding-box offset, like the above <cite>anchor_points</cite> and <cite>anchors</cite>. Many of them are only used in the training phase and do not take effect in the inference phase. These tensors should not be quantized as:</p>
<blockquote>
<div><ul class="simple">
<li><p>These parts of codes belonging to auxiliary codes, that are only used for the training. Will not be used for inference. For inference, we only need <cite>decoded_predictions</cite> shown above.</p></li>
<li><p>For better training precision, these auxiliary codes should not be quantized</p></li>
<li><p>In the following image, Tensor 1 &amp; Tensor 2 (Red) should be quantized as these two tensors take effect in the inference reasoning. But all the Tensors &amp; Operations in the Green Circle should be excluded from the quantization scope. If we quantize this green circle scope, it will introduce more quantization errors for tensor 1 and tensor 2 and decrease accuracy.</p></li>
</ul>
<img alt="../_images/3_quant_scope.png" class="align-center" src="../_images/3_quant_scope.png" />
</div></blockquote>
</li>
<li><p>How <strong>QuantStub</strong> and <strong>DeQuantStub</strong> works:</p>
<ul class="simple">
<li><p>QuantStub and DeQuantStub is a concept in Quark. Users should use these operations to modify the PyTorch source code to convey the desired quantization scope (which part should be quantized and others not).</p></li>
<li><p>All tensors propagated included in the [QuantStub, DeQuantStub] scope will be noted as quantizable. The Tensor/operation from the QuantStub will be regarded as seeds, then adopt depth and width first search to annotate the following computation operation/tensors as quantizable until [meet the DeQuantStub operation/ the network forward process finished].</p></li>
<li><p>More information can be founded in Quark Source code: <code class="docutils literal notranslate"><span class="pre">tag_quant_node.py</span></code></p></li>
<li><p>In addition: all code under the with no grad scope (<code class="docutils literal notranslate"><span class="pre">&#64;torch.no_grad()</span></code>) will be regarded as no quant.</p></li>
</ul>
</li>
<li><p><strong>How</strong> to modify the PyTorch source to convey the quantization motivation:</p>
<ul>
<li><p>In python file: <a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/detection_models/customizable_detector.py#L30">customizable_detector.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Import QuantStub and DeQuantStub from Quark</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.quantization.graph.ops.quant_stubs</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantStub</span><span class="p">,</span> <span class="n">DeQuantStub</span>

<span class="c1"># 2. Add the number in class CustomizableDetector&#39;s __init__ function</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CustomizableDetector</span><span class="p">(</span><span class="n">HasPredict</span><span class="p">,</span> <span class="n">SgModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_stub</span> <span class="o">=</span> <span class="n">QuantStub</span>      <span class="c1"># add this code</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequant_stub</span> <span class="o">=</span> <span class="n">DeQuantStub</span>  <span class="c1"># add this code (but not used)</span>

<span class="c1"># 3. Modify the forward() function</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_stub</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># add this code</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">neck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># NOTE: For the above modification, some explanations:</span>
<span class="c1"># In the above code, the quant scope starts from input of the forward function,</span>
<span class="c1"># meaning all param &amp; operation in [self.backbone] &amp; [self.neck] &amp; [self.heads] will be quantized</span>
<span class="c1"># (As we do not use self.dequant_stub),</span>

<span class="c1"># However some codes (self.heads) used for generating constant tensors we do not need quant.</span>
<span class="c1"># We need to modify the code in self.heads</span>
</pre></div>
</div>
</li>
<li><p>In python file: <a class="reference external" href="https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/models/detection_models/yolo_nas/dfl_heads.py">dfl_heads.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Import QuantStub and DeQuantStub from Quark</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.quantization.graph.ops.quant_stubs</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantStub</span><span class="p">,</span> <span class="n">DeQuantStub</span>

<span class="c1"># 2. ADD the number in NDFLHeads __init__ function</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NDFLHeads</span><span class="p">(</span><span class="n">BaseDetectionModule</span><span class="p">,</span> <span class="n">SupportsReplaceNumClasses</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dequant_stub</span> <span class="o">=</span> <span class="n">DeQuantStub</span>  <span class="c1"># add this code</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant_stub</span> <span class="o">=</span> <span class="n">QuantStub</span>      <span class="c1"># add this code</span>

<span class="c1"># 3. in function def _generate_anchors(self,...) modify the code as follows</span>
    <span class="c1"># Why we modify in this way:</span>
    <span class="c1"># Quant scope: quant_stub -&gt; tensors -&gt; dequant_stub</span>
    <span class="c1"># Quark Fx tool will quant the tensor among the Quant scope,</span>
    <span class="c1"># so, all tensor out of the Quant scope will not be quantized.</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_anchors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="o">...</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        the original super-gradients code</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">anchor_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">anchor_points</span><span class="p">)</span>  <span class="c1"># original code</span>
        <span class="n">stride_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">stride_tensor</span><span class="p">)</span>  <span class="c1"># original code</span>
        <span class="c1"># add the code below</span>
        <span class="n">anchor_points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_stub</span><span class="p">(</span><span class="n">anchor_points</span><span class="p">)</span>    <span class="c1"># add this code</span>
        <span class="n">anchor_points</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequant_stub</span><span class="p">(</span><span class="n">anchor_points</span><span class="p">)</span>  <span class="c1"># add this code</span>
        <span class="n">stride_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_stub</span><span class="p">(</span><span class="n">stride_tensor</span><span class="p">)</span>    <span class="c1"># add this code</span>
        <span class="n">stride_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dequant_stub</span><span class="p">(</span><span class="n">stride_tensor</span><span class="p">)</span>  <span class="c1"># add this code</span>
        <span class="k">return</span> <span class="n">anchor_points</span><span class="p">,</span> <span class="n">stride_tensor</span>
        <span class="c1"># NOTE: For the above modification, some explanations:</span>
        <span class="c1"># the code used for generate [anchor_points] &amp; [stride_tensor] should not be quantized,</span>
        <span class="c1"># as the code in this function:</span>
        <span class="c1">#   1) have no trainable parameters</span>
        <span class="c1">#   2) anchor_points &amp; stride_tensor are facilitate tensor will not change during inference &amp; training.</span>
        <span class="c1">#   3) quantizing these codes will accumulate quantized error and finally damage  [anchor_points] &amp; [anchor_points] representation ability.</span>
        <span class="c1">#   4) As we not use dequant_stub in customizable_detector.py, and the propagate mechanism, anchor_points and</span>
        <span class="c1">#      stride_tensor will be quantized. In this way, we can maintain the accuracy as much as possible.</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</section>
<section id="perform-ptq-qat-for-yolo-nas">
<h2>Perform PTQ &amp; QAT for YOLO-NAS<a class="headerlink" href="#perform-ptq-qat-for-yolo-nas" title="Link to this heading">#</a></h2>
<p>In the following, we give a brief introduction to the quantization.</p>
<section id="prepare-data-and-model">
<h3>1.Prepare data and model<a class="headerlink" href="#prepare-data-and-model" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">super_gradients.training.dataloaders</span><span class="w"> </span><span class="kn">import</span> <span class="n">coco2017_val_yolo_nas</span><span class="p">,</span> <span class="n">coco2017_train_yolo_nas</span>
<span class="c1"># ===== prepare the data for training, validation and calibration</span>
<span class="c1"># Calib is used for PTQ</span>
<span class="n">valid_dataloader</span> <span class="o">=</span> <span class="n">coco2017_val_yolo_nas</span><span class="p">(</span><span class="n">dataloader_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">25</span><span class="p">},</span>
                                         <span class="n">dataset_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;data_dir&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">})</span>
<span class="n">calib_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="n">valid_dataloader</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">calib_data_size</span><span class="p">))]</span>

<span class="c1"># validation &amp; training dataset</span>
<span class="n">valid_dataloader</span> <span class="o">=</span> <span class="n">coco2017_val_yolo_nas</span><span class="p">(</span><span class="n">dataloader_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">val_batch_size</span><span class="p">},</span>
                                         <span class="n">dataset_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;data_dir&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">})</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">coco2017_train_yolo_nas</span><span class="p">(</span><span class="n">dataloader_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">},</span>
                                           <span class="n">dataset_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;data_dir&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">})</span>

<span class="c1"># ===== prepare the data for training, validation and calibration</span>
<span class="n">yolo_nas</span> <span class="o">=</span> <span class="n">super_gradients</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;yolo_nas_s&quot;</span><span class="p">,</span> <span class="n">pretrained_weights</span><span class="o">=</span><span class="s2">&quot;coco&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">yolo_nas</span><span class="o">.</span><span class="n">prep_model_for_conversion</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">])</span>
<span class="n">graph_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export_for_training</span><span class="p">(</span><span class="n">yolo_nas</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="p">(</span><span class="n">dummy_input</span><span class="p">,</span> <span class="p">))</span><span class="o">.</span><span class="n">module</span><span class="p">()</span>
</pre></div>
</div>
<p>At this phase, the original <code class="docutils literal notranslate"><span class="pre">torch.nn.Module`</span></code> will be translated to <cite>torch.fx.GraphModule`</cite>, which only contains <code class="docutils literal notranslate"><span class="pre">torch.ops.aten</span></code> operators and is fully functional model.</p>
</section>
<section id="set-quantization-config-and-quantizer">
<h3>2.Set quantization Config and Quantizer<a class="headerlink" href="#set-quantization-config-and-quantizer" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># we adopt INT8, power of 2 format scale, symmetric configuration for weight, bias and activation.</span>
<span class="n">INT8_PER_TENSOR_SPEC</span> <span class="o">=</span> <span class="n">QuantizationSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">Dtype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
                                        <span class="n">qscheme</span><span class="o">=</span><span class="n">QSchemeType</span><span class="o">.</span><span class="n">per_tensor</span><span class="p">,</span>
                                        <span class="n">observer_cls</span><span class="o">=</span><span class="n">PerTensorPowOf2MinMaxObserver</span><span class="p">,</span>
                                        <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                        <span class="n">scale_type</span><span class="o">=</span><span class="n">ScaleType</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                                        <span class="n">round_method</span><span class="o">=</span><span class="n">RoundType</span><span class="o">.</span><span class="n">half_even</span><span class="p">,</span>
                                        <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">INT8_PER_TENSOR_SPEC</span><span class="p">,</span>
                                  <span class="n">input_tensors</span><span class="o">=</span><span class="n">INT8_PER_TENSOR_SPEC</span><span class="p">,</span>
                                  <span class="n">output_tensors</span><span class="o">=</span><span class="n">INT8_PER_TENSOR_SPEC</span><span class="p">,</span>
                                  <span class="n">bias</span><span class="o">=</span><span class="n">INT8_PER_TENSOR_SPEC</span><span class="p">)</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span><span class="n">global_quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">,</span>
                      <span class="n">quant_mode</span><span class="o">=</span><span class="n">QuantizationMode</span><span class="o">.</span><span class="n">fx_graph_mode</span><span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">quant_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="calibration-ptq-training-qat-optional">
<h3>3.Calibration (PTQ) / Training (QAT) (Optional)<a class="headerlink" href="#calibration-ptq-training-qat-optional" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PTQ will be performed automatically.</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">graph_model</span><span class="p">,</span> <span class="n">calib_data</span><span class="p">)</span>

<span class="c1"># QAT</span>
<span class="c1"># More Information user can see function train_model</span>
<span class="n">train_model</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</pre></div>
</div>
<p>NOTE: the training result (QAT) rely on training and other parameters.</p>
</section>
<section id="exported-to-onnx-prepare-for-npu-compile">
<h3>4.Exported to Onnx (prepare for NPU compile)<a class="headerlink" href="#exported-to-onnx-prepare-for-npu-compile" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>For AMD’s NPU deployment, there are some hardware constraints (e.g., the quantization scale of two inputs of the <cite>Add</cite> operator has a constraint: the abs(exponent_of_scale1 - exponent_of_scale2) should not be larger than 7).</p></li>
<li><p>As described above, YOLO-NAS has two parts of output called <cite>decoded_predictions</cite> and <cite>raw_predictions</cite>. Before exporting to the <cite>ONNX</cite> model, we need to neglect <cite>raw_predictions</cite> and only maintain the <cite>decoded_predictions</cite> as output.</p></li>
</ol>
</div>
</section>
<section id="preparation-before-export">
<h3>5.Preparation before export<a class="headerlink" href="#preparation-before-export" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Call freeze() will automatically perform the hardware constrain and optimization.</span>
<span class="n">frozen_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">quantized_model</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>

<span class="c1"># Mask the output of raw_predictions</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ModifiedModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ModifiedModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_model</span> <span class="o">=</span> <span class="n">original_model</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">outputs_1</span><span class="p">,</span> <span class="n">outputs_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs_1</span>

<span class="n">modified_mode</span> <span class="o">=</span> <span class="n">ModifiedModel</span><span class="p">(</span><span class="n">frozen_model</span><span class="p">)</span>  <span class="c1"># This model only has output of decoded_predictions, and used for export to ONNX</span>
</pre></div>
</div>
</section>
<section id="export-to-onnx-model">
<h3>6.Export to ONNX model<a class="headerlink" href="#export-to-onnx-model" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># export to onnx model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">export_onnx</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">640</span><span class="p">,</span> <span class="mi">640</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="p">)</span>  <span class="c1"># As NPU compile can better compile with batch-size 1</span>
<span class="n">export_onnx</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">modified_mode</span><span class="p">,</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">export_dir</span><span class="p">,</span>
    <span class="n">input_args</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="visualization-onnx-model-optional">
<h3>7.Visualization ONNX model (Optional)<a class="headerlink" href="#visualization-onnx-model-optional" title="Link to this heading">#</a></h3>
<p>After quantization, visualize the onnx model to check whether quantization meet the desired config. As the generated <cite>ONNX</cite> model is relatively difficult to view and inspect. You can use our supported Python script to simplify the <cite>ONNX</cite> model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">onnxsim</span><span class="w"> </span><span class="kn">import</span> <span class="n">simplify</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">exported_onnx_model</span><span class="p">)</span>
<span class="n">model_simp</span><span class="p">,</span> <span class="n">check</span> <span class="o">=</span> <span class="n">simplify</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
<span class="n">onnx</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">model_simp</span><span class="p">,</span> <span class="s2">&quot;./quant_result/sample_quark_model.onnx&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Then using <cite>netron</cite> to visualize:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>netron<span class="w"> </span>./quant_result/sample_quark_model.onnx
</pre></div>
</div>
</section>
<section id="compile-and-deploy-on-amd-npu-device">
<h3>8.Compile and deploy on AMD NPU device<a class="headerlink" href="#compile-and-deploy-on-amd-npu-device" title="Link to this heading">#</a></h3>
<p>For this session, please refer to AMD NPU compile and deployment documents.</p>
</section>
</section>
<section id="quick-start">
<h2>Quick start<a class="headerlink" href="#quick-start" title="Link to this heading">#</a></h2>
<p>We provide a clean and compact code for a quick start, the user can directly run to quant the YOLO-NAS,</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>yolo_nas_pow2_int_quant.py<span class="w"> </span>--data_dir<span class="o">={</span>DATA_PATH_TO_COCO<span class="o">}</span><span class="w"> </span>--qat
</pre></div>
</div>
</section>
<section id="quantization-result">
<h2>Quantization Result<a class="headerlink" href="#quantization-result" title="Link to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>After we call <code class="docutils literal notranslate"><span class="pre">freeze()</span></code>, it will automatically perform the hardware constraints for NPU compilation.
The test accuracy may change.</p>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model format</p></th>
<th class="head"><p><a class="reference external" href="mailto:mAP&#37;&#52;&#48;0&#46;50">mAP<span>&#64;</span>0<span>&#46;</span>50</a></p></th>
<th class="head"><p><a class="reference external" href="mailto:mAP&#37;&#52;&#48;0&#46;50">mAP<span>&#64;</span>0<span>&#46;</span>50</a>:0.95</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FP32 model</p></td>
<td><p>0.6466</p></td>
<td><p>0.4759</p></td>
</tr>
<tr class="row-odd"><td><p>FP32 prep_model</p></td>
<td><p>0.6466</p></td>
<td><p>0.4759</p></td>
</tr>
<tr class="row-even"><td><p>PTQ</p></td>
<td><p>0.5139</p></td>
<td><p>0.3244</p></td>
</tr>
<tr class="row-odd"><td><p>QAT</p></td>
<td><p>0.5408</p></td>
<td><p>0.3416</p></td>
</tr>
<tr class="row-even"><td><p>Hw align QAT</p></td>
<td><p>0.5408</p></td>
<td><p>0.3415</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Our experiments were conducted under the following environment:
Python=3.9, torch=2.5.0, super_gradients=3.7.1.</p>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="example_quark_fx_image_classification.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Image Classification Models FX Graph Quantization</p>
      </div>
    </a>
    <a class="right-next"
       href="sample_yolo_x_tiny_quant.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">YOLO-X Tiny FX Graph Quantization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#highlight-overview">Highlight Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites-information-knowledge">Prerequisites information/knowledge</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-network-structure">Understanding the Network structure</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-network-output">Understanding the network output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-fx-graph-model">PyTorch FX-Graph model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overall-summary">Overall summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation">Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-ptq-qat-for-yolo-nas">Perform PTQ &amp; QAT for YOLO-NAS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-data-and-model">1.Prepare data and model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#set-quantization-config-and-quantizer">2.Set quantization Config and Quantizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration-ptq-training-qat-optional">3.Calibration (PTQ) / Training (QAT) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exported-to-onnx-prepare-for-npu-compile">4.Exported to Onnx (prepare for NPU compile)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation-before-export">5.Preparation before export</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#export-to-onnx-model">6.Export to ONNX model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-onnx-model-optional">7.Visualization ONNX model (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compile-and-deploy-on-amd-npu-device">8.Compile and deploy on AMD NPU device</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quick-start">Quick start</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization-result">Quantization Result</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Sep 26, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2025 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>