
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Release Notes &#8212; AMD Quark 0.10 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=a66ef196" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_header.css?v=9557e3d1" />
    <link rel="stylesheet" type="text/css" href="_static/rocm_footer.css?v=7095035a" />
    <link rel="stylesheet" type="text/css" href="_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=e0f31c2e"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script defer="defer" src="_static/search.js?v=90a4452c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'release_note';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quantization with AMD Quark" href="intro.html" />
    <link rel="prev" title="Welcome to AMD Quark Documentation!" href="index.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].innerText = "Show code ▼\n\n";

            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">AMD Quark 0.10 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_usage.html">Getting started: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/basic_usage_onnx.html">Getting started: Quark for ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/basic_usage_pytorch.html">Getting started: Quark for PyTorch</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch/pytorch_examples.html">PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_fx_image_classification.html">Image Classification Models FX Graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/sample_yolo_nas_quant.html">YOLO-NAS FX graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/sample_yolo_x_tiny_quant.html">YOLO-X Tiny FX Graph Quantization</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx/onnx_examples.html">ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_layerwise_percentile.html">Layer-wise Percentile</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_auto_search.html">Auto-Search for General Yolov3 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_ryzenai_yolonas.html">Auto-Search for Ryzen AI Yolo-nas ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_ryzenai_autosearch_resnet50.html">Auto-Search for Ryzen AI Resnet50 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_ryzenai_yolov3_custom_evaluator.html">Auto-Search for Ryzen AI Yolov3 ONNX Quantization with Custom Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizing Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_crypto_mode.html">Quantizing a ResNet50 model in crypto mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/hugging_face_timm_quantization.html">Hugging Face TIMM Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_yolo_quantization.html">Yolo_nas and Yolox Quantization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported accelerators</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="supported_accelerators/ryzenai/index.html">AMD Ryzen AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="supported_accelerators/ryzenai/tutorial_quick_start_for_ryzenai.html">Quick Start for Ryzen AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="supported_accelerators/ryzenai/ryzen_ai_best_practice.html">Best Practice for Ryzen AI in AMD Quark ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_ryzenai.html">Auto-Search for Ryzen AI ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="supported_accelerators/ryzenai/tutorial_uint4_oga.html">Quantizing LLMs for ONNX Runtime GenAI</a></li>
<li class="toctree-l2"><a class="reference internal" href="supported_accelerators/ryzenai/tutorial_convert_fp32_or_fp16_to_bf16.html">FP32/FP16 to BF16 Model Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="supported_accelerators/ryzenai/tutorial_xint8_quantize.html">Power-of-Two Scales (XINT8) Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="supported_accelerators/ryzenai/tutorial_a8w8_and_a16w8_quantize.html">Float Scales (A8W8 and A16W8) Quantization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="supported_accelerators/mi_gpus/index.html">AMD Instinct</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_perplexity.html">Evaluation of Quantized Models</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pytorch/user_guide_config_for_llm.html">Configuring PyTorch Quantization for Large Language Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch/user_guide_config_description.html">Configuring PyTorch Quantization from Scratch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pytorch/export/quark_export_onnx.html">ONNX format</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/export/quark_export_hf.html">Hugging Face format (safetensors)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch/export/quark_export_gguf.html">GGUF format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/export/quark_export_quark.html">Quark format</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pytorch/feature_pruning_overall.html">LLM Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="tutorials/torch/example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="tutorials/torch/example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark and Trainer</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/torch/auto_smoothquant_document_and_example.html">Auto SmoothQuant</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch/awq_document.html">Activation-aware Weight Quantization (AWQ)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="tutorials/torch/example_awq.html">AWQ end-to-end demo</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx/custom_operators/ExtendedQuantizeLinear.html">ExtendedQuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/custom_operators/ExtendedDequantizeLinear.html">ExtendedDequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/custom_operators/ExtendedInstanceNormalization.html">ExtendedInstanceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/custom_operators/ExtendedLSTM.html">ExtendedLSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/custom_operators/BFPQuantizeDequantize.html">BFPQuantizeDequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/custom_operators/MXQuantizeDequantize.html">MXQuantizeDequantize</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/tutorial_bfp16_quantization.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/tutorial_microscaling_quantization.html">Microscaling (MX)</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/tutorial_microexponents_quantization.html">Microexponents (MX)</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/accuracy_algorithms/quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="onnx/example_quark_onnx_gptq.html">Quantizing a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/user_guide_auto_search.html">Automatic Search for Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/config/user_guide_onnx_model_inference_save_input_npy.html">Using ONNX Model Inference and Saving Input Data in NPY Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="tutorials/torch/quickstart_tutorial/quickstart_tutorial.html">AMD Quark Tutorial: PyTorch Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/torch/diffusion_tutorial/diffusion_tutorial.html">Quantizing a Diffusion Model using Quark</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/torch/depth_wise_pruning/llm_depth_pruning.html">LLM Model Depth-Wise Pruning (beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/torch/llm_tutorial/llm_tutorial.html">Quantizing a Large Language Model with Quark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Third-party contributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro_contrib.html">Introduction and guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="autoapi/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/torch/quantization/config/template/index.html">Quantizer Template</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autoapi/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="versions.html">AMD Quark release history</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">Quark license</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Release Notes</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Release Notes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-10">Release 0.10</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-9">Release 0.9</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-8-2">Release 0.8.2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#new-features">New Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-8-1">Release 0.8.1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bug-fixes-and-enhancements">Bug Fixes and Enhancements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-8">Release 0.8</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-7">Release 0.7</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">New Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Bug Fixes and Enhancements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-6">Release 0.6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-5-1">Release 0.5.1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-5-0">Release 0.5.0</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-2-0">Release 0.2.0</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-1-0">Release 0.1.0</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="release-notes">
<h1>Release Notes<a class="headerlink" href="#release-notes" title="Link to this heading">#</a></h1>
<section id="release-0-10">
<h2>Release 0.10<a class="headerlink" href="#release-0-10" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p>New Features</p>
<ul class="simple">
<li><p>Support PyTorch 2.7.1 and 2.8.0.</p></li>
<li><p>Support for int3 quantization and exporting of models.</p></li>
<li><p>Support the AWQ algorithm with Gemma3 and Phi4.</p></li>
<li><p>Support Qronos advanced quantization algorithm.</p></li>
<li><p>Applying the <a class="reference external" href="https://quark.docs.amd.com/latest/pytorch/quark_torch_best_practices.html#apply-quantization-algorithms">GPTQ algorithm</a> runs x3-x4 faster compared to AMD Quark 0.9, using <a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/cuda.html#cuda-graph-semantics">CUDA/HIP Graph</a> by default. If requirement, CUDA Graph for GPTQ can be disabled using the environment variable <code class="docutils literal notranslate"><span class="pre">QUARK_GRAPH_DEBUG=0</span></code>.</p></li>
<li><p><a class="reference external" href="https://quark.docs.amd.com/latest/pytorch/tutorial_quarot.html">Quarot</a> algorithm supports a new configuration parameter <code class="docutils literal notranslate"><span class="pre">rotation_size</span></code> to define custom hadamard rotation sizes. Please refer to <a class="reference external" href="https://quark.docs.amd.com/latest/autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QuaRotConfig">QuaRotConfig documentation</a>.</p></li>
<li><p>Support the Qronos post-training quantization algorithm. Please refer to the <a class="reference external" href="https://arxiv.org/abs/2505.11695">arXiv paper</a> and <a class="reference external" href="https://quark.docs.amd.com/latest/autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QronosConfig">Quark documentation</a>.</p></li>
</ul>
</li>
<li><p>QuantizationSpec check:</p>
<ul class="simple">
<li><p>Every time user finishes init <code class="docutils literal notranslate"><span class="pre">QuantizationSpec</span></code> will automatically perform config check. If any invalid config is supplied, a warning or error message will be given to user for better correction. In this way, find  potential error as early as possible rather than cause a runtime error during quantization process.</p></li>
</ul>
</li>
<li><p>LLM Depth-Wise Pruning tool:</p>
<ul class="simple">
<li><p>Depth-wise pruning tool that can decrease the LLM model size. This tool deletes the consecutive decode layers in LLM under a certain supplied pruning ratio.</p></li>
<li><p>Based on PPL influence, the consecutive layers that have less influence on PPL will be regarded as having less influence on LLM and can be deleted.</p></li>
</ul>
</li>
<li><p>Model Support:</p>
<ul class="simple">
<li><p>Support OCP MXFP4, MXFP6, MXFP8 quantization of new models: DeepSeek-R1, Llama4-Scout, Llama4-Maverick, gpt-oss-20b, gpt-oss-120b.</p></li>
</ul>
</li>
<li><p>Deprecations and breaking changes</p>
<ul class="simple">
<li><p>OCP MXFP6 weight packing layout is modified to fit the expected layout by <a class="reference external" href="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-cdna4-instruction-set-architecture.pdf">CDNA4</a> <code class="docutils literal notranslate"><span class="pre">mfma_scale</span></code> instruction.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">examples/language_modeling/llm_ptq/quantize_quark.py</span></code> example, the quantization scheme <cite>“w_mxfp4_a_mxfp6”</cite> is removed and replaced by <cite>“w_mxfp4_a_mxfp6_e2m3”</cite> and <cite>“w_mxfp4_a_mxfp6_e3m2”</cite>.</p></li>
</ul>
</li>
<li><p>Important bug fixes</p>
<blockquote>
<div><ul class="simple">
<li><p>A bug in <a class="reference external" href="https://quark.docs.amd.com/latest/pytorch/tutorial_quarot.html">Quarot</a> and <a class="reference external" href="https://quark.docs.amd.com/latest/pytorch/tutorial_rotation.html">Rotation</a> algorithms where fused rotations were wrongly applied twice on input embeddings / LM head weights is fixed.</p></li>
<li><p>Reduce the slowness of the reloading of large quantized models as DeepSeek-R1 using Transformers + Quark.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</li>
<li><p><strong>AMD Quark for ONNX</strong></p>
<ul class="simple">
<li><p>New Features:</p>
<ul>
<li><p>API Refactor (Introduced the new API design with improved consistency and usability)</p>
<ul>
<li><p>Supported class-based algorithm usage.</p></li>
<li><p>Aligned data type both for Quark Torch and Quark ONNX.</p></li>
<li><p>Refactored quantization configs.</p></li>
</ul>
</li>
<li><p>Auto Search Enhancements</p>
<ul>
<li><p>Two-Stage Search: First identifies the best calibration config, then searches for the optimal FastFinetune config based on it. Expands the search space for higher efficiency.</p></li>
<li><p>Advanced-Fastft Search: Supports continuous search spaces, advanced algorithms (e.g., TPE), and parallel execution for faster, smarter searching.</p></li>
<li><p>Joint-Parameter Search: Combines coupled parameters into a unified space to avoid ineffective configurations and improve search quality.</p></li>
</ul>
</li>
<li><p>Added support for ONNX 1.19 and ONNXRuntime 1.22.1</p></li>
<li><p>Added optimized weight-scale calculation with the MinMSE method to improve quantization accuracy.</p></li>
<li><p>Accelerated calibration with multi-process support, covering algorithms such as MinMSE, Percentile, Entropy, Distribution, and LayerwisePercentile.</p></li>
<li><p>Added progress bars for Percentile, Entropy, Distribution, and LayerwisePercentile algorithms.</p></li>
<li><p>Supported users to specify a directory for saving cache files.</p></li>
</ul>
</li>
<li><p>Enhancements:</p>
<ul>
<li><p>Significantly reduced memory usage across various configurations, including calibration and FastFinetune stages, with optimizations for both CPU and GPU memory.</p></li>
<li><p>Improved clarity of error and warning outputs, helping users select better parameters based on memory and disk conditions.</p></li>
</ul>
</li>
<li><p>Bug fixes and minor improvements:</p>
<ul>
<li><p>Provided actionable hints when OOM or insufficient disk space issues occur in calibration and fast fine-tuning.</p></li>
<li><p>Fixed multi-GPU issues during FastFinetune.</p></li>
<li><p>Fixed a bug related to converting BatchNorm to Conv.</p></li>
<li><p>Fixed a bug in BF16 conversion on models larger than 2GB.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Quark Torch API Refactor</strong></p>
<ul>
<li><p>LLMTemplate for simplified quantization configuration:</p>
<ul>
<li><p>Introduced <a class="reference internal" href="autoapi/quark/torch/quantization/config/template/index.html#quark.torch.quantization.config.template.LLMTemplate" title="quark.torch.quantization.config.template.LLMTemplate"><code class="xref py py-class docutils literal notranslate"><span class="pre">LLMTemplate</span></code></a> class for convenient LLM quantization configuration</p></li>
<li><p>Built-in templates for popular LLM architectures (Llama4, Qwen, Mistral, Phi, DeepSeek, GPT-OSS, etc.)</p></li>
<li><p>Support for multiple quantization schemes: int4/uint4 (group sizes 32, 64, 128), int8, fp8, mxfp4, mxfp6e2m3, mxfp6e3m2, bfp16, mx6</p></li>
<li><p>Advanced features: layer-wise quantization, KV cache quantization, attention quantization</p></li>
<li><p>Algorithm support: AWQ, GPTQ, SmoothQuant, AutoSmoothQuant, Rotation</p></li>
<li><p>Custom template and scheme registration capabilities for users to define their own template and quantization schemes</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMTemplate</span>

<span class="c1"># List available templates</span>
<span class="n">templates</span> <span class="o">=</span> <span class="n">LLMTemplate</span><span class="o">.</span><span class="n">list_available</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">templates</span><span class="p">)</span>  <span class="c1"># [&#39;llama&#39;, &#39;opt&#39;, &#39;qwen&#39;, &#39;mistral&#39;, ...]</span>

<span class="c1"># Get a specific template</span>
<span class="n">llama_template</span> <span class="o">=</span> <span class="n">LLMTemplate</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;llama&quot;</span><span class="p">)</span>

<span class="c1"># Create a basic configuration</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">llama_template</span><span class="o">.</span><span class="n">get_config</span><span class="p">(</span><span class="n">scheme</span><span class="o">=</span><span class="s2">&quot;fp8&quot;</span><span class="p">,</span> <span class="n">kv_cache_scheme</span><span class="o">=</span><span class="s2">&quot;fp8&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</li>
<li><p>Export and import APIs are deprecated in favor of new ones:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ModelExporter.export_safetensors_model</span></code> is deprecated in favor of <code class="docutils literal notranslate"><span class="pre">export_safetensors</span></code>:</p>
<p>Before:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelExporter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.export.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">ExporterConfig</span><span class="p">,</span> <span class="n">JsonExporterConfig</span>

<span class="n">export_config</span> <span class="o">=</span> <span class="n">ExporterConfig</span><span class="p">(</span><span class="n">json_export_config</span><span class="o">=</span><span class="n">JsonExporterConfig</span><span class="p">())</span>
<span class="n">exporter</span> <span class="o">=</span> <span class="n">ModelExporter</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">export_config</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">)</span>
<span class="n">exporter</span><span class="o">.</span><span class="n">export_safetensors_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">quant_config</span><span class="p">)</span>
</pre></div>
</div>
<p>After:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">export_safetensors</span>
<span class="n">export_safetensors</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">ModelImporter.import_model_info</span></code> is deprecated in favor of <code class="docutils literal notranslate"><span class="pre">import_model_from_safetensors</span></code>:</p>
<p>Before:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.export.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelImporter</span>

<span class="n">model_importer</span> <span class="o">=</span> <span class="n">ModelImporter</span><span class="p">(</span>
   <span class="n">model_info_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
   <span class="n">saved_format</span><span class="o">=</span><span class="s2">&quot;safetensors&quot;</span>
<span class="p">)</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">model_importer</span><span class="o">.</span><span class="n">import_model_info</span><span class="p">(</span><span class="n">original_model</span><span class="p">)</span>
</pre></div>
</div>
<p>After:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">import_model_from_safetensors</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">import_model_from_safetensors</span><span class="p">(</span>
   <span class="n">original_model</span><span class="p">,</span>
   <span class="n">model_dir</span><span class="o">=</span><span class="n">export_dir</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Quark ONNX API Refactor</strong></p>
<ul>
<li><p>Before:</p>
<ul class="simple">
<li><p>Basic Usage:</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.custom_config</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_default_config</span>

<span class="n">input_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo.onnx&quot;</span>
<span class="n">quantized_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo_quantized.onnx&quot;</span>
<span class="n">calib_data_path</span> <span class="o">=</span> <span class="s2">&quot;calib_data&quot;</span>
<span class="n">calib_data_reader</span> <span class="o">=</span> <span class="n">ImageDataReader</span><span class="p">(</span><span class="n">calib_data_path</span><span class="p">)</span>

<span class="n">a8w8_config</span> <span class="o">=</span> <span class="n">get_default_config</span><span class="p">(</span><span class="s2">&quot;A8W8&quot;</span><span class="p">)</span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span><span class="n">global_quant_config</span><span class="o">=</span><span class="n">a8w8_config</span> <span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">,</span> <span class="n">quantized_model_path</span><span class="p">,</span> <span class="n">calib_data_reader</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><p>Advanced Usage:</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span><span class="p">,</span> <span class="n">QuantizationConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime.quantization.calibrate</span><span class="w"> </span><span class="kn">import</span> <span class="n">CalibrationMethod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime.quantization.quant_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantFormat</span><span class="p">,</span> <span class="n">QuantType</span><span class="p">,</span> <span class="n">ExtendedQuantType</span>

<span class="n">input_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo.onnx&quot;</span>
<span class="n">quantized_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo_quantized.onnx&quot;</span>
<span class="n">calib_data_path</span> <span class="o">=</span> <span class="s2">&quot;calib_data&quot;</span>
<span class="n">calib_data_reader</span> <span class="o">=</span> <span class="n">ImageDataReader</span><span class="p">(</span><span class="n">calib_data_path</span><span class="p">)</span>

<span class="n">DEFAULT_ADAROUND_PARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;DataSize&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s2">&quot;FixedSeed&quot;</span><span class="p">:</span> <span class="mi">1705472343</span><span class="p">,</span>
    <span class="s2">&quot;BatchSize&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;NumIterations&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s2">&quot;LearningRate&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="s2">&quot;OptimAlgorithm&quot;</span><span class="p">:</span> <span class="s2">&quot;adaround&quot;</span><span class="p">,</span>
    <span class="s2">&quot;OptimDevice&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="s2">&quot;InferDevice&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="s2">&quot;EarlyStop&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">Percentile</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">QuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">QInt8</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">QInt8</span><span class="p">,</span>
    <span class="n">nodes_to_exclude</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;/layer.2/Conv_1&quot;</span><span class="p">,</span> <span class="s2">&quot;^/Conv/.*&quot;</span><span class="p">],</span>
    <span class="n">subgraphs_to_exclude</span><span class="o">=</span><span class="p">[([</span><span class="s2">&quot;start_node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;start_node_2&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;end_node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;end_node_2&quot;</span><span class="p">])],</span>
    <span class="n">include_cle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">include_fast_ft</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">specific_tensor_precision</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">use_external_data_format</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;MixedPrecisionTensor&quot;</span><span class="p">:</span> <span class="p">{</span><span class="n">ExtendedQuantType</span><span class="o">.</span><span class="n">QInt16</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;/layer.0/Conv_0&quot;</span><span class="p">,</span> <span class="s2">&quot;/layer.11/Conv_2&quot;</span><span class="p">]},</span>
        <span class="s2">&quot;CLESteps&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;FastFinetune&quot;</span><span class="p">:</span> <span class="n">DEFAULT_ADAROUND_PARAMS</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span><span class="n">global_quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">,</span> <span class="n">quantized_model_path</span><span class="p">,</span> <span class="n">calib_data_reader</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>After:</p>
<ul class="simple">
<li><p>Basic Usage:</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">QConfig</span>

<span class="n">input_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo.onnx&quot;</span>
<span class="n">quantized_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo_quantized.onnx&quot;</span>
<span class="n">calib_data_path</span> <span class="o">=</span> <span class="s2">&quot;calib_data&quot;</span>
<span class="n">calib_data_reader</span> <span class="o">=</span> <span class="n">ImageDataReader</span><span class="p">(</span><span class="n">calib_data_path</span><span class="p">)</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">QConfig</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">(</span><span class="s2">&quot;A8W8&quot;</span><span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">,</span> <span class="n">quantized_model_path</span><span class="p">,</span> <span class="n">calib_data_reader</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><p>Advanced Usage:</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">QConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.spec</span><span class="w"> </span><span class="kn">import</span> <span class="n">QLayerConfig</span><span class="p">,</span> <span class="n">Int8Spec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.data_type</span><span class="w"> </span><span class="kn">import</span> <span class="n">Int16</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.algorithm</span><span class="w"> </span><span class="kn">import</span> <span class="n">CLEConfig</span><span class="p">,</span> <span class="n">AdaRoundConfig</span>

<span class="n">input_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo.onnx&quot;</span>
<span class="n">quantized_model_path</span> <span class="o">=</span> <span class="s2">&quot;demo_quantized.onnx&quot;</span>
<span class="n">calib_data_path</span> <span class="o">=</span> <span class="s2">&quot;calib_data&quot;</span>
<span class="n">calib_data_reader</span> <span class="o">=</span> <span class="n">ImageDataReader</span><span class="p">(</span><span class="n">calib_data_path</span><span class="p">)</span>

<span class="n">int8_config</span> <span class="o">=</span> <span class="n">QLayerConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">Int8Spec</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">Int8Spec</span><span class="p">)</span>
<span class="n">cle_algo</span> <span class="o">=</span> <span class="n">CLEConfig</span><span class="p">(</span><span class="n">cle_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">adaround_algo</span> <span class="o">=</span> <span class="n">AdaRoundConfig</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">global_config</span><span class="o">=</span><span class="n">int8_config</span><span class="p">,</span>
    <span class="n">specific_layer_config</span><span class="o">=</span><span class="p">{</span><span class="n">Int16</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;/layer.0/Conv_0&quot;</span><span class="p">,</span> <span class="s2">&quot;/layer.11/Conv_2&quot;</span><span class="p">]},</span>
    <span class="n">layer_type_config</span><span class="o">=</span><span class="p">{</span><span class="n">Int16</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;MatMul&quot;</span><span class="p">]</span> <span class="kc">None</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Gemm&quot;</span><span class="p">]},</span>
    <span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;/layer.2/Conv_1&quot;</span><span class="p">,</span> <span class="s2">&quot;^/Conv/.*&quot;</span><span class="p">,</span> <span class="p">([</span><span class="s2">&quot;start_node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;start_node_2&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;end_node_1&quot;</span><span class="p">,</span> <span class="s2">&quot;end_node_2&quot;</span><span class="p">])],</span>
    <span class="n">algo_config</span><span class="o">=</span><span class="p">[</span><span class="n">cle_algo</span><span class="p">,</span> <span class="n">adaround_algo</span><span class="p">],</span>
    <span class="n">use_external_data_format</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">,</span> <span class="n">quantized_model_path</span><span class="p">,</span> <span class="n">calib_data_reader</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</li>
</ul>
</section>
<section id="release-0-9">
<h2>Release 0.9<a class="headerlink" href="#release-0-9" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p>New Features</p>
<ul class="simple">
<li><p>OCP MXFP4 fake quantization and dequantization kernels</p>
<ul>
<li><p>Efficient kernels are added to Quark’s <cite>torch/kernel/hw_emulation/csrc</cite> for OCP MXFP4 quantization and dequantization. They are useful to simulate OCP MXFP4 workload on hardware that does not support natively this data type (e.g. MI300X GPUs).</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Quantized models can be reloaded with no memory overhead</p>
<blockquote>
<div><ul class="simple">
<li><p>The method <code class="docutils literal notranslate"><span class="pre">ModelImporter.import_model_info</span></code> used to reload a quantized model checkpoint now supports using a non-quantized backbone placed on  <code class="docutils literal notranslate"><span class="pre">torch.device(&quot;meta&quot;)</span></code> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/meta.html">see PyTorch reference</a>) device, avoiding the memory overhead of instantiating the non-quantized model on device. More details are available <a class="reference external" href="https://quark.docs.amd.com/latest/pytorch/export/quark_export_hf.html#loading-quantized-models-saved-in-hugging-face-format-safetensors-format">here</a>.</p></li>
</ul>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.export.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelImporter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">model_importer</span> <span class="o">=</span> <span class="n">ModelImporter</span><span class="p">(</span>
   <span class="n">model_info_dir</span><span class="o">=</span><span class="s2">&quot;./opt-125m-quantized&quot;</span><span class="p">,</span>
   <span class="n">saved_format</span><span class="o">=</span><span class="s2">&quot;safetensors&quot;</span>
<span class="p">)</span>

<span class="c1"># We only need the backbone/architecture of the original model,</span>
<span class="c1"># not its weights, as weights are loaded from the quantized checkpoint.</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
   <span class="n">original_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">model_importer</span><span class="o">.</span><span class="n">import_model_info</span><span class="p">(</span><span class="n">original_model</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
</li>
<li><p>Deprecations and breaking changes</p>
<ul class="simple">
<li><p>Some quantization schemes in AMD Quark LLM PTQ example are deprecated (<a class="reference external" href="https://quark.docs.amd.com/latest/pytorch/example_quark_torch_llm_ptq.html">see torch LLM PTQ reference</a>):</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_fp4_a_mx_fp4_sym</span></code> is deprecated in favor of: <code class="docutils literal notranslate"><span class="pre">w_mxfp4_a_mxfp4</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_fp6_e3m2_sym</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxfp6_e3m2</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_fp6_e2m3_sym</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxfp6_e2m3</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_int8_per_group_sym</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxint8</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mxfp4_a_mxfp4_sym</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxfp4_a_mxfp4</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_fp6_e2m3_a_mx_fp6_e2m3</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxfp6_e2m3_a_mxfp6_e2m3</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_fp6_e3m2_a_mx_fp6_e3m2</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxfp6_e3m2_a_mxfp6_e3m2</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_fp4_a_mx_fp6_sym</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxfp4_a_mxfp6</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">w_mx_fp8_a_mx_fp8</span></code> in favor of <code class="docutils literal notranslate"><span class="pre">w_mxfp8_a_mxfp8</span></code>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Bug fixes and minor improvements</p>
<ul class="simple">
<li><p>Fake quantization methods for FP4 and FP6 are made compatible with CUDA Graph.</p></li>
<li><p>A summary of replaced modules for quantization is displayed when calling <code class="docutils literal notranslate"><span class="pre">ModelQuantizer.quantize_model</span></code> for easier inspection.</p></li>
</ul>
</li>
<li><p>Model Support:</p>
<ul class="simple">
<li><p>Support Gemma2 in OGA flow.</p></li>
</ul>
</li>
<li><p>Quantization and Export:</p>
<ul class="simple">
<li><p>Support quantization and export of models in MXFP settings, e.g. MXFP4, MXFP6.</p></li>
<li><p>Support sequential quantization, e.g. W-A-MXFP4+Scale-FP8e4m3.</p></li>
<li><p>Support more models with FP8 attention: OPT, LLaMA, Phi, Mixtral.</p></li>
</ul>
</li>
<li><p>Algorithms:</p>
<ul class="simple">
<li><p>Support GPTQ for MXFP4 Quantization.</p></li>
<li><p>QAT Enhancements using huggingface Trainer.</p></li>
<li><p>Fix AWQ implementation for qkv-packed MHA model (e.g., microsoft/Phi-3-mini-4k-instruct) and raise warning to users if using incorrect or unknown AWQ configurations.</p></li>
</ul>
</li>
<li><p>Performance:</p>
<ul class="simple">
<li><p>Speedup model export.</p></li>
<li><p>Accelerate FP8 inference acceleration.</p></li>
<li><p>Tensor parallelism for evaluation of quantized model.</p></li>
<li><p>Multi-device quantization as well as export.</p></li>
</ul>
</li>
<li><p>FX Graph quantization:</p>
<ul class="simple">
<li><p>Improve efficiency of power-of-2 scale quantization for less memory and faster computation.</p></li>
<li><p>Support channel-wise power-of-2 quantization by using per-channel MSE/NON-overflow observer.</p></li>
<li><p>Support Conv’s Bias for int32 power-of-2 quantization, where bias’s scale = weight’s scale * activation’s scale.</p></li>
<li><p>Support export of INT16/INT32 quantization model to ONNX format and the corresponding ONNXRuntime.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>AMD Quark for ONNX</strong></p>
<ul class="simple">
<li><p>New Features:</p>
<ul>
<li><p>Introduced an encrypted mode for scenarios demanding high model confidentiality.</p></li>
<li><p>Supported fixing the shape of all tensors.</p></li>
<li><p>Supported quantization with int16 bias.</p></li>
</ul>
</li>
<li><p>Enhancements:</p>
<ul>
<li><p>Supported compatibility with ONNX Runtime version 1.21.x and 1.22.0.</p></li>
<li><p>Reduced CPU/GPU memory usage to prevent OOM.</p></li>
<li><p>Improved auto search efficiency by utilizing a cached datareader.</p></li>
<li><p>Enhanced multi-platform support: now supports Windows (CPU/CUDA) and Linux (CPU/CUDA/ROCm).</p></li>
</ul>
</li>
<li><p>Examples:</p>
<ul>
<li><p>Provided quantization examples of TIMM models.</p></li>
</ul>
</li>
<li><p>Documentation:</p>
<ul>
<li><p>Added specifications for all custom operators.</p></li>
<li><p>Improved FAQ documentation.</p></li>
</ul>
</li>
<li><p>Custom Operations:</p>
<ul>
<li><p>Renamed custom operation types and updated their domain to the com.amd.quark:</p>
<ul>
<li><p>BFPFixNeuron → BFPQuantizeDequantize.</p></li>
<li><p>MXFixNeuron → MXQuantizeDequantize.</p></li>
<li><p>VitisQuantFormat and VitisQuantType → ExtendedQuantFormat and ExtendedQuantType.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Bug fixes and minor improvements</p>
<ul>
<li><p>Fixed the issue where extremely large or small values caused -inf/inf during scale calculation.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="release-0-8-2">
<h2>Release 0.8.2<a class="headerlink" href="#release-0-8-2" title="Link to this heading">#</a></h2>
<section id="new-features">
<h3>New Features<a class="headerlink" href="#new-features" title="Link to this heading">#</a></h3>
<p><strong>AMD Quark for PyTorch</strong></p>
<ul class="simple">
<li><p>Added support for ONNX Runtime 1.22.0</p></li>
</ul>
</section>
</section>
<section id="release-0-8-1">
<h2>Release 0.8.1<a class="headerlink" href="#release-0-8-1" title="Link to this heading">#</a></h2>
<section id="bug-fixes-and-enhancements">
<h3>Bug Fixes and Enhancements<a class="headerlink" href="#bug-fixes-and-enhancements" title="Link to this heading">#</a></h3>
<p><strong>AMD Quark for ONNX</strong></p>
<ul class="simple">
<li><p>Fixed BFP Kernel compilation issue for GCC 13</p></li>
</ul>
</section>
</section>
<section id="release-0-8">
<h2>Release 0.8<a class="headerlink" href="#release-0-8" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p>Model Support:</p>
<ul>
<li><p>Supported SD3.0 quantization with W-INT4, W-INT8-A-INT8, and W-FP8-A-FP8.</p></li>
<li><p>Supported FLUX.1 quantization with W-INT4, W-INT8-A-INT8, and W-FP8-A-FP8.</p></li>
<li><p>Supported DLRM embedding-bag UINT4 weight quantization.</p></li>
</ul>
</li>
<li><p>Quantization Enhancement:</p>
<ul>
<li><p>Supported fp8 attention quantization of Llama Family.</p></li>
<li><p>Integrated SmoothQuant algorithm for SDXL.</p></li>
<li><p>Enabled quantization for all SDXL components (UNet, VAE, text_encoder, text_encoder_2), supporting both W-INT8-A-INT8 and W-FP8-A-FP8 formats.</p></li>
</ul>
</li>
<li><p>Model Export:</p>
<ul>
<li><p>Exported diffusion models (SDXL, SDXL-Turbo and SD1.5) to ONNX format via optimum.</p></li>
</ul>
</li>
<li><p>Model Evaluation:</p>
<ul>
<li><p>Added Rouge and Meteor evaluation metrics for LLMs.</p></li>
<li><p>Supported evaluating ONNX models exported using torch.onnx.export for LLMs.</p></li>
<li><p>Supported offline evaluation mode (evaluation without generation) for LLMs.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>AMD Quark for ONNX</strong></p>
<ul>
<li><p>Model Support:</p>
<ul>
<li><p>Provided more ONNX quantization examples of detection models such as yolov7/yolov8.</p></li>
</ul>
</li>
<li><p>Data Types:</p>
<ul>
<li><p>Supported Microexponents (MX) data types, including MX4, MX6 and MX9.</p></li>
<li><p>Enhanced BFloat16 with more implementation formats suitable for deployment.</p></li>
</ul>
</li>
<li><p>ONNX Quantizer Enhancements:</p>
<ul>
<li><p>Supported compatibility with ONNX Runtime version 1.20.0 and 1.20.1.</p></li>
<li><p>Supported quantization with excluding subgraphs.</p></li>
<li><p>Enhanced mixed precision to support quantizing a model with any two data types.</p></li>
</ul>
</li>
<li><p>Documentation Enhancements:</p>
<ul>
<li><p>Supported Best Practice for Quark ONNX.</p></li>
<li><p>Supported documentation of converting from FP32/FP16 to BF16.</p></li>
<li><p>Supported documentation of XINT8, A8W8 and A16W8 quantization.</p></li>
</ul>
</li>
<li><p>Custom Operations:</p>
<ul>
<li><p>Optimized the customized “QuantizeLinear” and “DequantizeLinear” to support running on GPU.</p></li>
</ul>
</li>
<li><p>Advanced Quantization Algorithms:</p>
<ul>
<li><p>Supported Quarot Rotation R1 algorithm.</p></li>
<li><p>Improved AdaQuant algorithm to support Microexponents and Microscaling data types.</p></li>
<li><p>Added auto-search algorithm to automatically find the optimal quantized model with the best accuracy within the search space.</p></li>
<li><p>Enhanced the LLM quantization by using EMA algorithm.</p></li>
</ul>
</li>
<li><p>Model Evaluation:</p>
<ul>
<li><p>Supported evaluation of L2/PSNR/VMAF/COS.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="release-0-7">
<h2>Release 0.7<a class="headerlink" href="#release-0-7" title="Link to this heading">#</a></h2>
<section id="id2">
<h3>New Features<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p><strong>PyTorch</strong></p>
<ul>
<li><p>Added quantization error statistics collection tool.</p></li>
<li><p>Added support for reloading quantized models using <cite>load_state_dict</cite>.</p></li>
<li><p>Added support for W8A8 quantization for the Llama-3.1-8B-Instruct example.</p></li>
<li><p>Added option of saving metrics to CSV in examples.</p></li>
<li><p>Added support for HuggingFace integration.</p></li>
<li><p>Added support for more models</p>
<blockquote>
<div><ul class="simple">
<li><p>Added support for Gemma2 quantization using the OGA flow.</p></li>
<li><p>Added support for Llama-3.2 with FP8 quantization (weight, activation and KV-Cache) for the vision and language components.</p></li>
<li><p>Added support for Stable Diffusion v1-5 and Stable Diffusion XL Base 1.0</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p><strong>ONNX</strong></p>
<ul class="simple">
<li><p>Added a tool to replace BFloat16 QDQ with Cast op.</p></li>
<li><p>Added support for rouge and meteor evaluation metrics.</p></li>
<li><p>Added a feature to fuse Gelu ops into a single Gelu op.</p></li>
<li><p>Added the HQQ algorithm for MatMulNBits.</p></li>
<li><p>Added a tool to convert opset version.</p></li>
<li><p>Added support for fast fine-tuning BF16 quantized models.</p></li>
<li><p>Added U8U8_AAWA and some other built-in configurations.</p></li>
</ul>
</section>
<section id="id3">
<h3>Bug Fixes and Enhancements<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p><strong>PyTorch</strong></p>
<ul class="simple">
<li><p>Enhanced LLM examples to support layer group size customization.</p></li>
<li><p>Decoupled model inference from LLM evaluation harness.</p></li>
<li><p>Fixed OOM issues when quantizing the entire SDXL pipeline.</p></li>
<li><p>Fixed LLM eval bugs caused by export and multi-gpu usage.</p></li>
<li><p>Fixed QAT functionality.</p></li>
<li><p>Addressed AWQ preparation issues.</p></li>
<li><p>Fixed mismatching QDQ implementation compared to Torch.</p></li>
<li><p>Enhanced readability and added docstring for graph quantization.</p></li>
<li><p>Fixed config retrieval by name pattern.</p></li>
<li><p>Supported more Torch versions for auto config rotation.</p></li>
<li><p>Refactored dataloader of algorithms.</p></li>
<li><p>Fixed accuracy issues with Qwen2-MOE.</p></li>
<li><p>Fixed upscaling of scales during the export of quantized models.</p></li>
<li><p>Added support for reloading per-layer quantization config.</p></li>
<li><p>Fixed misleading code in <cite>ModelQuantizer._do_calibration</cite> for weight-only quantization.</p></li>
<li><p>Implemented transpose scales for per-group quantization for int8/uint8.</p></li>
<li><p>Implemented export and load for compressed models.</p></li>
<li><p>Fixed auto config rotation compatibility for more PyTorch versions.</p></li>
<li><p>Fixed bug in input of get_config in exporter.</p></li>
<li><p>Fixed bug in input of the eval_model function.</p></li>
<li><p>Refactored LLM PTQ examples.</p></li>
<li><p>Fixed infer_pack_shape function.</p></li>
<li><p>Documented smoothquant alpha and warned users about possible undesired values.</p></li>
<li><p>Fixed slightly misleading code in <cite>ModelQuantizer._do_calibration</cite>.</p></li>
<li><p>Aligned ONNX mean 2 GAP.</p></li>
</ul>
<p><strong>ONNX</strong></p>
<ul class="simple">
<li><p>Refactored documentation for LLM evaluations.</p></li>
<li><p>Fixed NaN issues caused by overflow for BF16 quantization.</p></li>
<li><p>Fixed an issue where trying to fast fine-tune the MatMul layers without weights.</p></li>
<li><p>Updated ONNX unit tests to use temporary paths.</p></li>
<li><p>Removed generated model “sym_shape_infer_temp.onnx” on infer_shape failure.</p></li>
<li><p>Fixed error in mixed-precision weights calculation.</p></li>
<li><p>Fixed a bug when simplifying Llama2-7b without kv_cache.</p></li>
<li><p>Fixed import path and add parent directory to system path in BFP quantize_model.py example.</p></li>
</ul>
</section>
</section>
<section id="release-0-6">
<h2>Release 0.6<a class="headerlink" href="#release-0-6" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p>Model Support:</p>
<ul>
<li><p>Provided more examples of LLM PTQ, such as Llama3.2 and Llama3.2-Vision models (only quantizing the language part).</p></li>
<li><p>Provided examples of Phi and ChatGLM for LLM QAT.</p></li>
<li><p>Provided examples of LLM pruning for Qwen2.5, Llama, OPT, CohereForAI/c4ai-command models.</p></li>
<li><p>Provided an example of YOLO-NAS, a detection model PTQ/QAT, which can partially quantize the model using your configuration under FX mode.</p></li>
<li><p>Provided an example of SDXL v1.0 with weight INT8 activation INT8 under Eager Mode.</p></li>
<li><p>Supported more models for rotation, such as Qwen models under Eager Mode.</p></li>
</ul>
</li>
<li><p>PyTorch Quantizer Enhancements:</p>
<ul>
<li><p>Supported partially quantizing the model by your config under FX mode.</p></li>
<li><p>Supported quantization of <code class="docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code> in Eager Mode and FX mode.</p></li>
<li><p>Advanced Quantization Algorithms: Improved rotation by auto-generating configurations.</p></li>
<li><p>Optimized Configuration with DataTypeSpec for ease of use.</p></li>
<li><p>Accelerated in-place replacement under Eager Mode.</p></li>
<li><p>Supported loading configuration from a file of algorithms and pre-optimizations under Eager Mode.</p></li>
</ul>
</li>
<li><p>Evaluation:</p>
<ul>
<li><p>Provided LLM evaluation method of quantized models on benchmark tasks: Open LLM Leaderboard and more such.</p></li>
</ul>
</li>
<li><p>Export Capabilities:</p>
<ul>
<li><p>Integrated the export configurations into the Quark format export content, standardizing the pack method for per-group quantization.</p></li>
</ul>
</li>
<li><p>PyTorch Pruning:</p>
<ul>
<li><p>Supported LLM pruning algorithm.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>AMD Quark for ONNX</strong></p>
<ul>
<li><p>Model Support:</p>
<ul>
<li><p>Provided more ONNX quantization examples of LLM models such as Llama2.</p></li>
</ul>
</li>
<li><p>Data Types:</p>
<ul>
<li><p>Supported int4 and uint4 data types.</p></li>
<li><p>Supported Microscaling (MX) data types with <code class="docutils literal notranslate"><span class="pre">int8</span></code>, <code class="docutils literal notranslate"><span class="pre">fp8_e4m3fn</span></code>, <code class="docutils literal notranslate"><span class="pre">fp8_e5m2</span></code>, <code class="docutils literal notranslate"><span class="pre">fp6_e3m2</span></code>, <code class="docutils literal notranslate"><span class="pre">fp6_e2m3</span></code>, and <code class="docutils literal notranslate"><span class="pre">fp4</span> <span class="pre">elements</span></code>.</p></li>
</ul>
</li>
<li><p>ONNX Quantizer Enhancements:</p>
<ul>
<li><p>Supported compatibility with ONNX Runtime version 1.19.</p></li>
<li><p>Supported MatMulNBits quantization for LLM models.</p></li>
<li><p>Supported fast fine-tuning on the MatMul operator.</p></li>
<li><p>Supported quantizing specified operators.</p></li>
<li><p>Supported quantization type alignment of element-wise operators.</p></li>
<li><p>Supported ONNX graph cleaning for Ryzen AI workflow.</p></li>
<li><p>Supported int32 bias quantization for Ryzen AI workflow.</p></li>
<li><p>Enhanced support for Windows systems and ROCm GPU.</p></li>
<li><p>Optimized the quantization of FP16 models to save memory.</p></li>
<li><p>Optimized the custom operator compilation process.</p></li>
<li><p>Optimized the default parameters for auto mixed precision.</p></li>
</ul>
</li>
<li><p>Advanced Quantization Algorithms:</p>
<ul>
<li><p>Supported GPTQ for both QDQ format and MatMulNBits format.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="release-0-5-1">
<h2>Release 0.5.1<a class="headerlink" href="#release-0-5-1" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p>Export Modifications:</p>
<ul>
<li><p>Ignore the configuration of preprocessing algorithms when exporting JSON-safetensors format</p></li>
<li><p>Remove sub-directory in the exporting path.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>AMD Quark for ONNX</strong></p>
<ul>
<li><p>ONNX Quantizer Enhancements:</p>
<ul>
<li><p>Supported compatibility with onnxruntime version 1.19.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="release-0-5-0">
<h2>Release 0.5.0<a class="headerlink" href="#release-0-5-0" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p>Model Support:</p>
<ul>
<li><p>Provided more examples of LLM models quantization:</p>
<ul>
<li><p>INT/OCP_FP8E4M3: Llama-3.1, gpt-j-6b, Qwen1.5-MoE-A2.7B, phi-2, Phi-3-mini, Phi-3.5-mini-instruct, Mistral-7B-v0.1</p></li>
<li><p>OCP_FP8E4M3: mistralai/Mixtral-8x7B-v0.1, hpcai-tech/grok-1, CohereForAI/c4ai-command-r-plus-08-2024, CohereForAI/c4ai-command-r-08-2024, CohereForAI/c4ai-command-r-plus, CohereForAI/c4ai-command-r-v01, databricks/dbrx-instruct, deepseek-ai/deepseek-moe-16b-chat</p></li>
</ul>
</li>
<li><p>Provided more examples of diffusion model quantization:</p>
<ul>
<li><p>Supported models: SDXL, SDXL-Turbo, SD1.5, Controlnet-Canny-SDXL, Controlnet-Depth-SDXL, Controlnet-Canny-SD1.5</p></li>
<li><p>Supported schemes: FP8, W8, W8A8 with and without SmoothQuant</p></li>
</ul>
</li>
</ul>
</li>
<li><p>PyTorch Quantizer Enhancements:</p>
<ul>
<li><p>Supported more CNN models for graph mode quantization.</p></li>
</ul>
</li>
<li><p>Data Types:</p>
<ul>
<li><p>Supported BFP16, MXFP8_E5M2.</p></li>
<li><p>Supported MX6 and MX9. (experimental)</p></li>
</ul>
</li>
<li><p>Advanced Quantization Algorithms:</p>
<ul>
<li><p>Supported Rotation for Llama models.</p></li>
<li><p>Supported SmoothQuant and AWQ for models with GQA and MQA (for example, Llama-3-8B, QWen2-7B).</p></li>
<li><p>Provided scripts for generating AWQ configuration automatically.(experimental)</p></li>
<li><p>Supported trained quantization thresholds (TQT) and learned step size quantization (LSQ) for better QAT results. (experimental)</p></li>
</ul>
</li>
<li><p>Export Capabilities:</p>
<ul>
<li><p>Supported reloading function of JSON-safetensors export format.</p></li>
<li><p>Enhanced quantization configuration in JSON-safetensors export format.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>AMD Quark for ONNX</strong></p>
<ul>
<li><p>ONNX Quantizer Enhancements:</p>
<ul>
<li><p>Supported compatibility with onnxruntime version 1.18.</p></li>
<li><p>Enhanced quantization support for LLM models.</p></li>
</ul>
</li>
<li><p>Quantization Strategy:</p>
<ul>
<li><p>Supported dynamic quantization.</p></li>
</ul>
</li>
<li><p>Custom operations:</p>
<ul>
<li><p>Optimized “BFPFixNeuron” to support running on GPU.</p></li>
</ul>
</li>
<li><p>Advanced Quantization Algorithms:</p>
<ul>
<li><p>Improved AdaQuant to support BFP data types.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="release-0-2-0">
<h2>Release 0.2.0<a class="headerlink" href="#release-0-2-0" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p><strong>PyTorch Quantizer Enhancements</strong>:</p>
<ul>
<li><p>Post Training Quantization (PTQ) and Quantization-Aware Training (QAT) are now supported in FX graph mode.</p></li>
<li><p>Introduced quantization support of the following modules: torch.nn.Conv2d.</p></li>
</ul>
</li>
<li><p><strong>Data Types</strong>:</p>
<ul>
<li><p><a class="reference internal" href="pytorch/adv_mx.html"><span class="doc">OCP Microscaling (MX) is supported. Valid element data types include INT8, FP8_E4M3, FP4, FP6_E3M2, and FP6_E2M3.</span></a></p></li>
</ul>
</li>
<li><p><strong>Export Capabilities</strong>:</p>
<ul>
<li><p><a class="reference internal" href="pytorch/export/gguf_llamacpp.html"><span class="doc">Quantized models can now be exported in GGUF format. The exported GGUF model is runnable with llama.cpp. Only Llama2 is supported for now.</span></a></p></li>
<li><p>Introduced Quark’s native JSON-safetensors export format, which is identical to AutoFP8 and AutoAWQ when used for FP8 and AWQ quantization.</p></li>
</ul>
</li>
<li><p><strong>Model Support</strong>:</p>
<ul>
<li><p>Added support for SDXL model quantization in eager mode, including fp8 per-channel and per-tensor quantization.</p></li>
<li><p>Added support for PTQ and QAT of CNN models in graph mode, including architectures like ResNet.</p></li>
</ul>
</li>
<li><p><strong>Integration with other toolkits</strong>:</p>
<ul>
<li><p>Provided the integrated example with APL (AMD Pytorch-light, internal project name), supporting the invocation of APL’s INT-K, BFP16, and BRECQ.</p></li>
<li><p>Introduced the experimental Quark extension interface, enabling seamless integration of Brevitas for Stable Diffusion and Imagenet classification model quantization.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>AMD Quark for ONNX</strong></p>
<ul>
<li><p><strong>ONNX Quantizer Enhancements</strong>:</p>
<ul>
<li><p>Multiple optimization and refinement strategies for different deployment backends.</p></li>
<li><p>Supported automatic mixing precision to balance accuracy and performance.</p></li>
</ul>
</li>
<li><p><strong>Quantization Strategy</strong>:</p>
<ul>
<li><p>Supported symmetric and asymmetric quantization.</p></li>
<li><p>Supported float scale, INT16 scale and power-of-two scale.</p></li>
<li><p>Supported static quantization and weight-only quantization.</p></li>
</ul>
</li>
<li><p><strong>Quantization Granularity</strong>:</p>
<ul>
<li><p>Supported for per-tensor and per-channel granularity.</p></li>
</ul>
</li>
<li><p><strong>Data Types</strong>:</p>
<ul>
<li><p>Multiple data types are supported, including INT32/UINT32,
Float16, Bfloat16, INT16/UINT16, INT8/UINT8 and BFP.</p></li>
</ul>
</li>
<li><p><strong>Calibration Methods</strong>:</p>
<ul>
<li><p>MinMax, Entropy and Percentile for float scale.</p></li>
<li><p>MinMax for INT16 scale.</p></li>
<li><p>NonOverflow and MinMSE for power-of-two scale.</p></li>
</ul>
</li>
<li><p><strong>Custom operations</strong>:</p>
<ul>
<li><p>“BFPFixNeuron” which supports block floating-point data type. It can run on the CPU on Windows, and on both the CPU and GPU on Linux.</p></li>
<li><p>“VitisQuantizeLinear” and “VitisDequantizeLinear” which support INT32/UINT32, Float16, Bfloat16, INT16/UINT16 quantization.</p></li>
<li><p>“VitisInstanceNormalization” and “VitisLSTM” which have customized Bfloat16 kernels.</p></li>
<li><p>All custom operations support running on the CPU on both Linux and Windows.</p></li>
</ul>
</li>
<li><p><strong>Advanced Quantization Algorithms</strong>:</p>
<ul>
<li><p>Supported CLE, BiasCorrection, AdaQuant, AdaRound and SmoothQuant.</p></li>
</ul>
</li>
<li><p><strong>Operating System Support</strong>:</p>
<ul>
<li><p>Linux and Windows.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="release-0-1-0">
<h2>Release 0.1.0<a class="headerlink" href="#release-0-1-0" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>AMD Quark for PyTorch</strong></p>
<ul>
<li><p><strong>Pytorch Quantizer Enhancements</strong>:</p>
<ul>
<li><p>Eager mode is supported.</p></li>
<li><p>Post Training Quantization (PTQ) is now available.</p></li>
<li><p>Automatic in-place replacement of nn.module operations.</p></li>
<li><p>Quantization of the following modules is supported: torch.nn.linear.</p></li>
<li><p>The customizable calibration process is introduced.</p></li>
</ul>
</li>
<li><p><strong>Quantization Strategy</strong>:</p>
<ul>
<li><p>Symmetric and asymmetric quantization are supported.</p></li>
<li><p>Weight-only, dynamic, and static quantization modes are available.</p></li>
</ul>
</li>
<li><p><strong>Quantization Granularity</strong>:</p>
<ul>
<li><p>Support for per-tensor, per-channel, and per-group granularity.</p></li>
</ul>
</li>
<li><p><strong>Data Types</strong>:</p>
<ul>
<li><p>Multiple data types are supported, including float16, bfloat16, int4, uint4, int8, and fp8 (e4m3fn).</p></li>
</ul>
</li>
<li><p><strong>Calibration Methods</strong>:</p>
<ul>
<li><p>MinMax, Percentile, and MSE calibration methods are now supported.</p></li>
</ul>
</li>
<li><p><strong>Large Language Model Support</strong>:</p>
<ul>
<li><p>FP8 KV-cache quantization for large language models (LLMs).</p></li>
</ul>
</li>
<li><p><strong>Advanced Quantization Algorithms</strong>:</p>
<ul>
<li><p>Support SmoothQuant, AWQ (uint4), and GPTQ (uint4) for LLMs. (Note: AWQ/GPTQ/SmoothQuant algorithms are currently limited to single GPU usage.)</p></li>
</ul>
</li>
<li><p><strong>Export Capabilities</strong>:</p>
<ul>
<li><p>Export of Q/DQ quantized models to ONNX and vLLM-adopted JSON-safetensors format now supported.</p></li>
</ul>
</li>
<li><p><strong>Operating System Support</strong>:</p>
<ul>
<li><p>Linux (supports ROCM and CUDA)</p></li>
<li><p>Windows (supports CPU only).</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Welcome to AMD Quark Documentation!</p>
      </div>
    </a>
    <a class="right-next"
       href="intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Quantization with AMD Quark</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-10">Release 0.10</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-9">Release 0.9</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-8-2">Release 0.8.2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#new-features">New Features</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-8-1">Release 0.8.1</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bug-fixes-and-enhancements">Bug Fixes and Enhancements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-8">Release 0.8</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-7">Release 0.7</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">New Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Bug Fixes and Enhancements</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-6">Release 0.6</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-5-1">Release 0.5.1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-5-0">Release 0.5.0</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-2-0">Release 0.2.0</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#release-0-1-0">Release 0.1.0</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Sep 26, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2025 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>