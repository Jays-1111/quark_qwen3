
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LLM Model Depth-Wise Pruning (beta) &#8212; AMD Quark 0.10 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=a66ef196" />
    <link rel="stylesheet" type="text/css" href="../../../_static/rocm_header.css?v=9557e3d1" />
    <link rel="stylesheet" type="text/css" href="../../../_static/rocm_footer.css?v=7095035a" />
    <link rel="stylesheet" type="text/css" href="../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=e0f31c2e"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script defer="defer" src="../../../_static/search.js?v=90a4452c"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tutorials/torch/depth_wise_pruning/llm_depth_pruning';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/content/dam/code/images/favicon/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Quantizing a Large Language Model with Quark" href="../llm_tutorial/llm_tutorial.html" />
    <link rel="prev" title="Quantizing a Diffusion Model using Quark" href="../diffusion_tutorial/diffusion_tutorial.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].innerText = "Show code ▼\n\n";

            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://github.com/amd/quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">AMD Quark 0.10 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../basic_usage.html">Getting started: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/basic_usage_onnx.html">Getting started: Quark for ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/basic_usage_pytorch.html">Getting started: Quark for PyTorch</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/pytorch_examples.html">PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_fx_image_classification.html">Image Classification Models FX Graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/sample_yolo_nas_quant.html">YOLO-NAS FX graph Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/sample_yolo_x_tiny_quant.html">YOLO-X Tiny FX Graph Quantization</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/onnx_examples.html">ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_layerwise_percentile.html">Layer-wise Percentile</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_auto_search.html">Auto-Search for General Yolov3 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_ryzenai_yolonas.html">Auto-Search for Ryzen AI Yolo-nas ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_ryzenai_autosearch_resnet50.html">Auto-Search for Ryzen AI Resnet50 ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_ryzenai_yolov3_custom_evaluator.html">Auto-Search for Ryzen AI Yolov3 ONNX Quantization with Custom Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizing Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_crypto_mode.html">Quantizing a ResNet50 model in crypto mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/hugging_face_timm_quantization.html">Hugging Face TIMM Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_yolo_quantization.html">Yolo_nas and Yolox Quantization</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supported accelerators</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../supported_accelerators/ryzenai/index.html">AMD Ryzen AI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../supported_accelerators/ryzenai/tutorial_quick_start_for_ryzenai.html">Quick Start for Ryzen AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../supported_accelerators/ryzenai/ryzen_ai_best_practice.html">Best Practice for Ryzen AI in AMD Quark ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_ryzenai.html">Auto-Search for Ryzen AI ONNX Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../supported_accelerators/ryzenai/tutorial_uint4_oga.html">Quantizing LLMs for ONNX Runtime GenAI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../supported_accelerators/ryzenai/tutorial_convert_fp32_or_fp16_to_bf16.html">FP32/FP16 to BF16 Model Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../supported_accelerators/ryzenai/tutorial_xint8_quantize.html">Power-of-Two Scales (XINT8) Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../supported_accelerators/ryzenai/tutorial_a8w8_and_a16w8_quantize.html">Float Scales (A8W8 and A16W8) Quantization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../supported_accelerators/mi_gpus/index.html">AMD Instinct</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Evaluation of Quantized Models</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/user_guide_config_for_llm.html">Configuring PyTorch Quantization for Large Language Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization from Scratch</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/export/quark_export_onnx.html">ONNX format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/export/quark_export_hf.html">Hugging Face format (safetensors)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/export/quark_export_gguf.html">GGUF format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/export/quark_export_quark.html">Quark format</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/feature_pruning_overall.html">LLM Pruning</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../example_fp4.html">FP4 Post Training Quantization (PTQ) for LLM models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_fp8.html">FP8 Post Training Quantization (PTQ) for LLM models</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark and Trainer</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation-Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation-Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_smoothquant_document_and_example.html">Auto SmoothQuant</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/awq_document.html">Activation-aware Weight Quantization (AWQ)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../example_awq.html">AWQ end-to-end demo</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/custom_operators/ExtendedQuantizeLinear.html">ExtendedQuantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/custom_operators/ExtendedDequantizeLinear.html">ExtendedDequantizeLinear</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/custom_operators/ExtendedInstanceNormalization.html">ExtendedInstanceNormalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/custom_operators/ExtendedLSTM.html">ExtendedLSTM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/custom_operators/BFPQuantizeDequantize.html">BFPQuantizeDequantize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/custom_operators/MXQuantizeDequantize.html">MXQuantizeDequantize</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tutorial_bfp16_quantization.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tutorial_microscaling_quantization.html">Microscaling (MX)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tutorial_microexponents_quantization.html">Microexponents (MX)</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/accuracy_algorithms/quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_gptq.html">Quantizing a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/user_guide_auto_search.html">Automatic Search for Model Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/config/user_guide_onnx_model_inference_save_input_npy.html">Using ONNX Model Inference and Saving Input Data in NPY Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../quickstart_tutorial/quickstart_tutorial.html">AMD Quark Tutorial: PyTorch Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../diffusion_tutorial/diffusion_tutorial.html">Quantizing a Diffusion Model using Quark</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LLM Model Depth-Wise Pruning (beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm_tutorial/llm_tutorial.html">Quantizing a Large Language Model with Quark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Third-party contributions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../intro_contrib.html">Introduction and guidelines</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../autoapi/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/quantization/config/template/index.html">Quantizer Template</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../autoapi/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../versions.html">AMD Quark release history</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../license.html">Quark license</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">LLM Model Depth-Wise Pruning (beta)</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLM Model Depth-Wise Pruning (beta)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-pruning">LLM Pruning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#depth-wise-pruning">Depth-Wise Pruning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisite-some-facts">Prerequisite &amp; Some Facts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environments-prepare">Environments prepare</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-depth-wise">Perform Depth-Wise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#import-the-necessary-packages">Import the necessary packages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-llm-model-and-dataset-for-evaluation">Init the LLM model and dataset for evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-llm">Init the LLM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-testdata">Init the testdata</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-prune-config">Prepare the Prune config</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-prune-config">Init the prune config</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#param-explanations">Param explanations:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-prune-config-and-the-pruner-instance">Init the prune config and the pruner instance.</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Init the prune config</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-pruner-instance">Init the pruner instance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-the-pruning-process">Perform the Pruning Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-the-pruned-model-to-the-specified-path">Save the pruned model to the specified path</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments-results-partly">Experiments results (Partly)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#others">Others</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="llm-model-depth-wise-pruning-beta">
<h1>LLM Model Depth-Wise Pruning (beta)<a class="headerlink" href="#llm-model-depth-wise-pruning-beta" title="Link to this heading">#</a></h1>
<p>Pruning is the process of making the model smaller, either by dropping
layers (<em>depth pruning</em>) or dropping neurons and attention heads and
embedding channels (<em>width pruning</em>).</p>
<p>In this tutorial, we will show how to use Quark Depth-Wise pruning tool
to perform depth-wise pruning on several main streams of LLMs.</p>
<p>Please note that this tutorial requires a GPU compatible with ROCm or
CUDA to run. Before you run the pruning code, users need to get the LLM
from Hugging Face (e.g., download the HF checkpoint).</p>
<section id="llm-pruning">
<h2>LLM Pruning<a class="headerlink" href="#llm-pruning" title="Link to this heading">#</a></h2>
<p>Pruning is a powerful and well-known technique for reducing model size.
Combined with the model quantization, makes it easier to deploy the LLMs
in a server under computation resource constraints environments.</p>
<ul class="simple">
<li><p><strong>Structured pruning:</strong></p>
<ul>
<li><p>Prunes (delete) entire rows or columns of weights;</p></li>
<li><p>Dropping entire decode layers;</p></li>
<li><p>Both pruning the entire rows or entire layers can bring certain
computation acceleration.</p></li>
</ul>
</li>
<li><p><strong>Unstructured pruning</strong>: select the specific individual weight to 0,
but it may not bring much computation acceleration.</p></li>
<li><p><strong>Semi-structured pruning.</strong> Exactly N non-zero values in each block
of M consecutive weights.</p></li>
</ul>
<p>According to the recent research papers and industrial practice.
Structured pruning is a usable and adaptable method, which can actually
bring model size decrease and computation speed acceleration. In this
tutorial, we adopt several famous LLMs as examples to show the
effectiveness of the Quark <strong>Depth-Pruning</strong> tool.</p>
<p>All the Models and the accuracy have been tested under AMD ROCM GPU.
Showing the power of AMD GPU and the affiliated software community.</p>
</section>
<section id="depth-wise-pruning">
<h2>Depth-Wise Pruning<a class="headerlink" href="#depth-wise-pruning" title="Link to this heading">#</a></h2>
<p>Pruning essentially involves two key aspects: <strong>discovering redundancy</strong>
and recovering performance.</p>
<p>We adopt the <strong>PPL influence</strong> as layers importance evaluation method:
As redundant blocks contribute less to the model’s outputs, and their
removal leads to smaller degradation in PPL.</p>
</section>
<section id="prerequisite-some-facts">
<h2>Prerequisite &amp; Some Facts<a class="headerlink" href="#prerequisite-some-facts" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>GPU: to run pruning, at least a GPU compatible with ROCm or CUDA.</p></li>
<li><p>We support three modes to run the pruning process:</p>
<ul>
<li><p><strong>Pure GPU mode</strong>. This will need less time to finish the pruning
process, but requires larger GPU resources.</p></li>
<li><p><strong>GPU CPU mix mode</strong>. During the pruning process, only the layer
that needs to be computed will be placed on GPU; this may largely
reduce the GPU memory requirements, but it needs more time.</p></li>
<li><p>The user can select the proper method based on the actual production
environments.</p></li>
</ul>
</li>
</ul>
</section>
<section id="environments-prepare">
<h2>Environments prepare<a class="headerlink" href="#environments-prepare" title="Link to this heading">#</a></h2>
<p>For model pruning, we need to use GPUs compatible with ROCm (AMD GPUs)
or CUDA(NVIDIA GPUs), and you need a GPU version of PyTorch. You should
check <a class="reference external" href="https://pytorch.org/get-started/locally/">PyTorch’s installation
page</a> for instructions. For
example, we used ROCm 6.2.4 in this tutorial, which we installed as
follows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/rocm6.2.4
</pre></div>
</div>
<p>To install Quark. Please refer to the <a class="reference external" href="https://quark.docs.amd.com/latest/install.html">Installation
Guide</a> for further
information on setup.</p>
</section>
<section id="perform-depth-wise">
<h2>Perform Depth-Wise<a class="headerlink" href="#perform-depth-wise" title="Link to this heading">#</a></h2>
<section id="import-the-necessary-packages">
<h3>Import the necessary packages<a class="headerlink" href="#import-the-necessary-packages" title="Link to this heading">#</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.pruning.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span><span class="p">,</span> <span class="n">LayerImportancePruneConfig</span>
</pre></div>
</div>
</section>
<section id="init-the-llm-model-and-dataset-for-evaluation">
<h3>Init the LLM model and dataset for evaluation<a class="headerlink" href="#init-the-llm-model-and-dataset-for-evaluation" title="Link to this heading">#</a></h3>
<p>We use the <code class="docutils literal notranslate"><span class="pre">wikitext</span></code> dataset to test the <code class="docutils literal notranslate"><span class="pre">PPL</span></code> result to decide
which layers can be deleted.</p>
<section id="init-the-llm">
<h4>Init the LLM<a class="headerlink" href="#init-the-llm" title="Link to this heading">#</a></h4>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;facebook/opt-125m&quot;</span>  <span class="c1"># User can assign to the folder where the LLM downloaded.</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># if not enough GPU memory to load the entire LLM, user can also load LLM to CPU,</span>
<span class="c1"># but user must make sure they have at least one GPU to perform the forward running.</span>
<span class="c1"># model = AutoModelForCausalLM.from_pretrained(model_path, device_map=&#39;cpu&#39;, torch_dtype=&quot;auto&quot;, trust_remote_code=True)</span>
<span class="c1"># NOTE: if LLM is loaded to the CPU, make sure to use save_gpu_memory=true to run.</span>
</pre></div>
</div>
</section>
<section id="init-the-testdata">
<h4>Init the testdata<a class="headerlink" href="#init-the-testdata" title="Link to this heading">#</a></h4>
<p>Each time the original model deletes several consecutive layers, we use
<code class="docutils literal notranslate"><span class="pre">wikitext</span></code> to test the model’s <code class="docutils literal notranslate"><span class="pre">ppl</span></code> so as to decide layers’
importance.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_wikitext_dataset</span><span class="p">(</span><span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dev</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="n">testdata</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;wikitext&quot;</span><span class="p">,</span> <span class="s2">&quot;wikitext-2-raw-v1&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">model_dir</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">testenc</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">testdata</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">seqlen_for_eval</span> <span class="o">=</span> <span class="mi">2048</span>
    <span class="n">testenc</span> <span class="o">=</span> <span class="n">testenc</span><span class="o">.</span><span class="n">input_ids</span>
    <span class="n">nsamples</span> <span class="o">=</span> <span class="n">testenc</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">seqlen_for_eval</span>
    <span class="n">batch_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">testenc</span> <span class="o">=</span> <span class="n">testenc</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">testenc</span><span class="p">[:,</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">seqlen_for_eval</span><span class="p">)</span> <span class="p">:</span> <span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">seqlen_for_eval</span><span class="p">)]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
        <span class="n">batch_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch_data</span>


<span class="n">main_device</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">device</span>

<span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">get_wikitext_dataset</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">main_device</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="prepare-the-prune-config">
<h3>Prepare the Prune config<a class="headerlink" href="#prepare-the-prune-config" title="Link to this heading">#</a></h3>
<p>As we use the <code class="docutils literal notranslate"><span class="pre">facebook/opt-125m</span></code> as an example, this model has the
following structure: - Decoder layers have name fields:
<code class="docutils literal notranslate"><span class="pre">model.decoder.layers</span></code>. - Final norm layer has name fields:
<code class="docutils literal notranslate"><span class="pre">model.decoder.final_layer_norm</span></code></p>
<p>The model structure is as follows.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<section id="init-the-prune-config">
<h4>Init the prune config<a class="headerlink" href="#init-the-prune-config" title="Link to this heading">#</a></h4>
<p>NOTE: - Different models have different model structures, so we prepare
different configurations. - User need to Prepare the <code class="docutils literal notranslate"><span class="pre">config.json</span></code>
file, the <code class="docutils literal notranslate"><span class="pre">config.json</span></code> file shows as follows. The concent in
<code class="docutils literal notranslate"><span class="pre">config.json</span></code> can be seen as follows.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;delete_layer_num&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;model_decoder_layers&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;model.decoder.layers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;layer_norm_field&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;model.decoder.final_layer_norm&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;layer_num_field&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;save_gpu_memory&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="p">}</span>
</pre></div>
</div>
<section id="param-explanations">
<h5>Param explanations:<a class="headerlink" href="#param-explanations" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p><strong>delete_layer_num</strong>: We want to finally delete 2 consecutive decode
layers.</p></li>
<li><p><strong>model_decoder_layers</strong>: the decode layers name field in
<code class="docutils literal notranslate"><span class="pre">facebook/opt-125m</span></code>;</p></li>
<li><p><strong>layer_norm_field</strong>: the final norm layer name field.</p></li>
<li><p><strong>layer_num_field</strong>: In model’s <code class="docutils literal notranslate"><span class="pre">config.json</span></code>, for example, in
<code class="docutils literal notranslate"><span class="pre">fakebook/opt</span></code>, <code class="docutils literal notranslate"><span class="pre">num_hidden_layers</span></code> indicates the decode layer
num.</p></li>
<li><p><strong>save_gpu_memory</strong>:</p>
<ul>
<li><p>if <code class="docutils literal notranslate"><span class="pre">false</span></code>: model fully running in GPU, save time but need more
GPU mem.</p></li>
<li><p>if <code class="docutils literal notranslate"><span class="pre">true</span></code>: during the evaluation, the model is tested layer by
layer, saving GPU memory but typically needing more time.</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
<section id="init-the-prune-config-and-the-pruner-instance">
<h3>Init the prune config and the pruner instance.<a class="headerlink" href="#init-the-prune-config-and-the-pruner-instance" title="Link to this heading">#</a></h3>
<section id="id1">
<h4>Init the prune config<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Method 1: load from the Json file</span>
<span class="c1"># algo_config_file = &quot;./config.json&quot;  # as describe above</span>
<span class="c1"># with open(algo_config_file, &#39;r&#39;) as file:</span>
<span class="c1">#         algo_config_info = json.load(file)</span>
<span class="c1"># pruning_algo_config = LayerImportanceConfig.from_dict(algo_config_info)</span>

<span class="c1"># Method 2: manually set the config</span>
<span class="n">pruning_config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">()</span>
<span class="n">pruning_config</span><span class="o">.</span><span class="n">algo_config</span> <span class="o">=</span> <span class="n">LayerImportancePruneConfig</span><span class="p">()</span>
<span class="n">pruning_config</span><span class="o">.</span><span class="n">algo_config</span><span class="o">.</span><span class="n">delete_layer_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pruning_config</span><span class="o">.</span><span class="n">algo_config</span><span class="o">.</span><span class="n">model_decoder_layers</span> <span class="o">=</span> <span class="s2">&quot;model.decoder.layers&quot;</span>
<span class="n">pruning_config</span><span class="o">.</span><span class="n">algo_config</span><span class="o">.</span><span class="n">layer_norm_field</span> <span class="o">=</span> <span class="s2">&quot;model.decoder.final_layer_norm&quot;</span>
<span class="n">pruning_config</span><span class="o">.</span><span class="n">algo_config</span><span class="o">.</span><span class="n">layer_num_field</span> <span class="o">=</span> <span class="s2">&quot;num_hidden_layers&quot;</span>
</pre></div>
</div>
</section>
<section id="init-the-pruner-instance">
<h4>Init the pruner instance<a class="headerlink" href="#init-the-pruner-instance" title="Link to this heading">#</a></h4>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelPruner</span>

<span class="n">model_pruner</span> <span class="o">=</span> <span class="n">ModelPruner</span><span class="p">(</span><span class="n">pruning_config</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_num</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Before pruning, the model with: </span><span class="si">{</span><span class="n">param_num</span><span class="si">}</span><span class="s2"> parameters&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="perform-the-pruning-process">
<h3>Perform the Pruning Process<a class="headerlink" href="#perform-the-pruning-process" title="Link to this heading">#</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model_pruner</span><span class="o">.</span><span class="n">pruning_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">eval_dataloader</span><span class="p">)</span>
</pre></div>
</div>
<p>Based on the above log, it shows that after deleting the [2, 3] layers,
it has the smallest impact on PPL. The original model has 125239296
parameters; after pruning, the model has 10 layers and 111063552. The
pruning ratio is 11.32%.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">param_num</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;After pruning, the model with: </span><span class="si">{</span><span class="n">param_num</span><span class="si">}</span><span class="s2"> parameters&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="save-the-pruned-model-to-the-specified-path">
<h3>Save the pruned model to the specified path<a class="headerlink" href="#save-the-pruned-model-to-the-specified-path" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">save_dir</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{YOUR_PATH}</span><span class="s2">/facebook/opt-125m&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">safe_serialization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="experiments-results-partly">
<h2>Experiments results (Partly)<a class="headerlink" href="#experiments-results-partly" title="Link to this heading">#</a></h2>
<p>We conducted several experiments on different types of models to show
the compatibility of our tools.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Model</strong></p></th>
<th class="head"><p><strong>PPL
(Original)</strong></p></th>
<th class="head"><p><strong>PPL</strong>
<strong>(After
prune)</strong></p></th>
<th class="head"><p><strong>prune
ratio</strong></p></th>
<th class="head"><p><strong>other info</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>llama-2-7b-chat-hf</p></td>
<td><p>6.9419</p></td>
<td><p>8.139</p></td>
<td><p>9.01%</p></td>
<td><p>delete 3 layer
[11-13] total 32</p></td>
</tr>
<tr class="row-odd"><td><p>llama-3.1-405B</p></td>
<td><p>1.85624</p></td>
<td><p>2.8429</p></td>
<td><p>10.3%</p></td>
<td><p>delete 13 layer
[18-30] total 126</p></td>
</tr>
<tr class="row-even"><td><p>Llama-2-70b-chat-h</p></td>
<td><p>4.6460</p></td>
<td><p>5.5305</p></td>
<td><p>11.16%</p></td>
<td><p>delete 9 layer
[27-35] total 80</p></td>
</tr>
<tr class="row-odd"><td><p>Qwen2.5-14B-Instruct</p></td>
<td><p>5.7010</p></td>
<td><p>7.0681</p></td>
<td><p>9.32%</p></td>
<td><p>delete 5 layers
[22-26] total 48</p></td>
</tr>
<tr class="row-even"><td><p>Mixtral-8x7B-Instruct-v0.1</p></td>
<td><p>4.1378</p></td>
<td><p>5.0338</p></td>
<td><p>10%</p></td>
<td><p>delete 3 layer
[12-14] total 32</p></td>
</tr>
<tr class="row-odd"><td><p>facebook/opt-6.7b</p></td>
<td><p>10.8605</p></td>
<td><p>14.4321</p></td>
<td><p>12.1%</p></td>
<td><p>delete 4 layer
[21-24] total 32</p></td>
</tr>
<tr class="row-even"><td><p>deepseek-moe-16b-chat</p></td>
<td><p>7.3593</p></td>
<td><p>8.94327</p></td>
<td><p>10.76%</p></td>
<td><p>delete 3 layer
[11-13] total 28</p></td>
</tr>
</tbody>
</table>
</div>
<p>From several research papers, the beginning and the ending decode layers
play an important role in LLMs. Through the depth pruning, we also get
the same results that the layers at the beginning and end are the most
important. As a result should not be deleted.</p>
</section>
<section id="others">
<h2>Others<a class="headerlink" href="#others" title="Link to this heading">#</a></h2>
<p>We will further update our API design to enable Quark can support more
LLMs and enrich more pruning experiments.</p>
<p><strong>NOTE</strong> For the entire runnable Python script, the user can find the
script under
<code class="docutils literal notranslate"><span class="pre">examples/torch/language_modeling/llm_pruning/llm_pruning/main_depth_pruning.py</span></code></p>
<p>The example running command can be as follows.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>main_depth_pruning.py<span class="w"> </span>--model_dir<span class="o">={</span>PATH_TO_HF_MODEL<span class="o">}</span>Llama-2-7b-chat-hf<span class="w"> </span>--multi_gpu
</pre></div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../diffusion_tutorial/diffusion_tutorial.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quantizing a Diffusion Model using Quark</p>
      </div>
    </a>
    <a class="right-next"
       href="../llm_tutorial/llm_tutorial.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Quantizing a Large Language Model with Quark</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-pruning">LLM Pruning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#depth-wise-pruning">Depth-Wise Pruning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisite-some-facts">Prerequisite &amp; Some Facts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#environments-prepare">Environments prepare</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-depth-wise">Perform Depth-Wise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#import-the-necessary-packages">Import the necessary packages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-llm-model-and-dataset-for-evaluation">Init the LLM model and dataset for evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-llm">Init the LLM</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-testdata">Init the testdata</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-prune-config">Prepare the Prune config</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-prune-config">Init the prune config</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#param-explanations">Param explanations:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-prune-config-and-the-pruner-instance">Init the prune config and the pruner instance.</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Init the prune config</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#init-the-pruner-instance">Init the pruner instance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-the-pruning-process">Perform the Pruning Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-the-pruned-model-to-the-specified-path">Save the pruned model to the specified path</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments-results-partly">Experiments results (Partly)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#others">Others</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Sep 26, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/content/dam/amd/en/documents/corporate/cr/supply-chain-transparency.pdf" target="_blank">Supply Chain Transparency</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2025 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>