{
  "model_decoder_layers": "model.layers",
  "name": "awq",
  "scaling_layers": [
    {
      "inp": "self_attn.q_a_proj",
      "layers": [
        "self_attn.q_a_proj",
        "self_attn.kv_a_proj_with_mqa"
      ],
      "module2inspect": "self_attn",
      "prev_op": "input_layernorm"
    },
    {
      "inp": "self_attn.q_b_proj",
      "layers": [
        "self_attn.q_b_proj"
      ],
      "module2inspect": "self_attn.q_b_proj",
      "prev_op": "self_attn.q_a_layernorm"
    },
    {
      "inp": "self_attn.kv_b_proj",
      "layers": [
        "self_attn.kv_b_proj"
      ],
      "module2inspect": "self_attn.kv_b_proj",
      "prev_op": "self_attn.kv_a_layernorm"
    },
    {
      "inp": "self_attn.o_proj",
      "layers": [
        "self_attn.o_proj"
      ],
      "prev_op": "self_attn.kv_a_proj_with_mqa"
    },
    {
      "inp": "mlp.gate_proj",
      "layers": [
        "mlp.gate_proj",
        "mlp.up_proj"
      ],
      "module2inspect": "mlp",
      "prev_op": "post_attention_layernorm"
    },
    {
      "inp": "mlp.down_proj",
      "layers": [
        "mlp.down_proj"
      ],
      "prev_op": "mlp.up_proj"
    }
  ]
}
