{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO-X Tiny Quant example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the ENV & Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(message)s\", stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from trainer import Trainer\n",
    "from yolo_x_tiny_exp import Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--ckpt\", default=\"./yolox_tiny.pth\", type=str, help=\"pre train checkpoint\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=64, help=\"batch size\")\n",
    "parser.add_argument(\"--random_size_range\", type=int, default=3, help=\"random_size\")\n",
    "parser.add_argument(\"--experiment_name\", type=str, default=\"0\", help=\"exp name\")\n",
    "parser.add_argument(\"--data_dir\", default=\"./coco_data\", help=\"Data set directory.\")\n",
    "\n",
    "parser.add_argument(\"--min_lr_ratio\", type=float, default=0.01, help=\"batch size\")\n",
    "parser.add_argument(\"--ema_decay\", type=float, default=0.9995, help=\"ema decay reate.\")\n",
    "\n",
    "parser.add_argument(\"--output_dir\", default=\"./YOLOX_outputs\", help=\"Experiments results save path.\")\n",
    "parser.add_argument(\"--workers\", default=4, type=int, help=\"Number of data loading workers to be used.\")\n",
    "parser.add_argument(\"--multiscale_range\", default=5, type=int, help=\"multiscale_range.\")\n",
    "parser.add_argument(\"--start_epoch\", type=int, default=280, help=\"batch size\")\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the experiments & trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Exp(args)\n",
    "trainer = Trainer(exp, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare FP32 model & test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"args: {trainer.args}\")\n",
    "logging.info(f\"exp value:\\n{trainer.exp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.exp.get_model()\n",
    "model.to(trainer.device)\n",
    "model = trainer.load_pretrain_weight(model)\n",
    "trainer.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluator = trainer.exp.get_evaluator(batch_size=int(trainer.args.batch_size / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the FP32 model on the COCO val dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*_, summary = trainer.evaluator.evaluate(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PTQ & evaluate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Quantization config & Quantizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quark.torch import ModelQuantizer\n",
    "from quark.torch.quantization.config.config import Config, QuantizationConfig, QuantizationSpec\n",
    "from quark.torch.quantization.config.type import Dtype, QSchemeType, QuantizationMode, RoundType, ScaleType\n",
    "from quark.torch.quantization.observer.observer import PerTensorPowOf2MinMSEObserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INT8_PER_WEIGHT_TENSOR_SPEC = QuantizationSpec(\n",
    "    dtype=Dtype.int8,\n",
    "    qscheme=QSchemeType.per_tensor,\n",
    "    observer_cls=PerTensorPowOf2MinMSEObserver,\n",
    "    symmetric=True,\n",
    "    scale_type=ScaleType.float,\n",
    "    round_method=RoundType.half_even,\n",
    "    is_dynamic=False,\n",
    ")\n",
    "quant_config = QuantizationConfig(\n",
    "    weight=INT8_PER_WEIGHT_TENSOR_SPEC,\n",
    "    input_tensors=INT8_PER_WEIGHT_TENSOR_SPEC,\n",
    "    output_tensors=INT8_PER_WEIGHT_TENSOR_SPEC,\n",
    "    bias=INT8_PER_WEIGHT_TENSOR_SPEC,\n",
    ")\n",
    "quant_config = Config(global_quant_config=quant_config, quant_mode=QuantizationMode.fx_graph_mode)\n",
    "trainer.quantizer = ModelQuantizer(quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare calibration Dataset & Fx graph model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_data = [x[0].to(trainer.device) for x in list(itertools.islice(trainer.evaluator.dataloader, 1))]\n",
    "dummy_input = torch.randn(1, 3, *trainer.exp.input_size).to(trainer.device)\n",
    "trainer.model = trainer.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Based on the original YOLO_X Tiny repo code, loss calculation and bounding-boxes decode code are integrated in YOLO_X Tiny `forward`, we modify the code and let the `trainer.model.base_model` only contain the backbone network. We only need to quantize this part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_model = torch.export.export_for_training(trainer.model.base_model, (dummy_input,)).module()\n",
    "graph_model = torch.fx.GraphModule(graph_model, graph_model.graph)\n",
    "trainer.model.base_model = graph_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PTQ & evaluate the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = trainer.quantizer.quantize_model(graph_model, calib_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.base_model = quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*_, summary = trainer.evaluator.evaluate(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform QAT based on PTQ results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Based on the PTQ results, we perform the PTQ, through training, and adjust the weight/bias.\n",
    "This can get higher results.\n",
    "2. We adopt the training code from the original YOLO-X Tiny repo, and we train the model from 280 epoch. Based on the development time and our work focused mainly on the Quark Fx QAT tool, we only tried several parameters to perform training. Differently, we using one single GPU to perform training to largely reduce the training complexity. The user can try other hyperparameters to get higher results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Dataloader & Optimizer etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DataPrefetcher\n",
    "from trainer import ModelEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.no_aug = trainer.start_epoch >= trainer.max_epoch - trainer.exp.no_aug_epochs\n",
    "trainer.train_loader = trainer.exp.get_data_loader(\n",
    "    batch_size=trainer.args.batch_size, no_aug=trainer.no_aug, cache_img=None\n",
    ")\n",
    "logging.info(\"init prefetcher, this might take one minute or less...\")\n",
    "trainer.prefetcher = DataPrefetcher(trainer.train_loader)\n",
    "\n",
    "trainer.max_iter = len(trainer.train_loader)\n",
    "trainer.lr_scheduler = trainer.exp.get_lr_scheduler(\n",
    "    trainer.exp.basic_lr_per_img * trainer.args.batch_size, trainer.max_iter\n",
    ")\n",
    "trainer.optimizer = trainer.exp.get_optimizer(trainer.args.batch_size)\n",
    "#  ------ using ema for better coverage ---\n",
    "if trainer.use_model_ema:\n",
    "    trainer.ema_model = ModelEMA(trainer.model, trainer.args.ema_decay)  # 0.9995\n",
    "    trainer.ema_model.updates = trainer.max_iter * trainer.start_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training to further improve accuracy\n",
    "NOTE: We only training one epoch for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Training start...\")\n",
    "# logging.info(\"\\n{}\".format(trainer.model))\n",
    "trainer.epoch = 280\n",
    "logging.info(f\"---> start train epoch{trainer.epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: in function, `train_in_iter`, \n",
    "  1. We close the observer, meaning, during training the scale will not change;\n",
    "  2. Based on experience, we found that during training, we close the `bn` update that can get higher results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_in_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "To simplify, we directly load the fintuned weight to test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.load_state_dict(\n",
    "    torch.load(\"./YOLOX_outputs/yolo_x_tiny_exp_3031/best_ckpt.pth\", weights_only=False)[\"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*_, summary = trainer.evaluator.evaluate(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze model & export to onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze model\n",
    "For better deployment in the AMD NPU device, we apply several hardware optimizations (e.g. adjust the scale, insert multiply nodes to perform adjustment for hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezeded_model = trainer.quantizer.freeze(trainer.model.base_model.eval())\n",
    "trainer.model.base_model = freezeded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quark.torch import export_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for NPU compile, it is better using batch-size = 1 for better compliance\n",
    "example_inputs = (torch.rand(1, 3, 416, 416).to(trainer.device),)\n",
    "export_onnx(model=trainer.model, output_dir=\"./export_onnx/\", input_args=example_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplity the Onnx model and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "quant_model = onnx.load(\"./export_onnx/quark_model.onnx\")\n",
    "model_simp, check = simplify(quant_model)\n",
    "onnx.save_model(model_simp, \"./export_onnx/sample_quark_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `netron` to visualize the model (Optional)\n",
    "```shell\n",
    "$netron  ./export_onnx/sample_quark_model.onnx\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quark_nv_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "license": "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
